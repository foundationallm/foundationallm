# Knowledge management agent

The FoundationaLLM(FLLM) knowledge management agent supports two use cases: internal context and retrieval augmented generation (RAG).

## Internal Context

The internal context flow provides a pass-through mechanism that sends the user prompt directly to the large language model (LLM) without any additional processing or context. This is useful when the user prompt is already prepared and does not require any additional context.

## Retrieval Augmented Generation (RAG)

The RAG flow augments the user prompt with additional context to generate a more accurate response. The RAG flow uses a retrieval model to retrieve relevant documents from a knowledge base, such as a vector store, and then uses the retrieved documents to augment the user prompt before sending it to the LLM.

## Knowledge management agent configuration

The knowledge management agent configuration may reference the following resources:
    - [Vectorization text embedding profile](../vectorization/vectorization-profiles.md#text-embedding-profiles): The text embedding profile contains the configuration of the text embedding model used to embed the user prompt and perform a vector search in the knowledge base. This must match the text embedding profile used to index the knowledge base.
    - [Vectorization indexing profile](../vectorization/vectorization-profiles.md#indexing-profiles): The indexing profile contains the configuration of the service hosting the index that is to be searched.
    - [Prompt](prompt-resource.md): The system prompt of the agent, describes the persona of the agent.

>**Note**: The knowledge management agent implementation currently supports the [`AzureAISearchIndexer`](../vectorization/vectorization-profiles.html#azureaisearchindexer) indexing profile.

The structure of a knowledge management agent is the following:

```json
{
  "name": "<name>",
  "type": "knowledge-management",
  "object_id": "/instances/<instance_id>/providers/FoundationaLLM.Agent/agents/<name>",
  "description": "<description>",
  "indexing_profile": "<indexing_profile_resource_objectid>",
  "embedding_profile": "<text_embedding_profile_resource_objectid>",
  "prompt": "<prompt_resource_objectid>",
  "language_model": {
    "type": "openai",
    "provider": "microsoft",
    "temperature": 0.0,
    "use_chat": true,
    "api_endpoint": "FoundationaLLM:AzureOpenAI:API:Endpoint",
    "api_key": "FoundationaLLM:AzureOpenAI:API:Key",
    "api_version": "FoundationaLLM:AzureOpenAI:API:Version",
    "version": "FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion",
    "deployment": "FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName"
  },
  "sessions_enabled": true,
  "conversation_history": {
    "enabled": true,
    "max_history": 5
  },
  "gatekeeper": {
    "use_system_setting": false,
    "options": [
      "ContentSafety",
      "Presidio"
    ]
  },
  "orchestrator": "LangChain"
}
```

where:

- `<name>` is the name of the agent.
- `<instance_id>` is the instance ID of the deployment.
- `<description>` is the description of the agent, ensure this description details the purpose of the agent.
- `<indexing_profile_resource_objectid>` is the object ID of the indexing profile resource.
- `<text_embedding_profile_resource_objectid>` is the object ID of the text embedding profile resource.
- `<prompt_resource_objectid>` is the object ID of the prompt resource.

> **Note**: When an internal context agent is desired, remove the `indexing_profile`, `embedding_profile`, and `prompt` fields from the agent configuration.

| Parameter | Description |
| --- | --- |
| `name` | The name of the agent. |
| `type` | The type of the agent - will always be `knowledge-management`. |
| `object_id` | The object ID of the agent. Remove this element when creating an agent as this is generated by the Management API. |
| `description` | The description of the agent, ensure this description details the purpose of the agent. |
| `indexing_profile` | The object ID of the indexing profile resource. |
| `embedding_profile` | The object ID of the text embedding profile resource. |
| `prompt` | The object ID of the prompt resource. |
| `language_model` | The language model configuration. This sample demonstrates the usage of the Azure OpenAI language model. |
| `sessions_enabled` | A boolean value that indicates whether the agent is session-less (false) or supports sessions(true). |
| `conversation_history` | The conversation history configuration. the `enabled` property indicates if conversation history is retained for subsequent agent interactions(true). The `max_history` indicates the number of messages to be retained. |
| `gatekeeper` | The gatekeeper configuration. The `use_system_setting` property indicates if the system settings are used for the gatekeeper. The `options` property contains the list of gatekeeper options. The sample provided overrides the system setting for gatekeeper and enables Azure Content Safety and MS Presidio in the messaging pipeline. |
| `orchestrator` | The orchestrator to be used for the agent. This can be set to Semantic Kernel or LangChain |

## Managing knowledge management agents

This section describes how to manage knowledge management agents using the Management API. `{{baseUrl}}` is the base URL of the Management API. `{{instanceId}}` is the unique identifier of the FLLM instance.

### Retrieve

```http
HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents
```

### Create or update

```http
HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name>
Content-Type: application/json

BODY
<agent_configuration>
```

where `<agent_configuration>` is the agent configuration structure described above.

### Delete

```http
HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name>
```
