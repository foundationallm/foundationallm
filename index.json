{
  "docs/CLEANUP-PLAN.html": {
    "href": "docs/CLEANUP-PLAN.html",
    "title": "Documentation Cleanup Plan | FoundationaLLM",
    "summary": "Documentation Cleanup Plan Status: ✅ EXECUTED on 2024-12-14 Update: Removed redundant reference/ folder on 2024-12-14 (content duplicated root-level files) This document outlines the plan to clean up legacy folders that remain after the documentation reorganization. Overview After the reorganization documented in REORGANIZATION-PLAN.md, content has been moved to the new structure. However, the original folders remain and should be cleaned up. Pre-Cleanup Actions Before archiving, the following content needs to be migrated to the new structure: 1. Concepts → Management Portal The concepts/ folder has two files that need to be moved to management-portal/reference/concepts/: Source Destination Action concepts/index.md management-portal/reference/concepts/index.md MOVE (glossary/overview) concepts/prompt/prompt-variable.md management-portal/reference/concepts/prompt-variables.md MOVE The other concepts/ content has already been migrated: concepts/data-pipeline/data-pipeline.md → ✅ management-portal/reference/concepts/data-pipelines.md concepts/plugin/*.md → ✅ management-portal/reference/concepts/plugins-packages.md concepts/quota/*.md → ✅ management-portal/reference/concepts/quotas.md 2. Release Notes - Keep in Place The release-notes/ folder should remain at the root level as part of the new documentation structure. Update outline.md and REORGANIZATION-PLAN.md to reflect this. Rationale: Release notes are a top-level documentation concern and should be easily discoverable at the root level alongside other primary sections. Verification Status ✅ deployment/ → platform-operations/deployment/ Original File New Location Status deployment/index.md platform-operations/deployment/index.md ✅ Migrated deployment/deployment-quick-start.md platform-operations/deployment/deployment-quick-start.md ✅ Migrated deployment/deployment-standard.md platform-operations/deployment/deployment-standard.md ✅ Migrated deployment/deployment-configuration.md platform-operations/deployment/deployment-configuration.md ✅ Migrated deployment/app-configuration-values.md platform-operations/deployment/app-configuration-values.md ✅ Migrated deployment/azure-resource-providers-requirements.md platform-operations/deployment/azure-resource-providers-requirements.md ✅ Migrated deployment/custom-domains.md platform-operations/deployment/custom-domains.md ✅ Migrated deployment/soft-delete.md platform-operations/deployment/soft-delete.md ✅ Migrated deployment/standard/manifest.md platform-operations/deployment/standard-manifest.md ✅ Migrated deployment/configure-access-control-for-services.md platform-operations/security-permissions/configure-access-control-services.md ✅ Migrated deployment/authentication-authorization/* platform-operations/security-permissions/authentication-authorization/* ✅ Migrated ✅ role-based-access-control/ → platform-operations/security-permissions/role-based-access-control/ Original File New Location Status role-based-access-control/index.md platform-operations/security-permissions/role-based-access-control/index.md ✅ Migrated role-based-access-control/role-definitions.md platform-operations/security-permissions/role-based-access-control/role-definitions.md ✅ Migrated role-based-access-control/role-assignments.md platform-operations/security-permissions/role-based-access-control/role-assignments.md ✅ Migrated role-based-access-control/scope.md platform-operations/security-permissions/role-based-access-control/scope.md ✅ Migrated role-based-access-control/role-management.md platform-operations/security-permissions/role-based-access-control/role-management.md ✅ Migrated role-based-access-control/agent-role-assignments.md platform-operations/security-permissions/role-based-access-control/agent-role-assignments.md ✅ Migrated ✅ setup-guides/ → Various Locations Original Location New Location Status setup-guides/agents/agents_workflows.md management-portal/reference/concepts/agents-workflows.md ✅ Migrated setup-guides/agents/Agent_AccessToken.md management-portal/reference/concepts/agent-access-tokens.md ✅ Migrated setup-guides/agents/prompt-resource.md management-portal/reference/concepts/prompts-resources.md ✅ Migrated setup-guides/agents/knowledge-management-agent.md archive/knowledge-management-agent.md ✅ Archived (obsolete) setup-guides/branding/* management-portal/reference/branding/* ✅ Migrated setup-guides/exposed-apis/core-api.md apis-sdks/apis/core-api/index.md ✅ Migrated setup-guides/exposed-apis/management-api.md apis-sdks/apis/management-api/index.md ✅ Migrated setup-guides/exposed-apis/resource-management/* management-portal/reference/concepts/resource-management.md ✅ Migrated setup-guides/management-ui/management-ui.md management-portal/index.md ✅ Migrated setup-guides/quickstart.md chat-user-portal/quick-start/quickstart.md ✅ Migrated setup-guides/vectorization/* archive/vectorization/* ✅ Archived (obsolete) ✅ operations/ → platform-operations/ Original File New Location Status operations/backups.md platform-operations/how-to-guides/backups.md ✅ Migrated operations/logs.md platform-operations/monitoring-troubleshooting/logs.md ✅ Migrated operations/troubleshooting.md platform-operations/monitoring-troubleshooting/troubleshooting.md ✅ Migrated operations/security.md platform-operations/security-permissions/platform-security.md ✅ Migrated operations/graph-api-permissions.md platform-operations/security-permissions/graph-api-permissions.md ✅ Migrated operations/network-security-groups.md platform-operations/security-permissions/network-security-groups.md ✅ Migrated operations/vulnerabilities.md platform-operations/security-permissions/vulnerabilities.md ✅ Migrated operations/update.md platform-operations/how-to-guides/updating-container-versions.md ✅ Migrated operations/purge-conversations.md platform-operations/how-to-guides/purge-conversations.md ✅ Migrated operations/release-notes.md platform-operations/how-to-guides/creating-release-notes.md ✅ Migrated ✅ how-to-guides/ → management-portal/how-to-guides/ Original File New Location Status how-to-guides/create-model-agnostic-agent-claude.md management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.md ✅ Migrated how-to-guides/create-model-agnostic-agent-gpt4o.md management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.md ✅ Migrated ✅ api/ → apis-sdks/ Original File New Location Status api/index.md apis-sdks/apis/core-api/api-reference.md ✅ Migrated api/dotnet/index.md apis-sdks/sdks/dotnet/index.md ✅ Migrated api/python/index.md apis-sdks/sdks/python/index.md ✅ Migrated Execution Plan Phase 1: Migrate Remaining Concepts Content # Move concepts/index.md to management-portal/reference/concepts/ mv docs/concepts/index.md docs/management-portal/reference/concepts/index.md # Move prompt-variable.md mv docs/concepts/prompt/prompt-variable.md docs/management-portal/reference/concepts/prompt-variables.md Phase 2: Archive Legacy Folders Move legacy folders to archive/ without \"legacy-\" prefix: # Create archive subdirectories mkdir -p docs/archive/api mkdir -p docs/archive/concepts mkdir -p docs/archive/deployment mkdir -p docs/archive/how-to-guides mkdir -p docs/archive/operations mkdir -p docs/archive/role-based-access-control mkdir -p docs/archive/setup-guides # Move content to archive mv docs/api/* docs/archive/api/ mv docs/concepts/* docs/archive/concepts/ mv docs/deployment/* docs/archive/deployment/ mv docs/how-to-guides/* docs/archive/how-to-guides/ mv docs/operations/* docs/archive/operations/ mv docs/role-based-access-control/* docs/archive/role-based-access-control/ mv docs/setup-guides/* docs/archive/setup-guides/ # Remove empty directories rmdir docs/api rmdir docs/concepts rmdir docs/deployment rmdir docs/how-to-guides rmdir docs/operations rmdir docs/role-based-access-control rmdir docs/setup-guides Phase 3: Keep release-notes in Place The release-notes/ folder remains at the root level. No action needed. Final Folder Structure After cleanup: docs/ ├── apis-sdks/ # API and SDK documentation ├── archive/ # Archived content │ ├── api/ # Old API docs │ ├── concepts/ # Old concepts docs │ ├── deployment/ # Old deployment docs │ ├── how-to-guides/ # Old how-to guides │ ├── knowledge-management-agent.md │ ├── operations/ # Old operations docs │ ├── README.md │ ├── role-based-access-control/ # Old RBAC docs │ ├── setup-guides/ # Old setup guides │ └── vectorization/ # Obsolete vectorization docs ├── chat-user-portal/ # End-user documentation ├── development/ # Developer guides ├── management-portal/ # Management Portal documentation ├── overview/ # Platform overview ├── platform-operations/ # Deployment and operations ├── release-notes/ # Release notes (kept at root) │ ├── breaking-changes.md │ └── release_notes_0.9.7.md ├── docfx.json # Build configuration ├── toc.yml # Table of contents ├── outline.md # Structure reference ├── schema.md # Schema reference ├── REORGANIZATION-PLAN.md ├── CONTENT-TRACKING.md └── CLEANUP-PLAN.md Update to Documentation Structure Addition to outline.md Add release-notes/ as a top-level section: docs/ ├── ...existing sections... ├── release-notes/ # Release notes │ ├── breaking-changes.md │ └── [version-specific files] Update archive/README.md # Archived Documentation This folder contains documentation that has been superseded by the reorganized documentation structure. ## Contents | Folder | Original Purpose | Archived Date | Reason | |--------|-----------------|---------------|--------| | `api/` | API reference | 2024-12-14 | Moved to `apis-sdks/` | | `concepts/` | Concept definitions | 2024-12-14 | Moved to `management-portal/reference/concepts/` | | `deployment/` | Deployment guides | 2024-12-14 | Moved to `platform-operations/deployment/` | | `how-to-guides/` | How-to guides | 2024-12-14 | Moved to `management-portal/how-to-guides/` | | `operations/` | Operations guides | 2024-12-14 | Moved to `platform-operations/` | | `role-based-access-control/` | RBAC documentation | 2024-12-14 | Moved to `platform-operations/security-permissions/` | | `setup-guides/` | Setup guides | 2024-12-14 | Distributed to new sections | | `vectorization/` | Vectorization docs | 2024-12-14 | Feature deprecated | | `knowledge-management-agent.md` | Agent type docs | 2024-12-14 | Obsolete | ## Note This content is kept for historical reference. Do not link to these files from the main documentation. For current documentation, refer to the corresponding files in the new documentation structure. Post-Cleanup Tasks Update toc.yml - Ensure TOC includes release-notes/ at root level Update outline.md - Add release-notes/ to the documented structure Update docfx.json - Ensure paths are correct Verify build - Run docfx to ensure documentation builds Update any external links - If documentation is published, update bookmarks"
  },
  "docs/CONTENT-TRACKING.html": {
    "href": "docs/CONTENT-TRACKING.html",
    "title": "Content Tracking for Documentation Reorganization | FoundationaLLM",
    "summary": "Content Tracking for Documentation Reorganization This document tracks all content affected by the documentation reorganization for review purposes. Quick Stats Total Existing Files: 82 markdown files Files to Move: 65 Files to Archive (Obsolete): 8 Files with Partial Extraction: 4 New Stubs to Create: 52 Files Staying in Place: 5 ✅ Content Moved (Full File Moves) These files are moved completely to their new location. Links and references may need updating. Deployment → Platform Operations Original Path New Path Review Status deployment/index.md platform-operations/deployment/index.md ⬜ Pending deployment/deployment-quick-start.md platform-operations/deployment/deployment-quick-start.md ⬜ Pending deployment/deployment-standard.md platform-operations/deployment/deployment-standard.md ⬜ Pending deployment/deployment-configuration.md platform-operations/deployment/deployment-configuration.md ⬜ Pending deployment/app-configuration-values.md platform-operations/deployment/app-configuration-values.md ⬜ Pending deployment/azure-resource-providers-requirements.md platform-operations/deployment/azure-resource-providers-requirements.md ⬜ Pending deployment/custom-domains.md platform-operations/deployment/custom-domains.md ⬜ Pending deployment/soft-delete.md platform-operations/deployment/soft-delete.md ⬜ Pending deployment/standard/manifest.md platform-operations/deployment/standard-manifest.md ⬜ Pending deployment/configure-access-control-for-services.md platform-operations/security-permissions/configure-access-control-services.md ⬜ Pending Authentication/Authorization → Platform Operations Original Path New Path Review Status deployment/authentication-authorization/index.md platform-operations/security-permissions/authentication-authorization/index.md ⬜ Pending deployment/authentication-authorization/core-authentication-setup-entra.md platform-operations/security-permissions/authentication-authorization/pre-deployment/core-authentication-setup.md ⬜ Pending deployment/authentication-authorization/management-authentication-setup-entra.md platform-operations/security-permissions/authentication-authorization/pre-deployment/management-authentication-setup.md ⬜ Pending deployment/authentication-authorization/authorization-setup-entra.md platform-operations/security-permissions/authentication-authorization/pre-deployment/authorization-setup.md ⬜ Pending deployment/authentication-authorization/post-core-deployment.md platform-operations/security-permissions/authentication-authorization/post-deployment/core-authentication-post.md ⬜ Pending deployment/authentication-authorization/post-management-deployment.md platform-operations/security-permissions/authentication-authorization/post-deployment/management-authentication-post.md ⬜ Pending deployment/authentication-authorization/post-authorization-deployment.md platform-operations/security-permissions/authentication-authorization/post-deployment/authorization-post.md ⬜ Pending Operations → Platform Operations Original Path New Path Review Status operations/security.md platform-operations/security-permissions/platform-security.md ⬜ Pending operations/graph-api-permissions.md platform-operations/security-permissions/graph-api-permissions.md ⬜ Pending operations/network-security-groups.md platform-operations/security-permissions/network-security-groups.md ⬜ Pending operations/vulnerabilities.md platform-operations/security-permissions/vulnerabilities.md ⬜ Pending operations/logs.md platform-operations/monitoring-troubleshooting/logs.md ⬜ Pending operations/troubleshooting.md platform-operations/monitoring-troubleshooting/troubleshooting.md ⬜ Pending operations/update.md platform-operations/how-to-guides/updating-container-versions.md ⬜ Pending operations/backups.md platform-operations/how-to-guides/backups.md ⬜ Pending operations/purge-conversations.md platform-operations/how-to-guides/purge-conversations.md ⬜ Pending operations/release-notes.md platform-operations/how-to-guides/creating-release-notes.md ⬜ Pending RBAC → Platform Operations Original Path New Path Review Status role-based-access-control/index.md platform-operations/security-permissions/role-based-access-control/index.md ⬜ Pending role-based-access-control/role-definitions.md platform-operations/security-permissions/role-based-access-control/role-definitions.md ⬜ Pending role-based-access-control/role-assignments.md platform-operations/security-permissions/role-based-access-control/role-assignments.md ⬜ Pending role-based-access-control/scope.md platform-operations/security-permissions/role-based-access-control/scope.md ⬜ Pending role-based-access-control/role-management.md platform-operations/security-permissions/role-based-access-control/role-management.md ⬜ Pending role-based-access-control/agent-role-assignments.md platform-operations/security-permissions/role-based-access-control/agent-role-assignments.md ⬜ Pending Setup Guides → Various Sections Original Path New Path Review Status setup-guides/quickstart.md chat-user-portal/quick-start/quickstart.md ⬜ Pending setup-guides/management-ui/management-ui.md management-portal/index.md ⬜ Pending setup-guides/agents/agents_workflows.md management-portal/reference/concepts/agents-workflows.md ⬜ Pending setup-guides/agents/Agent_AccessToken.md management-portal/reference/concepts/agent-access-tokens.md ⬜ Pending setup-guides/agents/prompt-resource.md management-portal/reference/concepts/prompts-resources.md ⬜ Pending setup-guides/exposed-apis/core-api.md apis-sdks/apis/core-api/index.md ⬜ Pending setup-guides/exposed-apis/management-api.md apis-sdks/apis/management-api/index.md ⬜ Pending setup-guides/exposed-apis/resource-management/resource-management.md management-portal/reference/concepts/resource-management.md ⬜ Pending setup-guides/branding/index.md management-portal/reference/branding/index.md ⬜ Pending setup-guides/branding/branding-app-configuration.md management-portal/reference/branding/using-app-configuration.md ⬜ Pending setup-guides/branding/branding-management-portal.md management-portal/reference/branding/using-management-portal.md ⬜ Pending How-To Guides → Management Portal Original Path New Path Review Status how-to-guides/create-model-agnostic-agent-claude.md management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.md ⬜ Pending how-to-guides/create-model-agnostic-agent-gpt4o.md management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.md ⬜ Pending Development → APIs/SDKs + Development Original Path New Path Review Status development/index.md development/index.md ⬜ Pending (structure same) development/development-approach.md development/development-approach.md ⬜ Pending (structure same) development/development-local.md development/development-local.md ⬜ Pending (structure same) development/calling-apis/directly-calling-core-api.md apis-sdks/apis/core-api/directly-calling-core-api.md ⬜ Pending development/calling-apis/directly-calling-management-api.md apis-sdks/apis/management-api/directly-calling-management-api.md ⬜ Pending development/calling-apis/standard-deployment-local-api-access.md apis-sdks/apis/core-api/standard-deployment-local-api-access.md ⬜ Pending development/contributing/index.md development/contributing/index.md ⬜ Pending (structure same) development/contributing/git-workflow.md development/contributing/git-workflow.md ⬜ Pending (structure same) development/contributing/style-guide.md development/contributing/style-guide.md ⬜ Pending (structure same) development/contributing/repro.md development/contributing/bug-report-reproduction.md ⬜ Pending API Documentation → APIs/SDKs Original Path New Path Review Status api/index.md apis-sdks/apis/core-api/api-reference.md ⬜ Pending api/dotnet/index.md apis-sdks/sdks/dotnet/index.md ⬜ Pending api/python/index.md apis-sdks/sdks/python/index.md ⬜ Pending Concepts → Management Portal Reference Original Path New Path Review Status concepts/data-pipeline/data-pipeline.md management-portal/reference/concepts/data-pipelines.md ⬜ Pending concepts/plugin/plugin.md management-portal/reference/concepts/plugins-packages.md (merged) ⬜ Pending concepts/plugin/plugin-package.md management-portal/reference/concepts/plugins-packages.md (merged) ⬜ Pending concepts/quota/quota-definition.md management-portal/reference/concepts/quotas.md (merged) ⬜ Pending concepts/quota/agent-request-rate.md management-portal/reference/concepts/quotas.md (merged) ⬜ Pending concepts/quota/api-raw-request-rate.md management-portal/reference/concepts/quotas.md (merged) ⬜ Pending Reference Materials → Reference Original Path New Path Review Status schema.md reference/agent-schemas.md ⬜ Pending documentation-generation.md reference/documentation-generation.md ⬜ Pending release-notes/breaking-changes.md reference/release-notes/breaking-changes.md ⬜ Pending release-notes/release_notes_0.9.7.md reference/release-notes/release_notes_0.9.7.md ⬜ Pending Root Level → Overview Original Path New Path Review Status index.md overview/index.md ⬜ Pending \uD83D\uDCCB Content Requiring Partial Extraction These files have content that needs to be extracted to new files while keeping some content in place. Source File Content to Extract Destination File Review Status index.md \"Why is FoundationaLLM Needed?\" section + \"Where can FoundationaLLM fill the need?\" section overview/why-foundationallm.md ⬜ Needs Review concepts/index.md Architecture mindmap and core concepts overview overview/architecture-concepts.md ⬜ Needs Review setup-guides/quickstart.md \"Find your Core API URL\" section apis-sdks/apis/core-api/finding-core-api-url.md ⬜ Needs Review setup-guides/management-ui/management-ui.md Agent creation tutorial section management-portal/how-to-guides/agents/create-new-agent.md ⬜ Needs Review \uD83D\uDDC4️ Content Archived (Obsolete) These files are marked as obsolete per the outline and should be moved to an archive folder. Original Path Archive Path Reason Review Status setup-guides/vectorization/index.md archive/vectorization/index.md Vectorization section dropped ⬜ Pending setup-guides/vectorization/vectorization-concepts.md archive/vectorization/vectorization-concepts.md Vectorization section dropped ⬜ Pending setup-guides/vectorization/vectorization-configuration.md archive/vectorization/vectorization-configuration.md Vectorization section dropped ⬜ Pending setup-guides/vectorization/vectorization-profiles.md archive/vectorization/vectorization-profiles.md Vectorization section dropped ⬜ Pending setup-guides/vectorization/vectorization-triggering.md archive/vectorization/vectorization-triggering.md Vectorization section dropped ⬜ Pending setup-guides/vectorization/vectorization-monitoring-troubleshooting.md archive/vectorization/vectorization-monitoring-troubleshooting.md Vectorization section dropped ⬜ Pending setup-guides/agents/knowledge-management-agent.md archive/knowledge-management-agent.md Marked [OBSOLETE] in outline ⬜ Pending development/calling-apis/directly-calling-vectorization-api.md archive/directly-calling-vectorization-api.md Vectorization API marked [OBSOLETE] ⬜ Pending ✨ New Content Required (Stubs to Create) These are new files that need to be created as stubs for future content. Chat User Portal - How-To Guides File Path Description Priority chat-user-portal/index.md Chat User Portal overview High chat-user-portal/quick-start/creating-first-agent.md Creating first agent guide High chat-user-portal/how-to-guides/using-agents/selecting-agent.md How to select an agent Medium chat-user-portal/how-to-guides/using-agents/managing-available-agents.md Managing available agents Medium chat-user-portal/how-to-guides/using-agents/managing-conversations.md Managing conversations Medium chat-user-portal/how-to-guides/using-agents/configuring-accessibility.md Accessibility configuration Low chat-user-portal/how-to-guides/using-agents/uploading-files.md Uploading files to conversation High chat-user-portal/how-to-guides/using-agents/downloading-files.md Downloading files from conversation Medium chat-user-portal/how-to-guides/using-agents/using-code-interpreter.md Using code interpreter tool High chat-user-portal/how-to-guides/using-agents/using-knowledge-tool.md Using knowledge tool High chat-user-portal/how-to-guides/using-agents/using-other-tools.md Using other tools Medium chat-user-portal/how-to-guides/using-agents/monitoring-tokens.md Monitoring token consumption Low chat-user-portal/how-to-guides/using-agents/rating-responses.md Rating agent responses Low chat-user-portal/how-to-guides/using-agents/copying-prompts-results.md Copying prompts & formatted results Medium chat-user-portal/how-to-guides/using-agents/printing-conversations.md Printing conversations Low chat-user-portal/how-to-guides/using-agents/viewing-agent-prompts.md Viewing agent prompts Low Management Portal - How-To Guides File Path Description Priority management-portal/quick-start/portal-tour.md Tour of the portal High management-portal/quick-start/creating-first-agent.md Creating first agent High management-portal/how-to-guides/agents/create-new-agent.md Create new agent guide High management-portal/how-to-guides/agents/all-agents.md All agents view Medium management-portal/how-to-guides/agents/my-agents.md My agents view Medium management-portal/how-to-guides/agents/prompts.md Managing prompts Medium management-portal/how-to-guides/data/data-sources.md Data sources guide High management-portal/how-to-guides/data/data-pipelines/creating-data-pipelines.md Creating data pipelines High management-portal/how-to-guides/data/data-pipelines/invoking-data-pipelines.md Invoking data pipelines High management-portal/how-to-guides/data/data-pipelines/monitoring-data-pipelines.md Monitoring data pipelines Medium management-portal/how-to-guides/data/data-pipeline-runs.md Data pipeline runs Medium management-portal/how-to-guides/data/knowledge-sources/sharepoint-online.md SharePoint Online source Medium management-portal/how-to-guides/data/knowledge-sources/azure-data-lake.md Azure Data Lake source Medium management-portal/how-to-guides/data/knowledge-sources/private-storage.md Private storage for agents Medium management-portal/how-to-guides/data/knowledge-sources/knowledge-graph-integration.md Knowledge graph integration Low management-portal/how-to-guides/data/knowledge-sources/image-description.md Image-to-text description Low management-portal/how-to-guides/models-endpoints/ai-models.md Managing AI models High management-portal/how-to-guides/models-endpoints/api-endpoints.md Managing API endpoints Medium management-portal/how-to-guides/security/instance-access-control.md Instance access control High management-portal/how-to-guides/fllm-platform/branding.md Branding configuration Low management-portal/how-to-guides/fllm-platform/configuration.md Platform configuration Medium management-portal/how-to-guides/fllm-platform/deployment-information.md Deployment information Low management-portal/how-to-guides/managing-plugins.md Managing plugins Medium management-portal/how-to-guides/configuring-quotas.md Configuring quotas Medium Management Portal - Reference File Path Description Priority management-portal/reference/branding/using-rest-api.md Branding via REST API Low management-portal/reference/configuration-reference.md Configuration reference (from code) Medium management-portal/reference/permissions-roles.md Permissions & roles reference High APIs & SDKs File Path Description Priority apis-sdks/apis/core-api/finding-core-api-url.md Finding Core API URL High apis-sdks/apis/management-api/resource-providers-overview.md Resource providers overview Medium apis-sdks/apis/management-api/api-reference.md Management API reference High apis-sdks/apis/management-api/data-pipelines.md Data pipelines API Medium Reference File Path Description Priority reference/release-notes/index.md Release notes index Low \uD83D\uDCC1 Files Staying in Place These files are not moving but may need link updates. File Reason concepts/index.md Core concepts hub (content extracted but file stays) concepts/prompt/prompt-variable.md Referenced by concepts docfx.json Configuration file .gitignore Git configuration .ignore Search ignore \uD83D\uDD17 Links Requiring Update After reorganization, these link patterns need to be updated throughout the documentation: Old Pattern New Pattern ../deployment/ ../platform-operations/deployment/ ../operations/ ../platform-operations/ (distributed) ../setup-guides/agents/ ../management-portal/reference/concepts/ ../setup-guides/vectorization/ ../archive/vectorization/ ../role-based-access-control/ ../platform-operations/security-permissions/role-based-access-control/ ../development/calling-apis/ ../apis-sdks/apis/ ../api/ ../apis-sdks/ ../how-to-guides/ ../management-portal/how-to-guides/agents/ \uD83D\uDCDD Review Checklist Use this checklist when reviewing each section: [ ] All files moved to correct location [ ] Media files moved/updated [ ] Internal links updated [ ] Cross-references verified [ ] Frontmatter/metadata updated (if applicable) [ ] TOC entry added [ ] Build tested (docfx) [ ] Content accurate and up-to-date Progress Tracking Section Files Moved Links Updated Reviewed Complete Overview ⬜ ⬜ ⬜ ⬜ Chat User Portal ⬜ ⬜ ⬜ ⬜ Management Portal ⬜ ⬜ ⬜ ⬜ APIs & SDKs ⬜ ⬜ ⬜ ⬜ Platform Operations ⬜ ⬜ ⬜ ⬜ Development ⬜ ⬜ ⬜ ⬜ Reference ⬜ ⬜ ⬜ ⬜ Archive ⬜ ⬜ ⬜ ⬜ Reorganization Status: COMPLETED The documentation reorganization has been executed. All phases are complete: ✅ Phase 1: Created new folder structure ✅ Phase 2: Moved existing files to new locations ✅ Phase 3: Extracted partial content to new files ✅ Phase 4: Created stub files for new content (52 stubs) ✅ Phase 5: Archived obsolete content (vectorization, knowledge-management-agent) ✅ Phase 6: Merged files (plugins, quotas) ✅ Phase 7: Updated toc.yml for new structure Total files in new structure: 119 markdown files Cleanup Status: COMPLETED Legacy folders have been cleaned up and archived: ✅ Migrated concepts/index.md → management-portal/reference/concepts/index.md ✅ Migrated concepts/prompt/prompt-variable.md → management-portal/reference/concepts/prompt-variables.md ✅ Archived api/ → archive/api/ ✅ Archived concepts/ → archive/concepts/ ✅ Archived deployment/ → archive/deployment/ ✅ Archived how-to-guides/ → archive/how-to-guides/ ✅ Archived operations/ → archive/operations/ ✅ Archived role-based-access-control/ → archive/role-based-access-control/ ✅ Archived setup-guides/ → archive/setup-guides/ ✅ Kept release-notes/ at root level (top-level section) ✅ Updated archive/README.md with all archived content Reorganization Completed: December 14, 2025 Cleanup Completed: December 14, 2025 Plan Version: 1.1"
  },
  "docs/DOCUMENTATION-STATUS.html": {
    "href": "docs/DOCUMENTATION-STATUS.html",
    "title": "Documentation Status Report | FoundationaLLM",
    "summary": "Documentation Status Report Generated: January 2026 This report summarizes the documentation created and updated as part of the comprehensive documentation initiative covering knowledge sources, data pipelines, agent management, self-service agent creation, system status messages, API documentation, and accessibility. Summary Category New Files Updated Files Knowledge Sources 0 2 Data Pipelines 0 1 User Portal Agent Management 4 1 System Status Messages 2 0 UX Walkthroughs 2 0 Core API / Platform 2 1 Accessibility 0 1 Total 10 6 Plus terminology updates across development documentation. New Documentation Created User Portal - Agent Management File Path Description Creating and Editing Agents chat-user-portal/how-to-guides/using-agents/creating-editing-agents.md Self-service agent creation guide Sharing Agents chat-user-portal/how-to-guides/using-agents/sharing-agents.md Agent sharing model (Owners/Collaborators/Users) Setting Default Agent chat-user-portal/how-to-guides/using-agents/setting-default-agent.md Default agent configuration Viewing Status Messages chat-user-portal/how-to-guides/using-agents/viewing-status-messages.md End-user status message guide Management Portal - Status Messages & Walkthroughs File Path Description Status Messages management-portal/how-to-guides/fllm-platform/status-messages.md Admin status message publishing Agent Management Walkthrough management-portal/how-to-guides/agents/agent-management-walkthrough.md Full UX walkthrough Agent Creation Walkthrough management-portal/how-to-guides/agents/agent-creation-walkthrough.md Step-by-step creation guide Core API / Platform File Path Description OpenAI Endpoint Facades apis-sdks/apis/core-api/openai-endpoint-facades.md OpenAI-compatible endpoint guide API Limits management-portal/reference/concepts/api-limits.md Token and API rate limits Updated Documentation Knowledge Sources File Changes Made sharepoint-online.md Added dual-approach documentation (OneDrive upload + backend knowledge source), comparison table, best practices image-description.md Enhanced LLM-generated description documentation, added model size limits, token considerations Data Pipelines File Changes Made monitoring-data-pipelines.md Added performance and latency section, optimization guidance Agent Management File Changes Made managing-available-agents.md Added agent catalog concept, User Portal vs Management Portal clarification Core API File Changes Made api-reference.md Major expansion with curl examples, authentication comparison (Bearer vs Agent Access Tokens), validation rules, complete workflow examples Accessibility File Changes Made configuring-accessibility.md Added WCAG 2.1 compliance details, color contrast, reduced motion, cognitive accessibility, expanded screen reader guidance Development File Changes Made directly-calling-management-api.md Updated terminology (vectorization → data pipelines) Pages Flagged \"Article Still Being Authored\" The following pages contain the header indicating incomplete content: Page Primary TODO Areas creating-editing-agents.md Website crawler tool (future feature) viewing-status-messages.md Exact display locations, notification preferences status-messages.md Navigation path, audience targeting, display locations openai-endpoint-facades.md Feature under development - entire page TODO Items Requiring Manual Review High Priority (Feature Under Development) OpenAI Model Endpoint Facades (openai-endpoint-facades.md) Feature is under active development Entire document needs completion when feature is released Placeholder examples need verification Medium Priority (Missing UI Details) Status Messages - Admin (status-messages.md) Navigation path to status message management Audience targeting options (if available) Display location configuration Status Messages - User (viewing-status-messages.md) Exact display locations in User Portal Notification preferences configuration Historical status access SharePoint Authentication (sharepoint-online.md) Specific fields for app registration authentication (Client ID, Client Secret, Tenant ID) when visible in UI Incremental sync capabilities Image Processing (image-description.md) Specific image processing stage configuration options in data pipelines Low Priority (Future Features) Website Crawler Tool (creating-editing-agents.md) Document when feature is released Currently marked as future placeholder Private Storage (existing file) Detailed configuration options pending UI documentation Knowledge Graph (existing file) Specific data source configuration pending Documentation Structure Notes All new documentation follows the existing structure: User Portal docs: docs/docs/chat-user-portal/ Management Portal docs: docs/docs/management-portal/ API docs: docs/docs/apis-sdks/ TOC Files Updated The following TOC files have been updated to include new pages: chat-user-portal/toc.yml - Added agent creation, sharing, default agent, and status messages management-portal/toc.yml - Added walkthroughs, status messages, and API limits apis-sdks/apis/core-api/index.md - Added link to OpenAI Endpoint Facades Recommended Follow-up Actions Immediate Add new pages to relevant toc.yml files for navigation Review all TODO items and prioritize based on feature release timeline Have subject matter experts review technical accuracy Short-term Complete OpenAI Endpoint Facades documentation when feature releases Add screenshots to walkthrough documents Update status message documentation when UI is finalized Long-term Remove \"Article Still Being Authored\" headers when content is complete Regular review cycle to update TODO items User feedback integration for documentation improvements Files Changed Summary docs/docs/ ├── chat-user-portal/ │ └── how-to-guides/ │ └── using-agents/ │ ├── creating-editing-agents.md (NEW) │ ├── sharing-agents.md (NEW) │ ├── setting-default-agent.md (NEW) │ ├── viewing-status-messages.md (NEW) │ ├── managing-available-agents.md (UPDATED) │ └── configuring-accessibility.md (UPDATED) ├── management-portal/ │ ├── how-to-guides/ │ │ ├── agents/ │ │ │ ├── agent-management-walkthrough.md (NEW) │ │ │ └── agent-creation-walkthrough.md (NEW) │ │ ├── data/ │ │ │ ├── knowledge-sources/ │ │ │ │ ├── sharepoint-online.md (UPDATED) │ │ │ │ └── image-description.md (UPDATED) │ │ │ └── data-pipelines/ │ │ │ └── monitoring-data-pipelines.md (UPDATED) │ │ └── fllm-platform/ │ │ └── status-messages.md (NEW) │ └── reference/ │ └── concepts/ │ └── api-limits.md (NEW) ├── apis-sdks/ │ └── apis/ │ └── core-api/ │ ├── api-reference.md (UPDATED) │ └── openai-endpoint-facades.md (NEW) ├── development/ │ └── calling-apis/ │ └── directly-calling-management-api.md (UPDATED) └── DOCUMENTATION-STATUS.md (THIS FILE) Contact For questions about this documentation update or to report issues, contact the documentation team or file an issue in the repository."
  },
  "docs/REORGANIZATION-PLAN.html": {
    "href": "docs/REORGANIZATION-PLAN.html",
    "title": "FoundationaLLM Documentation Reorganization Plan | FoundationaLLM",
    "summary": "FoundationaLLM Documentation Reorganization Plan This document outlines the plan to reorganize the documentation based on the structure defined in outline.md. Summary Category Count Files to Move (Full) 45 Files to Copy (Partial Content) 3 New Files to Create (Stubs) 52 Files to Mark as Obsolete 8 Folders to Create 25 New Folder Structure docs/ ├── overview/ │ ├── index.md │ ├── architecture-concepts.md │ └── why-foundationallm.md │ ├── chat-user-portal/ │ ├── index.md │ ├── quick-start/ │ │ ├── quickstart.md │ │ └── creating-first-agent.md │ └── how-to-guides/ │ └── using-agents/ │ ├── selecting-agent.md │ ├── managing-available-agents.md │ ├── managing-conversations.md │ ├── configuring-accessibility.md │ ├── uploading-files.md │ ├── downloading-files.md │ ├── using-code-interpreter.md │ ├── using-knowledge-tool.md │ ├── using-other-tools.md │ ├── monitoring-tokens.md │ ├── rating-responses.md │ ├── copying-prompts-results.md │ ├── printing-conversations.md │ └── viewing-agent-prompts.md │ ├── management-portal/ │ ├── index.md │ ├── quick-start/ │ │ ├── portal-tour.md │ │ └── creating-first-agent.md │ ├── how-to-guides/ │ │ ├── agents/ │ │ │ ├── create-new-agent.md │ │ │ ├── create-model-agnostic-agent-claude.md │ │ │ ├── create-model-agnostic-agent-gpt4o.md │ │ │ ├── all-agents.md │ │ │ ├── my-agents.md │ │ │ └── prompts.md │ │ ├── data/ │ │ │ ├── data-sources.md │ │ │ ├── data-pipelines/ │ │ │ │ ├── creating-data-pipelines.md │ │ │ │ ├── invoking-data-pipelines.md │ │ │ │ └── monitoring-data-pipelines.md │ │ │ ├── data-pipeline-runs.md │ │ │ └── knowledge-sources/ │ │ │ ├── sharepoint-online.md │ │ │ ├── azure-data-lake.md │ │ │ ├── private-storage.md │ │ │ ├── knowledge-graph-integration.md │ │ │ └── image-description.md │ │ ├── models-endpoints/ │ │ │ ├── ai-models.md │ │ │ └── api-endpoints.md │ │ ├── security/ │ │ │ └── instance-access-control.md │ │ ├── fllm-platform/ │ │ │ ├── branding.md │ │ │ ├── configuration.md │ │ │ └── deployment-information.md │ │ ├── managing-plugins.md │ │ └── configuring-quotas.md │ └── reference/ │ ├── concepts/ │ │ ├── agents-workflows.md │ │ ├── agent-access-tokens.md │ │ ├── prompts-resources.md │ │ ├── knowledge-management-agent.md (OBSOLETE) │ │ ├── resource-management.md │ │ ├── data-pipelines.md │ │ ├── plugins-packages.md │ │ ├── vectorization.md (OBSOLETE) │ │ └── quotas.md │ ├── branding/ │ │ ├── index.md │ │ ├── using-app-configuration.md │ │ ├── using-management-portal.md │ │ └── using-rest-api.md │ ├── configuration-reference.md │ └── permissions-roles.md │ ├── apis-sdks/ │ ├── apis/ │ │ ├── core-api/ │ │ │ ├── index.md │ │ │ ├── finding-core-api-url.md │ │ │ ├── directly-calling-core-api.md │ │ │ ├── standard-deployment-local-api-access.md │ │ │ └── api-reference.md │ │ └── management-api/ │ │ ├── index.md │ │ ├── resource-providers-overview.md │ │ ├── directly-calling-management-api.md │ │ ├── api-reference.md │ │ └── data-pipelines.md │ └── sdks/ │ ├── dotnet/ │ │ └── index.md │ └── python/ │ └── index.md │ ├── platform-operations/ │ ├── deployment/ │ │ ├── index.md │ │ ├── deployment-quick-start.md │ │ ├── deployment-standard.md │ │ ├── deployment-configuration.md │ │ ├── app-configuration-values.md │ │ ├── azure-resource-providers-requirements.md │ │ ├── custom-domains.md │ │ ├── soft-delete.md │ │ └── standard-manifest.md │ ├── security-permissions/ │ │ ├── platform-security.md │ │ ├── authentication-authorization/ │ │ │ ├── index.md │ │ │ ├── pre-deployment/ │ │ │ │ ├── core-authentication-setup.md │ │ │ │ ├── management-authentication-setup.md │ │ │ │ └── authorization-setup.md │ │ │ └── post-deployment/ │ │ │ ├── core-authentication-post.md │ │ │ ├── management-authentication-post.md │ │ │ └── authorization-post.md │ │ ├── role-based-access-control/ │ │ │ ├── index.md │ │ │ ├── role-definitions.md │ │ │ ├── role-assignments.md │ │ │ ├── scope.md │ │ │ ├── role-management.md │ │ │ └── agent-role-assignments.md │ │ ├── configure-access-control-services.md │ │ ├── graph-api-permissions.md │ │ ├── network-security-groups.md │ │ └── vulnerabilities.md │ ├── monitoring-troubleshooting/ │ │ ├── logs.md │ │ └── troubleshooting.md │ └── how-to-guides/ │ ├── updating-container-versions.md │ ├── backups.md │ ├── purge-conversations.md │ └── creating-release-notes.md │ ├── development/ │ ├── index.md │ ├── development-approach.md │ ├── development-local.md │ └── contributing/ │ ├── index.md │ ├── git-workflow.md │ ├── style-guide.md │ └── bug-report-reproduction.md │ ├── release-notes/ (kept at root level) │ ├── breaking-changes.md │ └── [version-specific files] │ └── archive/ (for obsolete content) └── vectorization/ ├── index.md ├── vectorization-concepts.md ├── vectorization-configuration.md ├── vectorization-profiles.md ├── vectorization-triggering.md └── vectorization-monitoring-troubleshooting.md Detailed File Mapping Legend \uD83D\uDD04 MOVE - Move existing file to new location \uD83D\uDCCB COPY - Copy subset of content to new file ✨ CREATE - New file needs to be created (stub) \uD83D\uDDC4️ ARCHIVE - Move to archive as obsolete ❌ DROP - Content marked for removal (per outline) 1. Overview Section New Location Source Action Notes overview/index.md index.md \uD83D\uDD04 MOVE Main landing page overview/architecture-concepts.md concepts/index.md \uD83D\uDCCB COPY Extract architecture overview; keep concepts in original overview/why-foundationallm.md index.md \uD83D\uDCCB COPY Extract \"Why FoundationaLLM?\" section 2. Chat User Portal Section New Location Source Action Notes chat-user-portal/index.md - ✨ CREATE New overview of Chat User Portal chat-user-portal/quick-start/quickstart.md setup-guides/quickstart.md \uD83D\uDD04 MOVE Full file move chat-user-portal/quick-start/creating-first-agent.md - ✨ CREATE New stub for first agent guide chat-user-portal/how-to-guides/using-agents/selecting-agent.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/managing-available-agents.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/managing-conversations.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/configuring-accessibility.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/uploading-files.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/downloading-files.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/using-code-interpreter.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/using-knowledge-tool.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/using-other-tools.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/monitoring-tokens.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/rating-responses.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/copying-prompts-results.md - ✨ CREATE Includes Dual-Format Copy behavior chat-user-portal/how-to-guides/using-agents/printing-conversations.md - ✨ CREATE New how-to guide chat-user-portal/how-to-guides/using-agents/viewing-agent-prompts.md - ✨ CREATE New how-to guide 3. Management Portal Section New Location Source Action Notes management-portal/index.md setup-guides/management-ui/management-ui.md \uD83D\uDD04 MOVE Portal overview management-portal/quick-start/portal-tour.md - ✨ CREATE New tour guide management-portal/quick-start/creating-first-agent.md - ✨ CREATE New quick start management-portal/how-to-guides/agents/create-new-agent.md setup-guides/management-ui/management-ui.md \uD83D\uDCCB COPY Extract agent creation section management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.md how-to-guides/create-model-agnostic-agent-claude.md \uD83D\uDD04 MOVE Full file move management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.md how-to-guides/create-model-agnostic-agent-gpt4o.md \uD83D\uDD04 MOVE Full file move management-portal/how-to-guides/agents/all-agents.md - ✨ CREATE New guide management-portal/how-to-guides/agents/my-agents.md - ✨ CREATE New guide management-portal/how-to-guides/agents/prompts.md - ✨ CREATE New guide management-portal/how-to-guides/data/data-sources.md - ✨ CREATE New guide management-portal/how-to-guides/data/data-pipelines/creating-data-pipelines.md - ✨ CREATE New guide management-portal/how-to-guides/data/data-pipelines/invoking-data-pipelines.md - ✨ CREATE New guide management-portal/how-to-guides/data/data-pipelines/monitoring-data-pipelines.md - ✨ CREATE New guide management-portal/how-to-guides/data/data-pipeline-runs.md - ✨ CREATE New guide management-portal/how-to-guides/data/knowledge-sources/sharepoint-online.md - ✨ CREATE New guide management-portal/how-to-guides/data/knowledge-sources/azure-data-lake.md - ✨ CREATE New guide management-portal/how-to-guides/data/knowledge-sources/private-storage.md - ✨ CREATE New guide management-portal/how-to-guides/data/knowledge-sources/knowledge-graph-integration.md - ✨ CREATE New guide management-portal/how-to-guides/data/knowledge-sources/image-description.md - ✨ CREATE New guide management-portal/how-to-guides/models-endpoints/ai-models.md - ✨ CREATE New guide management-portal/how-to-guides/models-endpoints/api-endpoints.md - ✨ CREATE New guide management-portal/how-to-guides/security/instance-access-control.md - ✨ CREATE New guide management-portal/how-to-guides/fllm-platform/branding.md - ✨ CREATE New guide management-portal/how-to-guides/fllm-platform/configuration.md - ✨ CREATE New guide management-portal/how-to-guides/fllm-platform/deployment-information.md - ✨ CREATE New guide management-portal/how-to-guides/managing-plugins.md - ✨ CREATE New guide management-portal/how-to-guides/configuring-quotas.md - ✨ CREATE New guide management-portal/reference/concepts/agents-workflows.md setup-guides/agents/agents_workflows.md \uD83D\uDD04 MOVE Full file move management-portal/reference/concepts/agent-access-tokens.md setup-guides/agents/Agent_AccessToken.md \uD83D\uDD04 MOVE Full file move management-portal/reference/concepts/prompts-resources.md setup-guides/agents/prompt-resource.md \uD83D\uDD04 MOVE Full file move management-portal/reference/concepts/knowledge-management-agent.md setup-guides/agents/knowledge-management-agent.md \uD83D\uDDC4️ ARCHIVE Marked OBSOLETE management-portal/reference/concepts/resource-management.md setup-guides/exposed-apis/resource-management/resource-management.md \uD83D\uDD04 MOVE Full file move management-portal/reference/concepts/data-pipelines.md concepts/data-pipeline/data-pipeline.md \uD83D\uDD04 MOVE Full file move management-portal/reference/concepts/plugins-packages.md concepts/plugin/plugin.md, concepts/plugin/plugin-package.md \uD83D\uDD04 MOVE Merge both files management-portal/reference/concepts/quotas.md concepts/quota/*.md \uD83D\uDD04 MOVE Merge quota files management-portal/reference/branding/index.md setup-guides/branding/index.md \uD83D\uDD04 MOVE Full file move management-portal/reference/branding/using-app-configuration.md setup-guides/branding/branding-app-configuration.md \uD83D\uDD04 MOVE Full file move management-portal/reference/branding/using-management-portal.md setup-guides/branding/branding-management-portal.md \uD83D\uDD04 MOVE Full file move management-portal/reference/branding/using-rest-api.md - ✨ CREATE New stub management-portal/reference/configuration-reference.md - ✨ CREATE Generate from code constants management-portal/reference/permissions-roles.md - ✨ CREATE Generate from code constants 4. APIs & SDKs Section New Location Source Action Notes apis-sdks/apis/core-api/index.md setup-guides/exposed-apis/core-api.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/core-api/finding-core-api-url.md setup-guides/quickstart.md \uD83D\uDCCB COPY Extract URL finding section apis-sdks/apis/core-api/directly-calling-core-api.md development/calling-apis/directly-calling-core-api.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/core-api/standard-deployment-local-api-access.md development/calling-apis/standard-deployment-local-api-access.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/core-api/api-reference.md api/index.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/management-api/index.md setup-guides/exposed-apis/management-api.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/management-api/resource-providers-overview.md - ✨ CREATE Generate from code apis-sdks/apis/management-api/directly-calling-management-api.md development/calling-apis/directly-calling-management-api.md \uD83D\uDD04 MOVE Full file move apis-sdks/apis/management-api/api-reference.md - ✨ CREATE New stub apis-sdks/apis/management-api/data-pipelines.md - ✨ CREATE New stub apis-sdks/sdks/dotnet/index.md api/dotnet/index.md \uD83D\uDD04 MOVE Full file move apis-sdks/sdks/python/index.md api/python/index.md \uD83D\uDD04 MOVE Full file move 5. Platform Operations Section New Location Source Action Notes platform-operations/deployment/index.md deployment/index.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/deployment-quick-start.md deployment/deployment-quick-start.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/deployment-standard.md deployment/deployment-standard.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/deployment-configuration.md deployment/deployment-configuration.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/app-configuration-values.md deployment/app-configuration-values.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/azure-resource-providers-requirements.md deployment/azure-resource-providers-requirements.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/custom-domains.md deployment/custom-domains.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/soft-delete.md deployment/soft-delete.md \uD83D\uDD04 MOVE Full file move platform-operations/deployment/standard-manifest.md deployment/standard/manifest.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/platform-security.md operations/security.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/index.md deployment/authentication-authorization/index.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/pre-deployment/core-authentication-setup.md deployment/authentication-authorization/core-authentication-setup-entra.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/pre-deployment/management-authentication-setup.md deployment/authentication-authorization/management-authentication-setup-entra.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/pre-deployment/authorization-setup.md deployment/authentication-authorization/authorization-setup-entra.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/post-deployment/core-authentication-post.md deployment/authentication-authorization/post-core-deployment.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/post-deployment/management-authentication-post.md deployment/authentication-authorization/post-management-deployment.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/authentication-authorization/post-deployment/authorization-post.md deployment/authentication-authorization/post-authorization-deployment.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/index.md role-based-access-control/index.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/role-definitions.md role-based-access-control/role-definitions.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/role-assignments.md role-based-access-control/role-assignments.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/scope.md role-based-access-control/scope.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/role-management.md role-based-access-control/role-management.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/role-based-access-control/agent-role-assignments.md role-based-access-control/agent-role-assignments.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/configure-access-control-services.md deployment/configure-access-control-for-services.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/graph-api-permissions.md operations/graph-api-permissions.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/network-security-groups.md operations/network-security-groups.md \uD83D\uDD04 MOVE Full file move platform-operations/security-permissions/vulnerabilities.md operations/vulnerabilities.md \uD83D\uDD04 MOVE Full file move platform-operations/monitoring-troubleshooting/logs.md operations/logs.md \uD83D\uDD04 MOVE Full file move platform-operations/monitoring-troubleshooting/troubleshooting.md operations/troubleshooting.md \uD83D\uDD04 MOVE Full file move platform-operations/how-to-guides/updating-container-versions.md operations/update.md \uD83D\uDD04 MOVE Full file move platform-operations/how-to-guides/backups.md operations/backups.md \uD83D\uDD04 MOVE Full file move platform-operations/how-to-guides/purge-conversations.md operations/purge-conversations.md \uD83D\uDD04 MOVE Full file move platform-operations/how-to-guides/creating-release-notes.md operations/release-notes.md \uD83D\uDD04 MOVE Full file move 6. Development Section New Location Source Action Notes development/index.md development/index.md \uD83D\uDD04 MOVE Full file move development/development-approach.md development/development-approach.md \uD83D\uDD04 MOVE Full file move development/development-local.md development/development-local.md \uD83D\uDD04 MOVE Full file move development/contributing/index.md development/contributing/index.md \uD83D\uDD04 MOVE Full file move development/contributing/git-workflow.md development/contributing/git-workflow.md \uD83D\uDD04 MOVE Full file move development/contributing/style-guide.md development/contributing/style-guide.md \uD83D\uDD04 MOVE Full file move development/contributing/bug-report-reproduction.md development/contributing/repro.md \uD83D\uDD04 MOVE Full file move 7. Reference Section New Location Source Action Notes reference/agent-schemas.md schema.md \uD83D\uDD04 MOVE Full file move reference/documentation-generation.md documentation-generation.md \uD83D\uDD04 MOVE Full file move 8. Release Notes Section (Root Level) New Location Source Action Notes release-notes/ release-notes/ ✅ KEEP Keep at root level release-notes/breaking-changes.md Already exists ✅ KEEP No change release-notes/release_notes_0.9.7.md Already exists ✅ KEEP No change Note: The release-notes/ folder remains at the root level as a top-level documentation section for easy discoverability. 9. Archive Section (Obsolete Content) New Location Source Action Notes archive/vectorization/index.md setup-guides/vectorization/index.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/vectorization/vectorization-concepts.md setup-guides/vectorization/vectorization-concepts.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/vectorization/vectorization-configuration.md setup-guides/vectorization/vectorization-configuration.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/vectorization/vectorization-profiles.md setup-guides/vectorization/vectorization-profiles.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/vectorization/vectorization-triggering.md setup-guides/vectorization/vectorization-triggering.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/vectorization/vectorization-monitoring-troubleshooting.md setup-guides/vectorization/vectorization-monitoring-troubleshooting.md \uD83D\uDDC4️ ARCHIVE Per outline: DROP archive/directly-calling-vectorization-api.md development/calling-apis/directly-calling-vectorization-api.md \uD83D\uDDC4️ ARCHIVE Per outline: OBSOLETE Content Requiring Additional Review The following items need additional attention during integration: Files with Partial Content Extraction Source File Content to Extract Destination index.md \"Why FoundationaLLM?\" section overview/why-foundationallm.md concepts/index.md Architecture overview/mindmap overview/architecture-concepts.md setup-guides/quickstart.md \"Find your Core API URL\" section apis-sdks/apis/core-api/finding-core-api-url.md setup-guides/management-ui/management-ui.md Agent creation steps management-portal/how-to-guides/agents/create-new-agent.md Files to Generate from Code Destination Source Code Location Notes management-portal/reference/configuration-reference.md src/dotnet/Common/Constants/Data Generate from code constants management-portal/reference/permissions-roles.md src/dotnet/Common/Constants/Data/AuthorizableActions.json, RoleDefinitions.json Generate from code constants apis-sdks/apis/management-api/resource-providers-overview.md src/dotnet/Common/Constants/ResourceProviders Generate from metadata files Internal Links to Update After reorganization, the following types of links will need updating: All relative links between documentation files References to media files (may need to move/consolidate media folders) TOC file (toc.yml) needs complete rewrite Links in docfx.json configuration Media Files The following media folders exist and may need consolidation: docs/media/ (root media) docs/setup-guides/media/ docs/setup-guides/agents/media/ docs/setup-guides/vectorization/media/ Recommendation: Create a centralized docs/media/ structure organized by section. Implementation Phases Phase 1: Create New Folder Structure Create all new directories without moving files. Phase 2: Move Existing Files Move files that map 1:1 to new locations. Phase 3: Extract Partial Content Create new files from partial content extraction. Phase 4: Create Stub Files Create placeholder files for new content. Phase 5: Archive Obsolete Content Move vectorization and obsolete content to archive. Phase 6: Update Links and TOC Update all internal links and rebuild TOC. Phase 7: Media Consolidation Organize and consolidate media files. Phase 8: Validation Verify all links, test docfx build. Change Tracking Summary Existing Files Affected Original Location Status New Location index.md MOVE + EXTRACT overview/index.md concepts/index.md MOVE management-portal/reference/concepts/index.md (glossary/overview) concepts/data-pipeline/data-pipeline.md MOVE management-portal/reference/concepts/data-pipelines.md concepts/plugin/plugin.md MOVE management-portal/reference/concepts/plugins-packages.md concepts/plugin/plugin-package.md MERGE management-portal/reference/concepts/plugins-packages.md concepts/prompt/prompt-variable.md MOVE management-portal/reference/concepts/prompt-variables.md concepts/quota/quota-definition.md MERGE management-portal/reference/concepts/quotas.md concepts/quota/agent-request-rate.md MERGE management-portal/reference/concepts/quotas.md concepts/quota/api-raw-request-rate.md MERGE management-portal/reference/concepts/quotas.md deployment/index.md MOVE platform-operations/deployment/index.md deployment/deployment-quick-start.md MOVE platform-operations/deployment/deployment-quick-start.md deployment/deployment-standard.md MOVE platform-operations/deployment/deployment-standard.md deployment/deployment-configuration.md MOVE platform-operations/deployment/deployment-configuration.md deployment/app-configuration-values.md MOVE platform-operations/deployment/app-configuration-values.md deployment/azure-resource-providers-requirements.md MOVE platform-operations/deployment/azure-resource-providers-requirements.md deployment/configure-access-control-for-services.md MOVE platform-operations/security-permissions/configure-access-control-services.md deployment/custom-domains.md MOVE platform-operations/deployment/custom-domains.md deployment/soft-delete.md MOVE platform-operations/deployment/soft-delete.md deployment/standard/manifest.md MOVE platform-operations/deployment/standard-manifest.md deployment/authentication-authorization/index.md MOVE platform-operations/security-permissions/authentication-authorization/index.md deployment/authentication-authorization/core-authentication-setup-entra.md MOVE platform-operations/security-permissions/authentication-authorization/pre-deployment/core-authentication-setup.md deployment/authentication-authorization/management-authentication-setup-entra.md MOVE platform-operations/security-permissions/authentication-authorization/pre-deployment/management-authentication-setup.md deployment/authentication-authorization/authorization-setup-entra.md MOVE platform-operations/security-permissions/authentication-authorization/pre-deployment/authorization-setup.md deployment/authentication-authorization/post-core-deployment.md MOVE platform-operations/security-permissions/authentication-authorization/post-deployment/core-authentication-post.md deployment/authentication-authorization/post-management-deployment.md MOVE platform-operations/security-permissions/authentication-authorization/post-deployment/management-authentication-post.md deployment/authentication-authorization/post-authorization-deployment.md MOVE platform-operations/security-permissions/authentication-authorization/post-deployment/authorization-post.md deployment/authentication-authorization/pre-requisites.md REVIEW May merge into index development/index.md MOVE development/index.md development/development-approach.md MOVE development/development-approach.md development/development-local.md MOVE development/development-local.md development/calling-apis/index.md REVIEW Content may be distributed development/calling-apis/directly-calling-core-api.md MOVE apis-sdks/apis/core-api/directly-calling-core-api.md development/calling-apis/directly-calling-management-api.md MOVE apis-sdks/apis/management-api/directly-calling-management-api.md development/calling-apis/standard-deployment-local-api-access.md MOVE apis-sdks/apis/core-api/standard-deployment-local-api-access.md development/contributing/index.md MOVE development/contributing/index.md development/contributing/git-workflow.md MOVE development/contributing/git-workflow.md development/contributing/repro.md MOVE development/contributing/bug-report-reproduction.md development/contributing/style-guide.md MOVE development/contributing/style-guide.md how-to-guides/create-model-agnostic-agent-claude.md MOVE management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.md how-to-guides/create-model-agnostic-agent-gpt4o.md MOVE management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.md operations/backups.md MOVE platform-operations/how-to-guides/backups.md operations/graph-api-permissions.md MOVE platform-operations/security-permissions/graph-api-permissions.md operations/index.md REVIEW Content may be distributed operations/logs.md MOVE platform-operations/monitoring-troubleshooting/logs.md operations/network-security-groups.md MOVE platform-operations/security-permissions/network-security-groups.md operations/purge-conversations.md MOVE platform-operations/how-to-guides/purge-conversations.md operations/release-notes.md MOVE platform-operations/how-to-guides/creating-release-notes.md operations/security.md MOVE platform-operations/security-permissions/platform-security.md operations/troubleshooting.md MOVE platform-operations/monitoring-troubleshooting/troubleshooting.md operations/update.md MOVE platform-operations/how-to-guides/updating-container-versions.md operations/vulnerabilities.md MOVE platform-operations/security-permissions/vulnerabilities.md role-based-access-control/index.md MOVE platform-operations/security-permissions/role-based-access-control/index.md role-based-access-control/agent-role-assignments.md MOVE platform-operations/security-permissions/role-based-access-control/agent-role-assignments.md role-based-access-control/role-assignments.md MOVE platform-operations/security-permissions/role-based-access-control/role-assignments.md role-based-access-control/role-definitions.md MOVE platform-operations/security-permissions/role-based-access-control/role-definitions.md role-based-access-control/role-management.md MOVE platform-operations/security-permissions/role-based-access-control/role-management.md role-based-access-control/scope.md MOVE platform-operations/security-permissions/role-based-access-control/scope.md setup-guides/index.md DEPRECATE Content distributed to new sections setup-guides/quickstart.md MOVE + EXTRACT chat-user-portal/quick-start/quickstart.md setup-guides/agents/index.md REVIEW Content may be distributed setup-guides/agents/Agent_AccessToken.md MOVE management-portal/reference/concepts/agent-access-tokens.md setup-guides/agents/agents_workflows.md MOVE management-portal/reference/concepts/agents-workflows.md setup-guides/agents/knowledge-management-agent.md ARCHIVE archive/knowledge-management-agent.md setup-guides/agents/prompt-resource.md MOVE management-portal/reference/concepts/prompts-resources.md setup-guides/branding/index.md MOVE management-portal/reference/branding/index.md setup-guides/branding/branding-app-configuration.md MOVE management-portal/reference/branding/using-app-configuration.md setup-guides/branding/branding-management-portal.md MOVE management-portal/reference/branding/using-management-portal.md setup-guides/exposed-apis/index.md REVIEW Content distributed setup-guides/exposed-apis/core-api.md MOVE apis-sdks/apis/core-api/index.md setup-guides/exposed-apis/management-api.md MOVE apis-sdks/apis/management-api/index.md setup-guides/exposed-apis/resource-management/resource-management.md MOVE management-portal/reference/concepts/resource-management.md setup-guides/management-ui/management-ui.md MOVE + EXTRACT management-portal/index.md setup-guides/vectorization/index.md ARCHIVE archive/vectorization/index.md setup-guides/vectorization/vectorization-concepts.md ARCHIVE archive/vectorization/vectorization-concepts.md setup-guides/vectorization/vectorization-configuration.md ARCHIVE archive/vectorization/vectorization-configuration.md setup-guides/vectorization/vectorization-monitoring-troubleshooting.md ARCHIVE archive/vectorization/vectorization-monitoring-troubleshooting.md setup-guides/vectorization/vectorization-profiles.md ARCHIVE archive/vectorization/vectorization-profiles.md setup-guides/vectorization/vectorization-triggering.md ARCHIVE archive/vectorization/vectorization-triggering.md api/index.md MOVE apis-sdks/apis/core-api/api-reference.md api/dotnet/index.md MOVE apis-sdks/sdks/dotnet/index.md api/python/index.md MOVE apis-sdks/sdks/python/index.md documentation-generation.md MOVE reference/documentation-generation.md schema.md MOVE reference/agent-schemas.md release-notes/breaking-changes.md KEEP Stays at root level release-notes/release_notes_0.9.7.md KEEP Stays at root level toc.yml REWRITE Complete rewrite needed docfx.json UPDATE Update paths Files NOT Affected (Keep in Place) File Reason docfx.json Configuration file (update paths only) .gitignore Git configuration .ignore Search ignore concepts/prompt/prompt-variable.md Referenced by concepts index release-notes/ Top-level documentation section (kept at root for discoverability) Next Steps Review this plan with stakeholders Prioritize which sections to implement first Create stubs for new content pages Execute moves in logical order (start with leaf nodes) Update links iteratively Test build with docfx after each major change Finalize TOC once structure is stable"
  },
  "docs/apis-sdks/apis/core-api/api-reference.html": {
    "href": "docs/apis-sdks/apis/core-api/api-reference.html",
    "title": "Core API Reference | FoundationaLLM",
    "summary": "Core API Reference Complete endpoint reference for the FoundationaLLM Core API, designed for developers integrating FoundationaLLM into applications. Developer Overview The Core API is the primary interface for building applications that interact with FoundationaLLM agents. Use this API to: Send prompts to agents and receive AI-generated responses Manage conversations (sessions) and message history Upload files for agent analysis List available agents the user has access to Check service status and configuration Common Integration Patterns Pattern Use Case Key Endpoints Chatbot Integration Embed AI chat in your app POST /completions, POST /sessions Batch Processing Process many requests POST /async-completions Document Analysis Analyze uploaded files POST /files/upload, POST /completions Custom UI Build custom chat interface All session and completion endpoints Base URL https://{core-api-url}/instances/{instanceId} Variable Description Where to Find {core-api-url} Core API endpoint App Config: FoundationaLLM:APIs:CoreAPI:APIUrl {instanceId} FoundationaLLM instance ID App Config: FoundationaLLM:Instance:Id Authentication All Core API endpoints require authentication. FoundationaLLM supports two authentication methods: Method 1: Entra ID Bearer Token (Recommended) Use Microsoft Entra ID (Azure AD) JWT tokens for authenticated user access. This is the standard approach for applications where users sign in. Header: Authorization: Bearer <jwt-token> Getting a token with Azure CLI: # Login to Azure az login # Get token for Core API (replace with your client ID) TOKEN=$(az account get-access-token \\ --resource api://{core-api-client-id} \\ --query accessToken -o tsv) Example curl request with Bearer token: curl -X GET \"https://{core-api-url}/instances/{instanceId}/completions/agents\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" Method 2: Agent Access Token Use Agent Access Tokens for scenarios where Entra ID authentication isn't practical, such as: Public-facing applications Embedded widgets Automated systems without user context Header: X-AGENT-ACCESS-TOKEN: <agent-token> Example curl request with Agent Access Token: curl -X POST \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"X-AGENT-ACCESS-TOKEN: your-agent-access-token-here\" \\ -H \"Content-Type: application/json\" \\ -d '{\"user_prompt\": \"What can you help me with?\"}' Authentication Comparison Feature Entra ID Bearer Token Agent Access Token User Identity Tracks actual user Uses agent's virtual identity Setup Requires Entra ID app registration Created in Management Portal Token Lifetime Short-lived (typically 1 hour) Configurable expiration Multi-agent Access all permitted agents Bound to single agent Audit Trail Full user attribution Agent-level attribution Best For User-authenticated apps Public integrations Creating Agent Access Tokens Open the Management Portal Navigate to the agent's edit page Scroll to the Security section Click Create Access Token Set description and expiration date Copy the generated token immediately See Agent Access Tokens for detailed setup instructions. Completions Endpoints POST /completions Request a synchronous completion from an agent. The API waits for the full response before returning. curl Example (Bearer Token): curl -X POST \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"What is FoundationaLLM?\", \"agent_name\": \"knowledge-agent\" }' curl Example (Agent Access Token): curl -X POST \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"X-AGENT-ACCESS-TOKEN: your-token-here\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"What is FoundationaLLM?\" }' Request Body: { \"operation_id\": \"optional-guid\", \"user_prompt\": \"What can you help me with?\", \"session_id\": \"conversation-session-id\", \"agent_name\": \"my-agent\", \"attachments\": [], \"settings\": { \"model_parameters\": { \"temperature\": 0.7, \"max_new_tokens\": 2000, \"top_p\": 0.95 }, \"agent_parameters\": { \"index_top_n\": 10, \"index_filter_expression\": \"category eq 'documentation'\" } } } Field Type Required Description operation_id string No Custom operation ID (auto-generated if not provided) user_prompt string Yes User's question or instruction session_id string No Conversation session for context continuity agent_name string No Specific agent to use (required for Bearer auth if multiple agents) attachments array No File attachment references from uploads settings object No Runtime parameter overrides Response (200 OK): { \"operation_id\": \"abc123-def456\", \"user_prompt\": \"What can you help me with?\", \"completion\": \"I can assist you with...\", \"content\": [], \"citations\": [], \"analysis_results\": [], \"user_prompt_embedding\": [], \"prompt_tokens\": 125, \"completion_tokens\": 350, \"total_tokens\": 475, \"total_cost\": 0.0024 } Error Responses: Status Description Example 400 Invalid request body Missing required field user_prompt 401 Authentication required Invalid or expired token 403 Access denied No permission for specified agent 429 Quota exceeded Rate limit reached POST /async-completions Start an asynchronous completion operation for long-running requests. Returns immediately with an operation ID. curl Example: curl -X POST \"https://{core-api-url}/instances/{instanceId}/async-completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"Analyze this complex document and provide a detailed summary...\", \"agent_name\": \"document-agent\" }' Response (202 Accepted): { \"operation_id\": \"abc123-def456\", \"status\": \"InProgress\", \"status_message\": \"Operation started\", \"result\": null } Use the operation_id to poll for completion status. GET /completions/agents List agents available to the current user. curl Example: curl -X GET \"https://{core-api-url}/instances/{instanceId}/completions/agents\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): [ { \"resource\": { \"type\": \"agent\", \"name\": \"knowledge-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/knowledge-agent\", \"display_name\": \"Knowledge Agent\", \"description\": \"Answers questions from documentation\" }, \"roles\": [\"Reader\"], \"actions\": [\"read\"] } ] Sessions Endpoints Sessions maintain conversation context across multiple messages. GET /sessions List all conversations for the current user. curl Example: curl -X GET \"https://{core-api-url}/instances/{instanceId}/sessions\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): [ { \"id\": \"session-guid\", \"name\": \"My Conversation\", \"type\": \"Session\", \"upn\": \"user@example.com\", \"messages\": [], \"created_on\": \"2024-01-15T10:30:00Z\", \"updated_on\": \"2024-01-15T11:45:00Z\" } ] POST /sessions Create a new conversation session. curl Example (Bearer Token): curl -X POST \"https://{core-api-url}/instances/{instanceId}/sessions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\": \"New Conversation\"}' curl Example (Agent Access Token): curl -X POST \"https://{core-api-url}/instances/{instanceId}/sessions\" \\ -H \"X-AGENT-ACCESS-TOKEN: your-token-here\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\": \"New Conversation\"}' Response (200 OK): { \"id\": \"new-session-guid\", \"name\": \"New Conversation\", \"type\": \"Session\", \"upn\": \"user@example.com\", \"messages\": [], \"created_on\": \"2024-01-15T12:00:00Z\" } GET /sessions/{sessionId}/messages Retrieve messages for a conversation. curl Example: curl -X GET \"https://{core-api-url}/instances/{instanceId}/sessions/{sessionId}/messages\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): [ { \"id\": \"message-guid\", \"type\": \"Message\", \"session_id\": \"session-guid\", \"sender\": \"User\", \"sender_display_name\": \"John Doe\", \"text\": \"What is FoundationaLLM?\", \"tokens\": 10, \"timestamp\": \"2024-01-15T10:30:00Z\" }, { \"id\": \"message-guid-2\", \"type\": \"Message\", \"session_id\": \"session-guid\", \"sender\": \"Assistant\", \"sender_display_name\": \"Knowledge Agent\", \"text\": \"FoundationaLLM is a platform for...\", \"tokens\": 150, \"timestamp\": \"2024-01-15T10:30:05Z\" } ] DELETE /sessions/{sessionId} Delete a conversation and all its messages. curl Example: curl -X DELETE \"https://{core-api-url}/instances/{instanceId}/sessions/{sessionId}\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): Empty body POST /sessions/{sessionId}/message/{messageId}/rate Rate an assistant response (thumbs up/down). curl Example: curl -X POST \"https://{core-api-url}/instances/{instanceId}/sessions/{sessionId}/message/{messageId}/rate\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"rating\": true, \"comments\": \"Very helpful response\" }' Files Endpoints POST /files/upload Upload a file attachment for agent analysis. curl Example: curl -X POST \"https://{core-api-url}/instances/{instanceId}/files/upload\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -F \"file=@/path/to/document.pdf\" \\ -F \"agent_name=knowledge-agent\" \\ -F \"session_id=session-guid\" Response (200 OK): { \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Attachment/attachments/file-guid\", \"display_name\": \"document.pdf\", \"content_type\": \"application/pdf\" } Status Endpoints GET /status Get Core API service status. curl Example: curl -X GET \"https://{core-api-url}/instances/{instanceId}/status\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): { \"status\": \"ready\", \"name\": \"CoreAPI\", \"version\": \"1.0.0\", \"instance_id\": \"instance-guid\" } Branding Endpoints GET /branding Get portal branding configuration. curl Example: curl -X GET \"https://{core-api-url}/instances/{instanceId}/branding\" \\ -H \"Authorization: Bearer $TOKEN\" Response (200 OK): { \"company_name\": \"FoundationaLLM\", \"page_title\": \"FoundationaLLM User Portal\", \"logo_url\": \"foundationallm-logo-white.svg\", \"primary_color\": \"#131833\", \"primary_text_color\": \"#fff\" } Rate Limiting and Quotas The Core API enforces quota limits to prevent abuse and ensure fair usage. Quota Response When quota is exceeded: Response (429 Too Many Requests): { \"quota_exceeded\": true, \"quota_name\": \"AgentCompletionRequestRate\", \"retry_after_seconds\": 60, \"message\": \"Rate limit exceeded. Try again later.\" } Handling Rate Limits in Code # Example retry logic with curl response=$(curl -s -w \"\\n%{http_code}\" -X POST \\ \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{\"user_prompt\": \"Hello\"}') http_code=$(echo \"$response\" | tail -1) if [ \"$http_code\" = \"429\" ]; then echo \"Rate limited. Waiting 60 seconds...\" sleep 60 # Retry request fi Validation Rules Request Validation Field Validation Error Message user_prompt Required, non-empty \"user_prompt is required\" session_id Valid GUID format \"Invalid session_id format\" agent_name Must exist and be accessible \"Agent not found or access denied\" temperature 0.0 to 1.0 \"temperature must be between 0 and 1\" max_new_tokens Positive integer \"max_new_tokens must be positive\" Common Validation Errors { \"error\": { \"code\": \"ValidationError\", \"message\": \"One or more validation errors occurred\", \"details\": [ { \"field\": \"user_prompt\", \"message\": \"The user_prompt field is required.\" } ] } } Complete Workflow Example Here's a complete example of creating a session, sending a message, and retrieving the response: # Set variables CORE_API=\"https://your-core-api.azurecontainerapps.io\" INSTANCE_ID=\"your-instance-guid\" TOKEN=$(az account get-access-token --resource api://your-client-id --query accessToken -o tsv) # 1. Create a new session SESSION=$(curl -s -X POST \"$CORE_API/instances/$INSTANCE_ID/sessions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\": \"API Test Conversation\"}') SESSION_ID=$(echo $SESSION | jq -r '.id') echo \"Created session: $SESSION_ID\" # 2. Send a message RESPONSE=$(curl -s -X POST \"$CORE_API/instances/$INSTANCE_ID/completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"user_prompt\\\": \\\"What is FoundationaLLM?\\\", \\\"session_id\\\": \\\"$SESSION_ID\\\", \\\"agent_name\\\": \\\"knowledge-agent\\\" }\") echo \"Response: $(echo $RESPONSE | jq -r '.completion')\" # 3. Retrieve conversation history MESSAGES=$(curl -s -X GET \"$CORE_API/instances/$INSTANCE_ID/sessions/$SESSION_ID/messages\" \\ -H \"Authorization: Bearer $TOKEN\") echo \"Messages in session: $(echo $MESSAGES | jq length)\" Related Topics Core API Overview Directly Calling Core API Agent Access Tokens Quotas Reference .NET SDK Python SDK"
  },
  "docs/apis-sdks/apis/core-api/directly-calling-core-api.html": {
    "href": "docs/apis-sdks/apis/core-api/directly-calling-core-api.html",
    "title": "Directly Calling the Core API | FoundationaLLM",
    "summary": "Directly Calling the Core API This guide explains how to call the FoundationaLLM Core API directly using tools like Postman, curl, or custom applications. Overview While users typically interact with the Core API through the Chat User Portal, developers can call the API directly to: Integrate FoundationaLLM into custom applications Build automated workflows Test agent configurations Create custom chat interfaces API Architecture The Core API is the entry point to FoundationaLLM's orchestration layer: sequenceDiagram actor U as Caller participant C as Core API participant G as Gatekeeper API participant O as Orchestration API participant L as LangChain API U->>C: POST /completions C->>G: Content safety check G->>C: Approved request C->>O: Process completion O->>L: Execute orchestration L->>O: LLM response O->>C: Processed response C->>U: Final response Note: The Gatekeeper can be bypassed by setting FoundationaLLM:APIs:CoreAPI:BypassGatekeeper to true in App Configuration. This improves performance but disables content filtering. Prerequisites Before calling the Core API, you need: Core API URL - See Finding Your Core API URL Instance ID - Found in App Configuration: FoundationaLLM:Instance:Id Authentication Token - Entra ID bearer token or Agent Access Token Using Postman Install Postman Download and install Postman. Import the Collection Click the button below to fork and import the official FoundationaLLM Core API Postman collection: Configure Variables Select the FoundationaLLM.Core.API collection Click the Variables tab Update the following Current value fields: Variable Value Where to Find baseUrl Your Core API URL App Config: FoundationaLLM:APIs:CoreAPI:APIUrl instanceId Your instance GUID App Config: FoundationaLLM:Instance:Id tenantId Azure AD tenant ID Entra ID app registration appClientId Client ID Entra ID app registration appScope API scope Entra ID app registration Click Save Configure Authentication Select the Authorization tab in the collection Scroll to the bottom and click Get New Access Token Log in with your credentials Click Use Token after authentication Click Save Important: Add https://oauth.pstmn.io/v1/callback as a Redirect URI in your Entra ID app registration for Postman authentication to work. Make Your First Request Expand the sessions folder in the collection Select the GET Sessions request Click Send Verify you receive a 200 OK response Using curl Get Authentication Token First, obtain a bearer token using Azure CLI: # Login to Azure az login # Get token for Core API TOKEN=$(az account get-access-token \\ --resource api://{core-api-client-id} \\ --query accessToken -o tsv) List Sessions curl -X GET \\ \"https://{core-api-url}/instances/{instanceId}/sessions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" Request a Completion curl -X POST \\ \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"What is FoundationaLLM?\", \"agent_name\": \"default-agent\" }' Create a Session curl -X POST \\ \"https://{core-api-url}/instances/{instanceId}/sessions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"My New Conversation\" }' Completion with Session Context curl -X POST \\ \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"Tell me more about that\", \"session_id\": \"{session-id}\", \"agent_name\": \"default-agent\" }' Using Agent Access Tokens For unauthenticated access scenarios, use Agent Access Tokens: curl -X POST \\ \"https://{core-api-url}/instances/{instanceId}/completions\" \\ -H \"X-AGENT-ACCESS-TOKEN: {agent-access-token}\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_prompt\": \"Hello, what can you do?\" }' See Agent Access Tokens for setup instructions. Swagger UI For interactive API exploration: Deployment Swagger URL ACA https://{core-api-url}/swagger/ AKS https://{aks-url}/core/swagger/v1/swagger.json Common Request Patterns Sessionless Completion (Quick Query) No conversation history: { \"user_prompt\": \"What time is it in Tokyo?\" } Session-based Completion (Conversation) Maintains context: { \"user_prompt\": \"What about in London?\", \"session_id\": \"existing-session-id\" } Completion with Parameter Overrides Customize model behavior: { \"user_prompt\": \"Write a creative story\", \"settings\": { \"model_parameters\": { \"temperature\": 0.9, \"max_new_tokens\": 2000 } } } Completion with Agent Selection Target a specific agent: { \"user_prompt\": \"Analyze this data\", \"agent_name\": \"data-analysis-agent\" } Error Handling Status Meaning Action 400 Invalid request Check request body format 401 Unauthorized Refresh authentication token 403 Forbidden Check user permissions 404 Not found Verify endpoint URL and IDs 429 Rate limited Wait and retry 500 Server error Check API logs Related Topics Core API Overview API Reference Finding Your Core API URL .NET SDK"
  },
  "docs/apis-sdks/apis/core-api/finding-core-api-url.html": {
    "href": "docs/apis-sdks/apis/core-api/finding-core-api-url.html",
    "title": "Finding Your Core API URL | FoundationaLLM",
    "summary": "Finding Your Core API URL This guide explains how to locate the Core API URL for your FoundationaLLM deployment. Overview The Core API URL depends on your deployment type: Deployment Type URL Pattern Azure Container Apps (ACA) Container app URL ending in coreca Azure Kubernetes Service (AKS) Cluster FQDN with /core path Method 1: Azure App Configuration The easiest way to find your Core API URL: Open the Azure Portal Navigate to your FoundationaLLM resource group Open the App Configuration resource Select Configuration explorer Search for: FoundationaLLM:APIs:CoreAPI:APIUrl Copy the value Method 2: Azure Container Apps (ACA) Deployment For ACA deployments: Navigate to your FoundationaLLM resource group in Azure Portal Find the Container App resource with a name ending in coreca Example: fllm001coreca In the Container App Overview pane, copy the Application Url Example: https://fllmaca002coreca.graybush-c554b849.eastus.azurecontainerapps.io Method 3: Azure Kubernetes Service (AKS) Deployment For AKS deployments: Navigate to your FoundationaLLM resource group in Azure Portal Open the Kubernetes Service resource Select Properties in the left menu Copy the HTTP application routing domain Append /core to the domain Example: If the domain is https://1cf699fd0d89446eabf2.eastus.aksapp.io/, then the Core API URL is https://1cf699fd0d89446eabf2.eastus.aksapp.io/core Method 4: Azure CLI Use the Azure CLI to retrieve the URL: For ACA: # List container apps and find the core API az containerapp list \\ --resource-group <your-resource-group> \\ --query \"[?contains(name, 'coreca')].properties.configuration.ingress.fqdn\" \\ -o tsv For AKS: # Get the AKS cluster's HTTP routing domain az aks show \\ --resource-group <your-resource-group> \\ --name <your-aks-cluster> \\ --query \"addonProfiles.httpApplicationRouting.config.HTTPApplicationRoutingZoneName\" \\ -o tsv Verification Verify your Core API URL is correct by accessing the status endpoint: curl https://{your-core-api-url}/status Expected response: { \"status\": \"ready\", \"name\": \"CoreAPI\", \"version\": \"x.x.x\" } Or access the Swagger UI (ACA only): https://{your-core-api-url}/swagger/ URL Formats Summary Deployment Format Example ACA Quick Start https://{prefix}coreca.{env}.{region}.azurecontainerapps.io https://fllmaca002coreca.graybush-c554b849.eastus.azurecontainerapps.io ACA Standard https://{prefix}coreca.{env}.{region}.azurecontainerapps.io https://fllmprod-coreca.fllmenv.westus2.azurecontainerapps.io AKS https://{cluster-fqdn}/core https://1cf699fd0d89446eabf2.eastus.aksapp.io/core Troubleshooting URL Returns 404 Verify the URL path is correct (AKS requires /core) Check that the Container App or AKS ingress is running URL Returns 401/403 The API is accessible but requires authentication This confirms the URL is correct Cannot Access URL Check network connectivity Verify the deployment is running For AKS, ensure the ingress controller is configured Related Topics Core API Overview Directly Calling Core API Standard Deployment Local API Access"
  },
  "docs/apis-sdks/apis/core-api/index.html": {
    "href": "docs/apis-sdks/apis/core-api/index.html",
    "title": "Core API | FoundationaLLM",
    "summary": "Core API The Core API is the primary entry point for user-facing interactions with FoundationaLLM. It handles chat completions, session management, file uploads, and agent queries. Overview The Core API provides: Completions: Synchronous and asynchronous chat/completion endpoints Sessions: Conversation management (create, list, delete) Agents: List available agents for the current user Attachments: File upload capabilities Branding: Portal branding configuration Status: Health and status endpoints Base URL Deployment Type URL Pattern Azure Container Apps (ACA) https://{prefix}coreca.{region}.azurecontainerapps.io Azure Kubernetes Service (AKS) https://{cluster-fqdn}/core See Finding Your Core API URL for detailed instructions. Authentication All Core API endpoints require authentication: Method Header Description Entra ID (Azure AD) Authorization: Bearer <token> Standard user authentication Agent Access Token X-AGENT-ACCESS-TOKEN: <token> Token-based agent access API Endpoints Completions Synchronous Completion POST /instances/{instanceId}/completions Content-Type: application/json Request body: { \"user_prompt\": \"What are your capabilities?\", \"session_id\": \"optional-session-id\", \"agent_name\": \"agent-name\", \"settings\": { \"model_parameters\": { \"temperature\": 0.4, \"max_new_tokens\": 1000 }, \"agent_parameters\": { \"index_top_n\": 5 } } } Parameter Type Required Description user_prompt string Yes The user's question or prompt session_id string No Session ID for conversation context agent_name string No Specific agent to use settings object No Override model and agent parameters Response: { \"operation_id\": \"guid\", \"user_prompt\": \"What are your capabilities?\", \"completion\": \"I can help you with...\", \"citations\": [], \"user_prompt_embedding\": [], \"prompt_tokens\": 150, \"completion_tokens\": 200 } Asynchronous Completion For long-running completions, use the async endpoint: POST /instances/{instanceId}/async-completions Content-Type: application/json Returns a LongRunningOperation object with operation status: { \"operation_id\": \"guid\", \"status\": \"InProgress\", \"status_message\": \"Processing request\" } Sessions (Conversations) List Sessions GET /instances/{instanceId}/sessions Create Session POST /instances/{instanceId}/sessions Content-Type: application/json { \"name\": \"New Conversation\" } Get Session Messages GET /instances/{instanceId}/sessions/{sessionId}/messages Delete Session DELETE /instances/{instanceId}/sessions/{sessionId} Rate Message POST /instances/{instanceId}/sessions/{sessionId}/message/{messageId}/rate Content-Type: application/json { \"rating\": true, \"comments\": \"Helpful response\" } Agents List Available Agents GET /instances/{instanceId}/completions/agents Returns agents the current user has access to: [ { \"resource\": { \"name\": \"my-agent\", \"display_name\": \"My Agent\", \"description\": \"Agent description\" }, \"roles\": [\"Owner\"], \"actions\": [\"read\", \"write\", \"delete\"] } ] Status Service Status GET /instances/{instanceId}/status Model Parameters Parameter Type Description temperature float Randomness (0.0-1.0). Lower = more deterministic top_p float Nucleus sampling probability top_k int Top-k filtering max_new_tokens int Maximum tokens in response deployment_name string Specific model deployment do_sample bool Enable sampling vs greedy decoding Agent Parameters Parameter Type Description index_filter_expression string Search filter for index retriever index_top_n int Number of search results for context Error Responses Status Description 400 Bad request (invalid parameters) 401 Unauthorized (invalid/missing token) 403 Forbidden (insufficient permissions) 404 Resource not found 429 Rate limit exceeded (quota) 500 Internal server error Error response body: { \"error\": { \"code\": \"QuotaExceeded\", \"message\": \"Rate limit exceeded. Try again in 60 seconds.\" } } Swagger Documentation Access the interactive API documentation: ACA Deployment: https://{core-api-url}/swagger/ AKS Deployment: https://{aks-url}/core/swagger/v1/swagger.json Related Topics API Reference OpenAI Endpoint Facades Directly Calling Core API Finding Your Core API URL Standard Deployment Local API Access .NET SDK"
  },
  "docs/apis-sdks/apis/core-api/openai-endpoint-facades.html": {
    "href": "docs/apis-sdks/apis/core-api/openai-endpoint-facades.html",
    "title": "OpenAI Model Endpoint Facades | FoundationaLLM",
    "summary": "This article is still being authored. This feature is currently under development. Content will be updated as the feature is finalized. OpenAI Model Endpoint Facades Learn about FoundationaLLM's OpenAI-compatible endpoint facades for simplified model integration. Overview TODO: This feature is under active development. Documentation will be completed when the feature is released. OpenAI Model Endpoint Facades provide an OpenAI-compatible API layer in front of FoundationaLLM's agent capabilities. This allows applications built for the OpenAI API to integrate with FoundationaLLM with minimal code changes. Purpose and Use Cases Why Use Endpoint Facades? Benefit Description Compatibility Use existing OpenAI SDK code Migration Path Gradually move from OpenAI to FoundationaLLM Familiar Interface Standard chat completion format Agent Benefits Access FoundationaLLM's agent capabilities Ideal Scenarios Migrating existing OpenAI-integrated applications Using libraries that expect OpenAI-compatible endpoints Building applications that may switch between providers Teams familiar with OpenAI API patterns Feature Status Component Status Chat Completions endpoint Under Development Embeddings endpoint Under Development Streaming responses Under Development Function calling Under Development Configuration TODO: Document configuration steps once the feature is finalized, including: Enabling endpoint facades in deployment Configuring facade endpoints Mapping facades to agents Authentication configuration Prerequisites FoundationaLLM deployment with endpoint facades enabled Appropriate API credentials configured Agent(s) to expose via the facade Setup Steps TODO: Provide step-by-step setup instructions API Reference Chat Completions Endpoint TODO: Document the endpoint specification Expected Endpoint: POST /v1/chat/completions Expected Request Format: { \"model\": \"agent-name\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"} ], \"temperature\": 0.7, \"max_tokens\": 1000 } Expected Response Format: { \"id\": \"chatcmpl-xxx\", \"object\": \"chat.completion\", \"created\": 1234567890, \"model\": \"agent-name\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Hello! How can I help you today?\" }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 10, \"completion_tokens\": 15, \"total_tokens\": 25 } } Embeddings Endpoint TODO: Document embeddings endpoint if included Usage Examples Python with OpenAI SDK TODO: Verify and complete example from openai import OpenAI # Configure client to use FoundationaLLM facade client = OpenAI( base_url=\"https://{fllm-api}/v1\", api_key=\"your-api-key\" # TODO: Document key format ) # Use standard OpenAI SDK methods response = client.chat.completions.create( model=\"your-agent-name\", # Maps to FLLM agent messages=[ {\"role\": \"user\", \"content\": \"What can you help me with?\"} ] ) print(response.choices[0].message.content) curl Example TODO: Verify and complete example curl -X POST \"https://{fllm-api}/v1/chat/completions\" \\ -H \"Authorization: Bearer your-api-key\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"your-agent-name\", \"messages\": [ {\"role\": \"user\", \"content\": \"Hello!\"} ] }' Mapping to FoundationaLLM Concepts OpenAI Concept FoundationaLLM Equivalent model parameter Agent name messages array Conversation context temperature Model parameter override max_tokens max_new_tokens parameter API Key Agent Access Token or Bearer token Limitations TODO: Document known limitations once feature is finalized Expected Limitations: Not all OpenAI parameters may be supported Some agent features may not map to OpenAI concepts Streaming behavior may differ Rate limits follow FoundationaLLM quotas Migration Guide From OpenAI to FoundationaLLM Facades TODO: Provide migration steps Configure FoundationaLLM endpoint facade Update base URL in your application Map model names to agent names Test and verify functionality Considerations Review agent configurations match expected behavior Test streaming if used Verify token counting differences Update error handling for FLLM-specific errors Troubleshooting Common Issues TODO: Document common issues and solutions Issue Possible Cause Solution 404 Not Found Facade not enabled Enable in configuration Authentication Error Invalid credentials Check API key configuration Model Not Found Agent doesn't exist Verify agent name Related Topics Core API Reference Agent Access Tokens AI Models Configuration"
  },
  "docs/apis-sdks/apis/core-api/standard-deployment-local-api-access.html": {
    "href": "docs/apis-sdks/apis/core-api/standard-deployment-local-api-access.html",
    "title": "Standard Deployment Local API Access | FoundationaLLM",
    "summary": "Standard Deployment Local API Access Standard (AKS) deployments expose backend services internally, preventing direct API access over the public internet. This guide explains how to use kubectl port forwarding to access FoundationaLLM APIs locally during development. Overview In standard deployments: APIs are deployed within Kubernetes and not publicly exposed Access requires port forwarding through kubectl This approach is useful for development and testing Prerequisites Required Tools Tool Description Installation kubectl Kubernetes CLI az aks install-cli kubelogin Azure authentication for kubectl az aks install-cli Azure CLI For authentication Install Azure CLI Setup Steps Install kubectl and kubelogin: az aks install-cli Restart your terminal after installation to update $PATH. Authenticate with Azure: az login Get Kubernetes credentials: az aks get-credentials \\ --name <your-aks-cluster-name> \\ --resource-group <your-resource-group> This stores credentials in $HOME/.kube/config. Port Forwarding Script The following PowerShell script forwards all FoundationaLLM API services to local ports: Service Port Mappings Service Local Port Orchestration API 5000 Gatekeeper API 5001 Agent Hub API 5002 Core API 5003 Data Source Hub API 5004 Gatekeeper Integration API 5005 LangChain API 5006 Management API 5007 Prompt Hub API 5008 Semantic Kernel API 5009 Vectorization API 5010 Forwarding Script Save this script or run from /deploy/standard/scripts/Kubectl-Proxy.ps1: #!/bin/pwsh Set-PSDebug -Trace 0 Set-StrictMode -Version 3.0 $ErrorActionPreference = \"Stop\" $services = @{ \"foundationallm-orchestration-api\" = 5000 \"foundationallm-gatekeeper-api\" = 5001 \"foundationallm-agent-hub-api\" = 5002 \"foundationallm-core-api\" = 5003 \"foundationallm-data-source-hub-api\" = 5004 \"foundationallm-gatekeeper-integration-api\" = 5005 \"foundationallm-langchain-api\" = 5006 \"foundationallm-management-api\" = 5007 \"foundationallm-prompt-hub-api\" = 5008 \"foundationallm-semantic-kernel-api\" = 5009 \"foundationallm-vectorization-api\" = 5010 } $jobIds = @() try { foreach ($servicePortPairing in $services.GetEnumerator()) { Write-Host \"Starting Kubectl Tunnel for $($servicePortPairing.key)\" $job = Start-Job -ScriptBlock ([scriptblock]::Create( \"kubectl port-forward service/$($servicePortPairing.key) $($servicePortPairing.value):80\" )) Write-Host \"Job: $($job.Command)\" $jobIds += $job.Id } Write-Host \"Press any key to kill the Kubernetes tunnels...\" $Host.UI.RawUI.ReadKey(\"NoEcho,IncludeKeyDown\") } catch {} finally { foreach ($jobId in $jobIds) { Write-Host \"Killing $jobId\" Stop-Job -Id $jobId } } Usage Ensure ports 5000-5010 are not in use Run the script: ./Kubectl-Proxy.ps1 The script runs until you press any key Access APIs at http://localhost:{port} Verification Script Verify all APIs are accessible: #!/bin/pwsh Set-PSDebug -Trace 0 Set-StrictMode -Version 3.0 $ErrorActionPreference = \"Stop\" foreach ($servicePort in 5000..5010) { Write-Host \"Testing Port #$servicePort...\" try { $response = Invoke-WebRequest -Uri \"http://localhost:$servicePort/status\" -TimeoutSec 5 Write-Host \" Status: $($response.StatusCode)\" -ForegroundColor Green } catch { Write-Host \" Failed: $_\" -ForegroundColor Red } } Calling Local APIs Once forwarding is active: Core API # Status check curl http://localhost:5003/status # List sessions (with auth) curl http://localhost:5003/instances/{instanceId}/sessions \\ -H \"Authorization: Bearer $TOKEN\" Management API # Status check curl http://localhost:5007/status # List agents (with auth) curl http://localhost:5007/instances/{instanceId}/providers/FoundationaLLM.Agent/agents \\ -H \"Authorization: Bearer $TOKEN\" Troubleshooting \"Unable to connect\" Errors Verify kubectl is authenticated: kubectl get pods Check the service exists: kubectl get services Ensure port is not already in use Connection Drops The forwarding script requires an active terminal Rerun the script if connection drops Node restarts may require script restart Authentication Errors Run az login to refresh Azure credentials Run az aks get-credentials to refresh Kubernetes credentials Verify kubelogin is installed: kubelogin --version Alternative: Single Service Forwarding To forward just one service: # Forward Core API only kubectl port-forward service/foundationallm-core-api 5003:80 # Forward Management API only kubectl port-forward service/foundationallm-management-api 5007:80 Related Topics Core API Overview Finding Your Core API URL Directly Calling Core API"
  },
  "docs/apis-sdks/apis/management-api/api-reference.html": {
    "href": "docs/apis-sdks/apis/management-api/api-reference.html",
    "title": "Management API Reference | FoundationaLLM",
    "summary": "Management API Reference Complete endpoint reference for the FoundationaLLM Management API. Base URL https://{management-api-url}/instances/{instanceId} Authentication All endpoints require Entra ID bearer token: Authorization: Bearer <jwt-token> Resource Operations All resource providers follow a consistent pattern: /instances/{instanceId}/providers/{providerName}/{resourceType}/{resourceName} GET - List Resources GET /instances/{instanceId}/providers/{provider}/{resourceType} Response (200 OK): [ { \"resource\": { \"type\": \"agent\", \"name\": \"my-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\", \"display_name\": \"My Agent\", \"description\": \"Agent description\" }, \"roles\": [\"Owner\"], \"actions\": [\"read\", \"write\", \"delete\"] } ] GET - Get Resource GET /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Response (200 OK): { \"resource\": { \"type\": \"agent\", \"name\": \"my-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\", // Full resource definition }, \"roles\": [\"Owner\"], \"actions\": [\"read\", \"write\", \"delete\"] } POST - Create/Update Resource POST /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Content-Type: application/json { // Resource definition } Response (200 OK): { \"object_id\": \"/instances/{instanceId}/providers/{provider}/{resourceType}/{name}\" } DELETE - Delete Resource DELETE /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Response (200 OK): { \"object_id\": \"/instances/{instanceId}/providers/{provider}/{resourceType}/{name}\", \"deleted\": true } POST - Purge Resource Permanently removes a soft-deleted resource: POST /instances/{instanceId}/providers/{provider}/{resourceType}/{name}/purge Agent Resource Provider Provider: FoundationaLLM.Agent Agents List Agents GET /instances/{instanceId}/providers/FoundationaLLM.Agent/agents Get Agent GET /instances/{instanceId}/providers/FoundationaLLM.Agent/agents/{agentName} Create/Update Agent POST /instances/{instanceId}/providers/FoundationaLLM.Agent/agents/{agentName} Content-Type: application/json { \"type\": \"agent\", \"name\": \"my-agent\", \"display_name\": \"My Agent\", \"description\": \"Agent description\", \"inline_context\": false, \"conversation_history_settings\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper_settings\": { \"use_system_setting\": false, \"options\": [\"ContentSafety\"] }, \"workflow_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/my-workflow\", \"tool_object_ids\": [] } Workflows List Workflows GET /instances/{instanceId}/providers/FoundationaLLM.Agent/workflows Create/Update Workflow POST /instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/{workflowName} Content-Type: application/json { \"type\": \"workflow\", \"name\": \"my-workflow\", \"workflow_type\": \"OpenAIAssistants\", \"workflow_host\": \"LangChain\", \"main_ai_model_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.AIModel/aiModels/gpt-4o\", \"main_prompt_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts/system-prompt\" } Tools List Tools GET /instances/{instanceId}/providers/FoundationaLLM.Agent/tools Create/Update Tool POST /instances/{instanceId}/providers/FoundationaLLM.Agent/tools/{toolName} Content-Type: application/json { \"type\": \"tool\", \"name\": \"knowledge-search\", \"tool_type\": \"knowledge-search\", \"description\": \"Search knowledge base\" } Prompt Resource Provider Provider: FoundationaLLM.Prompt List Prompts GET /instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts Get Prompt GET /instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts/{promptName} Create/Update Prompt POST /instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts/{promptName} Content-Type: application/json { \"type\": \"prompt\", \"name\": \"system-prompt\", \"display_name\": \"System Prompt\", \"description\": \"Main system prompt\", \"category\": \"AgentWorkflow\", \"prefix\": \"You are a helpful assistant...\", \"suffix\": \"\" } Data Source Resource Provider Provider: FoundationaLLM.DataSource List Data Sources GET /instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources Create/Update Data Source POST /instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources/{dataSourceName} Content-Type: application/json { \"type\": \"data-source\", \"name\": \"azure-storage\", \"display_name\": \"Azure Storage\", \"data_source_type\": \"AzureDataLake\", \"configuration\": { \"connection_string_secret_name\": \"storage-connection\", \"containers\": [\"documents\"] } } Data Pipeline Resource Provider Provider: FoundationaLLM.DataPipeline See Data Pipelines API for complete documentation. List Pipelines GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines Execute Pipeline POST /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName}/process List Pipeline Runs GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelineRuns AI Model Resource Provider Provider: FoundationaLLM.AIModel List AI Models GET /instances/{instanceId}/providers/FoundationaLLM.AIModel/aiModels Create/Update AI Model POST /instances/{instanceId}/providers/FoundationaLLM.AIModel/aiModels/{modelName} Content-Type: application/json { \"type\": \"ai-model\", \"name\": \"gpt-4o\", \"display_name\": \"GPT-4o\", \"model_type\": \"completion\", \"deployment_name\": \"gpt-4o-deployment\", \"api_endpoint_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/azure-openai\" } Configuration Resource Provider Provider: FoundationaLLM.Configuration App Configurations List Configurations GET /instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations Update Configuration POST /instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations/{configName} Content-Type: application/json { \"type\": \"app-configuration\", \"name\": \"Branding-CompanyName\", \"key\": \"FoundationaLLM:Branding:CompanyName\", \"value\": \"Contoso\" } API Endpoints List API Endpoints GET /instances/{instanceId}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations Authorization Resource Provider Provider: FoundationaLLM.Authorization Role Assignments List Role Assignments GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments Create Role Assignment POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{assignmentName} Content-Type: application/json { \"type\": \"role-assignment\", \"name\": \"assignment-guid\", \"principal_id\": \"user-or-group-guid\", \"principal_type\": \"User\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/1301f8d4-3bea-4880-945f-315dbd2ddb46\", \"scope\": \"/instances/{instanceId}\" } Role Definitions List Role Definitions GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions Status Endpoints Service Status GET /status Response: { \"status\": \"ready\", \"name\": \"ManagementAPI\", \"version\": \"1.0.0\", \"instance_id\": \"instance-guid\" } Identity Endpoints Get Current User GET /instances/{instanceId}/identity Search Security Principals GET /instances/{instanceId}/identity/securityPrincipals?search={searchTerm} Error Responses Status Code Description 400 BadRequest Invalid request body 401 Unauthorized Authentication required 403 Forbidden Insufficient permissions 404 NotFound Resource not found 409 Conflict Resource already exists 500 InternalError Server error Error Response Format: { \"error\": { \"code\": \"ResourceNotFound\", \"message\": \"The resource 'my-agent' was not found in provider 'FoundationaLLM.Agent'.\" } } Related Topics Management API Overview Resource Providers Overview Data Pipelines API"
  },
  "docs/apis-sdks/apis/management-api/data-pipelines.html": {
    "href": "docs/apis-sdks/apis/management-api/data-pipelines.html",
    "title": "Data Pipelines API | FoundationaLLM",
    "summary": "Data Pipelines API API reference for managing data pipelines programmatically through the Management API. Overview The Data Pipelines API enables: Creating and configuring data pipelines Executing pipeline runs Monitoring pipeline execution history Managing pipeline lifecycle Resource Provider Provider: FoundationaLLM.DataPipeline Resource Types: dataPipelines - Pipeline definitions dataPipelineRuns - Execution history Pipeline Endpoints List Data Pipelines GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines Authorization: Bearer <token> Response (200 OK): [ { \"resource\": { \"type\": \"data-pipeline\", \"name\": \"documents-pipeline\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/documents-pipeline\", \"display_name\": \"Documents Pipeline\", \"description\": \"Processes documents from Azure Storage\", \"active\": true }, \"roles\": [\"Owner\"], \"actions\": [\"read\", \"write\", \"delete\", \"process\"] } ] Get Data Pipeline GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName} Authorization: Bearer <token> Response (200 OK): { \"resource\": { \"type\": \"data-pipeline\", \"name\": \"documents-pipeline\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/documents-pipeline\", \"display_name\": \"Documents Pipeline\", \"description\": \"Processes documents from Azure Storage\", \"active\": true, \"data_source_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources/azure-storage\", \"stages\": [ { \"name\": \"text-extraction\", \"type\": \"TextExtraction\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/text-extractor\", \"parameters\": {}, \"next_stages\": [\"text-partitioning\"] }, { \"name\": \"text-partitioning\", \"type\": \"TextPartitioning\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/text-partitioner\", \"parameters\": { \"chunk_size\": 1000, \"chunk_overlap\": 200 }, \"next_stages\": [\"embedding\"] }, { \"name\": \"embedding\", \"type\": \"Embedding\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/embedder\", \"parameters\": {}, \"next_stages\": [\"indexing\"] }, { \"name\": \"indexing\", \"type\": \"Indexing\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/indexer\", \"parameters\": { \"index_name\": \"documents-index\" }, \"next_stages\": [] } ], \"trigger\": { \"type\": \"Manual\" } }, \"roles\": [\"Owner\"], \"actions\": [\"read\", \"write\", \"delete\", \"process\"] } Create Data Pipeline POST /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName} Content-Type: application/json Authorization: Bearer <token> Request Body: { \"type\": \"data-pipeline\", \"name\": \"my-pipeline\", \"display_name\": \"My Pipeline\", \"description\": \"Pipeline description\", \"active\": true, \"data_source_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources/azure-storage\", \"stages\": [ { \"name\": \"text-extraction\", \"type\": \"TextExtraction\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/text-extractor\", \"parameters\": {}, \"next_stages\": [\"text-partitioning\"] }, { \"name\": \"text-partitioning\", \"type\": \"TextPartitioning\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/text-partitioner\", \"parameters\": { \"chunk_size\": 1000, \"chunk_overlap\": 200 }, \"next_stages\": [\"embedding\"] }, { \"name\": \"embedding\", \"type\": \"Embedding\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/embedder\", \"parameters\": {}, \"next_stages\": [\"indexing\"] }, { \"name\": \"indexing\", \"type\": \"Indexing\", \"plugin_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/indexer\", \"parameters\": { \"index_name\": \"my-index\" }, \"next_stages\": [] } ], \"trigger\": { \"type\": \"Manual\" } } Response (200 OK): { \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/my-pipeline\" } Update Data Pipeline Same endpoint as create - use POST with the updated pipeline definition. Delete Data Pipeline DELETE /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName} Authorization: Bearer <token> Purge Data Pipeline POST /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName}/purge Authorization: Bearer <token> Execute Data Pipeline Trigger a pipeline run: POST /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/{pipelineName}/process Content-Type: application/json Authorization: Bearer <token> Request Body (Optional): { \"run_type\": \"full\", \"parameters\": {} } Response (202 Accepted): { \"run_id\": \"run-guid\", \"status\": \"Pending\", \"message\": \"Pipeline execution started\" } Pipeline Runs Endpoints List Pipeline Runs GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelineRuns Authorization: Bearer <token> Query Parameters: Parameter Type Description pipelineName string Filter by pipeline name status string Filter by status (Pending, Running, Completed, Failed) startTime datetime Filter by start time Response (200 OK): [ { \"resource\": { \"type\": \"data-pipeline-run\", \"name\": \"run-guid\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelineRuns/run-guid\", \"pipeline_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/documents-pipeline\", \"pipeline_name\": \"documents-pipeline\", \"status\": \"Completed\", \"start_time\": \"2024-01-15T10:30:00Z\", \"end_time\": \"2024-01-15T10:45:00Z\", \"documents_processed\": 150, \"documents_failed\": 2 } } ] Get Pipeline Run GET /instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelineRuns/{runId} Authorization: Bearer <token> Response (200 OK): { \"resource\": { \"type\": \"data-pipeline-run\", \"name\": \"run-guid\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelineRuns/run-guid\", \"pipeline_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.DataPipeline/dataPipelines/documents-pipeline\", \"pipeline_name\": \"documents-pipeline\", \"status\": \"Completed\", \"start_time\": \"2024-01-15T10:30:00Z\", \"end_time\": \"2024-01-15T10:45:00Z\", \"documents_processed\": 150, \"documents_failed\": 2, \"stage_results\": [ { \"stage_name\": \"text-extraction\", \"status\": \"Completed\", \"items_processed\": 150, \"items_failed\": 0, \"duration_seconds\": 120 }, { \"stage_name\": \"text-partitioning\", \"status\": \"Completed\", \"items_processed\": 150, \"items_failed\": 0, \"duration_seconds\": 60 } ] } } Pipeline Structure Stage Types Type Description TextExtraction Extract text from documents TextPartitioning Split text into chunks Embedding Generate vector embeddings Indexing Store in vector database ImageDescription Generate descriptions for images Trigger Types Type Description Manual Triggered via API or UI Scheduled Runs on a schedule Event Triggered by events (e.g., new files) Run Status Values Status Description Pending Queued for execution Running Currently executing Completed Finished successfully Failed Execution failed Cancelled Cancelled by user Code Examples Python import requests base_url = \"https://management-api.example.com\" instance_id = \"your-instance-id\" token = \"your-bearer-token\" headers = { \"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\" } # List pipelines response = requests.get( f\"{base_url}/instances/{instance_id}/providers/FoundationaLLM.DataPipeline/dataPipelines\", headers=headers ) pipelines = response.json() # Execute pipeline response = requests.post( f\"{base_url}/instances/{instance_id}/providers/FoundationaLLM.DataPipeline/dataPipelines/my-pipeline/process\", headers=headers, json={\"run_type\": \"full\"} ) run = response.json() print(f\"Started run: {run['run_id']}\") PowerShell $baseUrl = \"https://management-api.example.com\" $instanceId = \"your-instance-id\" $token = \"your-bearer-token\" $headers = @{ \"Authorization\" = \"Bearer $token\" \"Content-Type\" = \"application/json\" } # List pipelines $pipelines = Invoke-RestMethod ` -Uri \"$baseUrl/instances/$instanceId/providers/FoundationaLLM.DataPipeline/dataPipelines\" ` -Headers $headers # Execute pipeline $run = Invoke-RestMethod ` -Uri \"$baseUrl/instances/$instanceId/providers/FoundationaLLM.DataPipeline/dataPipelines/my-pipeline/process\" ` -Method Post ` -Headers $headers ` -Body '{\"run_type\": \"full\"}' Write-Host \"Started run: $($run.run_id)\" Related Topics Management API Overview Creating Data Pipelines Monitoring Data Pipelines Data Pipelines Concepts"
  },
  "docs/apis-sdks/apis/management-api/directly-calling-management-api.html": {
    "href": "docs/apis-sdks/apis/management-api/directly-calling-management-api.html",
    "title": "Directly Calling the Management API | FoundationaLLM",
    "summary": "Directly Calling the Management API This guide provides step-by-step instructions for configuring Postman and making direct calls to the FoundationaLLM Management API. Overview The Management API enables programmatic management of FoundationaLLM resources: Agents, workflows, and tools Prompts Data sources and pipelines AI models and API endpoints Branding and configuration Role assignments Prerequisites Before calling the Management API, you need: Management API URL - From App Config: FoundationaLLM:APIs:ManagementAPI:APIUrl Instance ID - From App Config: FoundationaLLM:Instance:Id Entra ID App Registration - Management Portal client app Using Postman Install Postman Download and install Postman. Import the Collection Click the button below to fork the official FoundationaLLM Management API collection: Configure Variables Select the FoundationaLLM.Management.API collection Click the Variables tab Update these Current value fields: Variable Description Where to Find baseUrl Management API URL App Config: FoundationaLLM:APIs:ManagementAPI:APIUrl instanceId FoundationaLLM instance GUID App Config: FoundationaLLM:Instance:Id tenantId Azure AD tenant ID Entra ID portal appClientId Management Portal client ID Entra ID app registration appScope API scope Entra ID app registration Click Save Configure Authentication Important: Add https://oauth.pstmn.io/v1/callback as a Redirect URI in your Management Portal Entra ID app registration. Select the Authorization tab in the collection Verify the OAuth 2.0 settings use the collection variables Scroll down and click Get New Access Token Complete the login flow Click Use Token Click Save Make Your First Request Expand the collection and find Get Agents Click Send Verify a 200 OK response Using curl Get Authentication Token # Login to Azure az login # Get token for Management API TOKEN=$(az account get-access-token \\ --resource api://{management-api-client-id} \\ --query accessToken -o tsv) List Agents curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents\" \\ -H \"Authorization: Bearer $TOKEN\" Get Specific Agent curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\" \\ -H \"Authorization: Bearer $TOKEN\" Create Agent curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/new-agent\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"agent\", \"name\": \"new-agent\", \"display_name\": \"New Agent\", \"description\": \"A new agent\", \"inline_context\": false, \"conversation_history_settings\": { \"enabled\": true, \"max_history\": 5 } }' Delete Agent curl -X DELETE \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\" \\ -H \"Authorization: Bearer $TOKEN\" Purge Agent (Permanent Delete) curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent/purge\" \\ -H \"Authorization: Bearer $TOKEN\" Common Operations Managing Prompts List Prompts: curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts\" \\ -H \"Authorization: Bearer $TOKEN\" Create Prompt: curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts/system-prompt\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"prompt\", \"name\": \"system-prompt\", \"display_name\": \"System Prompt\", \"category\": \"AgentWorkflow\", \"prefix\": \"You are a helpful assistant specialized in...\" }' Managing Data Sources List Data Sources: curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources\" \\ -H \"Authorization: Bearer $TOKEN\" Create Azure Data Lake Source: curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.DataSource/dataSources/my-storage\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"data-source\", \"name\": \"my-storage\", \"display_name\": \"My Azure Storage\", \"data_source_type\": \"AzureDataLake\", \"configuration\": { \"authentication_type\": \"ManagedIdentity\", \"endpoint\": \"https://mystorageaccount.blob.core.windows.net\" } }' Managing Branding Get Branding Settings: curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations\" \\ -H \"Authorization: Bearer $TOKEN\" Update Branding: curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations/Branding-CompanyName\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"app-configuration\", \"name\": \"Branding-CompanyName\", \"key\": \"FoundationaLLM:Branding:CompanyName\", \"value\": \"My Company\" }' Managing Role Assignments List Role Assignments: curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments\" \\ -H \"Authorization: Bearer $TOKEN\" Create Role Assignment: curl -X POST \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/new-assignment\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"role-assignment\", \"name\": \"new-assignment\", \"principal_id\": \"user-guid\", \"principal_type\": \"User\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"scope\": \"/instances/{instanceId}\" }' Swagger UI For interactive API exploration: Deployment URL ACA https://{management-api-url}/swagger/ AKS https://{aks-url}/management/swagger/v1/swagger.json Error Handling Status Meaning Action 400 Invalid request Check JSON format and required fields 401 Unauthorized Refresh authentication token 403 Forbidden Check role permissions 404 Not found Verify resource path and name 409 Conflict Resource already exists 500 Server error Check API logs Tips Use purge carefully - Purged resources cannot be recovered Check permissions - Some operations require Owner or Contributor role Validate JSON - API returns 400 for malformed requests Use instance ID - All resource paths require the instance ID Related Topics Management API Overview API Reference Resource Providers Overview .NET SDK"
  },
  "docs/apis-sdks/apis/management-api/index.html": {
    "href": "docs/apis-sdks/apis/management-api/index.html",
    "title": "Management API | FoundationaLLM",
    "summary": "Management API The Management API provides programmatic access to create, update, and manage FoundationaLLM resources. While the Core API handles user-facing chat interactions, the Management API handles administrative operations. Overview The Management API enables: Resource Management: Create, read, update, delete agents, prompts, data sources, pipelines Configuration: Manage branding, app settings, API endpoints Authorization: Manage role assignments and access control Monitoring: Check API health and service status Comparison: Core API vs Management API Aspect Core API Management API Purpose User interactions Administrative operations Primary Users End users, applications Administrators, CI/CD pipelines Operations Chat completions, sessions Resource CRUD, configuration Data Access Read-only resources Full CRUD on resources Base URL Deployment Type URL Pattern Azure Container Apps (ACA) https://{prefix}managementca.{region}.azurecontainerapps.io Azure Kubernetes Service (AKS) https://{cluster-fqdn}/management Find your URL in App Configuration: FoundationaLLM:APIs:ManagementAPI:APIUrl Authentication All Management API endpoints require Entra ID authentication: Authorization: Bearer <entra-id-token> API Architecture The Management API uses a resource provider pattern similar to Azure Resource Manager: /instances/{instanceId}/providers/{resourceProvider}/{resourceType}/{resourceName} Resource Providers Provider Description Resource Types FoundationaLLM.Agent Agent management agents, workflows, tools FoundationaLLM.Prompt Prompt templates prompts FoundationaLLM.DataSource Data connections dataSources FoundationaLLM.DataPipeline Data pipelines dataPipelines, dataPipelineRuns FoundationaLLM.AIModel AI model configs aiModels FoundationaLLM.Configuration Platform settings appConfigurations, apiEndpoints FoundationaLLM.Authorization Access control roleAssignments, roleDefinitions FoundationaLLM.Plugin Plugin system plugins, pluginPackages See Resource Providers Overview for complete details. Common Operations List Resources GET /instances/{instanceId}/providers/{provider}/{resourceType} Example - List all agents: curl -X GET \\ \"https://{management-api}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents\" \\ -H \"Authorization: Bearer $TOKEN\" Get Resource GET /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Create/Update Resource POST /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Content-Type: application/json { // resource definition } Delete Resource DELETE /instances/{instanceId}/providers/{provider}/{resourceType}/{name} Purge Resource (Permanent Delete) POST /instances/{instanceId}/providers/{provider}/{resourceType}/{name}/purge Access Methods 1. Direct REST Calls Use tools like Postman, curl, or HTTP clients. See Directly Calling Management API. 2. Management Portal The Management Portal UI consumes the Management API internally. All portal operations translate to API calls. 3. .NET SDK Use the FoundationaLLM.Client.Management NuGet package: var managementClient = new ManagementClient(apiUrl, credential, instanceId); var agents = await managementClient.Agents.GetAgentsAsync(); See .NET SDK Documentation. 4. CLI (Coming Soon) A command-line interface for Management API operations is planned. Key Features Resource Lifecycle The Management API supports full resource lifecycle: Create: Define new resources Read: Retrieve resource configurations Update: Modify existing resources Delete: Soft delete (recoverable) Purge: Permanent deletion Validation Resources are validated on create/update: Required fields Name uniqueness Reference integrity Schema compliance Auditing All operations are logged with: User identity Timestamp Operation type Resource affected Swagger Documentation Access interactive API documentation: Deployment URL ACA https://{management-api-url}/swagger/ AKS https://{aks-url}/management/swagger/v1/swagger.json Error Responses Status Description 400 Bad request (validation error) 401 Unauthorized 403 Forbidden (insufficient permissions) 404 Resource not found 409 Conflict (resource already exists) 500 Internal server error Error response format: { \"error\": { \"code\": \"ResourceNotFound\", \"message\": \"The agent 'my-agent' was not found.\" } } Related Topics API Reference Directly Calling Management API Resource Providers Overview Data Pipelines API .NET SDK"
  },
  "docs/apis-sdks/apis/management-api/resource-providers-overview.html": {
    "href": "docs/apis-sdks/apis/management-api/resource-providers-overview.html",
    "title": "Resource Providers Overview | FoundationaLLM",
    "summary": "Resource Providers Overview Resource providers are the core platform components that manage specific types of resources in FoundationaLLM. This pattern mirrors Azure Resource Manager, providing a consistent API for all resource operations. Overview Each resource provider: Manages a specific category of resources Exposes a consistent REST API pattern Enforces authorization through RBAC Handles resource lifecycle (create, read, update, delete, purge) API Pattern All resource providers follow this URL structure: /instances/{instanceId}/providers/{providerName}/{resourceType}/{resourceName} Component Description Example instanceId FoundationaLLM instance GUID abc123-def456-... providerName Resource provider namespace FoundationaLLM.Agent resourceType Type of resource agents resourceName Resource identifier my-agent Available Resource Providers FoundationaLLM.Agent Manages AI agents, their workflows, and tools. Resource Type Description agents Agent configurations workflows Orchestration workflows tools Agent tools (code interpreter, knowledge search, etc.) accessTokens Agent access tokens for unauthenticated access Example Endpoints: GET /instances/{id}/providers/FoundationaLLM.Agent/agents GET /instances/{id}/providers/FoundationaLLM.Agent/agents/{name} POST /instances/{id}/providers/FoundationaLLM.Agent/agents/{name} DELETE /instances/{id}/providers/FoundationaLLM.Agent/agents/{name} FoundationaLLM.AIModel Manages AI model configurations and deployments. Resource Type Description aiModels AI model definitions (GPT-4, Claude, etc.) AI Model Types: Type Description completion Text generation models embedding Embedding/vectorization models image Image generation models (DALL-E) FoundationaLLM.Attachment Manages file attachments uploaded through the portal. Resource Type Description attachments User-uploaded files FoundationaLLM.Authorization Manages role-based access control. Resource Type Description roleAssignments Bindings between principals, roles, and scopes roleDefinitions Available role definitions (Owner, Contributor, Reader, etc.) Built-in Roles: Role ID Owner 1301f8d4-3bea-4880-945f-315dbd2ddb46 Contributor a9f0020f-6e3a-49bf-8d1d-35fd53058edf Reader 00a53e72-f66e-4c03-8f81-7e885fd2eb35 User Access Administrator fb8e0fd0-f7e2-4957-89d6-19f44f7d6618 FoundationaLLM.Configuration Manages platform configuration and API endpoints. Resource Type Description appConfigurations App Configuration key-value settings apiEndpointConfigurations External API endpoint definitions Common Configuration Categories: Branding settings (FoundationaLLM:Branding:*) API endpoints (FoundationaLLM:APIs:*) Feature flags (FoundationaLLM:Features:*) FoundationaLLM.Conversation Manages user conversations and messages. Resource Type Description conversations Chat sessions/conversations Note: Conversations are typically managed through the Core API. The Management API provides administrative access. FoundationaLLM.DataPipeline Manages data ingestion and processing pipelines. Resource Type Description dataPipelines Pipeline definitions dataPipelineRuns Pipeline execution history See Data Pipelines API for detailed documentation. FoundationaLLM.DataSource Manages connections to external data repositories. Resource Type Description dataSources Data source configurations Supported Data Source Types: Type Description AzureDataLake Azure Blob Storage / Data Lake Gen2 OneLake Microsoft Fabric OneLake SharePointOnline SharePoint document libraries AzureSQLDatabase Azure SQL Database Web Web page sources FoundationaLLM.Plugin Manages the plugin system for extensibility. Resource Type Description pluginPackages Plugin package definitions plugins Individual plugin configurations Plugin Types: Type Description AgentWorkflow Custom workflow implementations AgentTool Custom agent tools DataSource Custom data source connectors DataPipelineStage Custom pipeline processing stages FoundationaLLM.Prompt Manages reusable prompt templates. Resource Type Description prompts Prompt template definitions Prompt Categories: Category Use Case AgentWorkflow Agent system prompts AgentTool Tool-specific instructions DataPipeline Pipeline processing instructions FoundationaLLM.Vector Manages vector database configurations. Resource Type Description vectorDatabases Vector database connection definitions Resource Object IDs Every resource has a unique object ID following this format: /instances/{instanceId}/providers/{providerName}/{resourceType}/{resourceName} Object IDs are used for: Cross-resource references Authorization scopes Resource identification Example: { \"object_id\": \"/instances/abc123/providers/FoundationaLLM.Agent/agents/my-agent\", \"workflow_object_id\": \"/instances/abc123/providers/FoundationaLLM.Agent/workflows/my-workflow\" } Standard Operations All resource providers support these operations: Operation HTTP Method Description List GET /{resourceType} List all resources Get GET /{resourceType}/{name} Get specific resource Create/Update POST /{resourceType}/{name} Create or update resource Delete DELETE /{resourceType}/{name} Soft delete resource Purge POST /{resourceType}/{name}/purge Permanent delete Response Format All GET operations return resources with metadata: { \"resource\": { \"type\": \"agent\", \"name\": \"my-agent\", \"object_id\": \"/instances/{id}/providers/FoundationaLLM.Agent/agents/my-agent\", // Resource-specific properties }, \"roles\": [\"Owner\", \"Contributor\"], \"actions\": [\"read\", \"write\", \"delete\"] } Field Description resource The resource definition roles User's assigned roles for this resource actions Actions the user can perform Related Topics Management API Overview API Reference Permissions & Roles Reference"
  },
  "docs/apis-sdks/sdks/dotnet/index.html": {
    "href": "docs/apis-sdks/sdks/dotnet/index.html",
    "title": ".NET SDK | FoundationaLLM",
    "summary": ".NET SDK The FoundationaLLM .NET SDK provides client libraries for integrating with FoundationaLLM APIs in .NET applications. Overview The SDK includes two NuGet packages: Package Description NuGet FoundationaLLM.Client.Core Core API client for completions, sessions, agents NuGet FoundationaLLM.Client.Management Management API client for resource management NuGet Installation # Core API client dotnet add package FoundationaLLM.Client.Core # Management API client dotnet add package FoundationaLLM.Client.Management Client Architecture Each package provides two client classes: Client Type Description RESTClient Low-level client with direct access to all API endpoints Client High-level client with simplified, user-friendly methods Choose based on your needs: RESTClient for complete API access, Client for simpler common operations. Core Client Quick Start using FoundationaLLM.Client.Core; using Azure.Identity; // Configuration var coreApiUrl = \"https://your-core-api.azurecontainerapps.io\"; var instanceId = \"your-instance-guid\"; var credential = new AzureCliCredential(); // or ManagedIdentityCredential for production // Create client var coreClient = new CoreClient(coreApiUrl, credential, instanceId); // Get available agents var agents = await coreClient.GetAgentsAsync(); // Request a completion var response = await coreClient.GetCompletionAsync( userPrompt: \"What can you help me with?\", agentName: \"default-agent\" ); Console.WriteLine(response.Completion); Using Dependency Injection 1. Configuration file (appsettings.json): { \"FoundationaLLM\": { \"APIEndpoints\": { \"CoreAPI\": { \"Essentials\": { \"APIUrl\": \"https://your-core-api.azurecontainerapps.io\" } } }, \"Instance\": { \"Id\": \"your-instance-guid\" } } } 2. Service registration: using FoundationaLLM.Client.Core; using FoundationaLLM.Common.Constants.Configuration; var builder = WebApplication.CreateBuilder(args); var credential = new DefaultAzureCredential(); // Register Core clients builder.Services.AddCoreClient( builder.Configuration[AppConfigurationKeys.FoundationaLLM_APIEndpoints_CoreAPI_Essentials_APIUrl]!, credential, builder.Configuration[AppConfigurationKeys.FoundationaLLM_Instance_Id]! ); 3. Inject and use: public class ChatService { private readonly ICoreClient _coreClient; public ChatService(ICoreClient coreClient) { _coreClient = coreClient; } public async Task<string> GetResponseAsync(string question) { var response = await _coreClient.GetCompletionAsync(question); return response.Completion; } } Core Client Methods Method Description GetAgentsAsync() List available agents GetCompletionAsync(prompt, agent) Request a completion GetChatCompletionAsync(prompt, sessionId, agent) Chat completion with session context Core REST Client Methods Client Method Description Sessions GetSessionsAsync() List all sessions Sessions CreateSessionAsync(name) Create new session Sessions GetMessagesAsync(sessionId) Get session messages Sessions DeleteSessionAsync(sessionId) Delete session Completions GetCompletionAsync(request) Request completion Completions StartCompletionOperationAsync(request) Async completion Status GetServiceStatusAsync() Check API status Branding GetBrandingAsync() Get branding config Management Client Quick Start using FoundationaLLM.Client.Management; using Azure.Identity; // Configuration var managementApiUrl = \"https://your-management-api.azurecontainerapps.io\"; var instanceId = \"your-instance-guid\"; var credential = new AzureCliCredential(); // Create client var managementClient = new ManagementClient(managementApiUrl, credential, instanceId); // List agents var agents = await managementClient.Agents.GetAgentsAsync(); // Create a prompt await managementClient.Prompts.CreatePromptAsync(new PromptRequest { Name = \"my-prompt\", DisplayName = \"My Prompt\", Category = \"AgentWorkflow\", Prefix = \"You are a helpful assistant...\" }); // Delete and purge a data source await managementClient.DataSources.DeleteDataSourceAsync(\"old-source\"); await managementClient.DataSources.PurgeDataSourceAsync(\"old-source\"); Using Dependency Injection 1. Configuration file (appsettings.json): { \"FoundationaLLM\": { \"APIEndpoints\": { \"ManagementAPI\": { \"Essentials\": { \"APIUrl\": \"https://your-management-api.azurecontainerapps.io\" } } }, \"Instance\": { \"Id\": \"your-instance-guid\" } } } 2. Service registration: using FoundationaLLM.Client.Management; builder.Services.AddManagementClient( builder.Configuration[AppConfigurationKeys.FoundationaLLM_APIEndpoints_ManagementAPI_Essentials_APIUrl]!, credential, builder.Configuration[AppConfigurationKeys.FoundationaLLM_Instance_Id]! ); 3. Inject and use: public class AgentService { private readonly IManagementClient _managementClient; public AgentService(IManagementClient managementClient) { _managementClient = managementClient; } public async Task<IEnumerable<Agent>> GetAllAgentsAsync() { return await _managementClient.Agents.GetAgentsAsync(); } } Management Client Interfaces Interface Purpose IAgentManagementClient Manage agents IPromptManagementClient Manage prompts IDataSourceManagementClient Manage data sources IAIModelManagementClient Manage AI models IConfigurationManagementClient Manage configuration IVectorizationManagementClient Manage vectorization profiles IAttachmentManagementClient Manage attachments Azure App Configuration Integration Load configuration from Azure App Configuration: var configuration = new ConfigurationBuilder() .AddJsonFile(\"appsettings.json\", optional: true) .AddEnvironmentVariables() .AddAzureAppConfiguration(options => { options.Connect(Environment.GetEnvironmentVariable(\"FLLM_AppConfig_ConnectionString\")); options.ConfigureKeyVault(kv => kv.SetCredential(credential)); options.Select(AppConfigurationKeyFilters.FoundationaLLM_Instance); options.Select(AppConfigurationKeyFilters.FoundationaLLM_APIEndpoints_CoreAPI_Essentials); options.Select(AppConfigurationKeyFilters.FoundationaLLM_APIEndpoints_ManagementAPI_Essentials); }) .Build(); Authentication Development (Azure CLI) var credential = new AzureCliCredential(); Production (Managed Identity) var credential = new ManagedIdentityCredential(); Using DefaultAuthentication Helper using FoundationaLLM.Common.Authentication; // Initialize for environment DefaultAuthentication.Initialize(isProduction: false, serviceName: \"MyApp\"); var credential = DefaultAuthentication.AzureCredential; Configuration Options var options = new APIClientSettings { Timeout = TimeSpan.FromSeconds(600) // Default: 900 seconds }; var coreClient = new CoreClient(apiUrl, credential, instanceId, options); Error Handling try { var response = await coreClient.GetCompletionAsync(\"What is FoundationaLLM?\"); } catch (HttpRequestException ex) when (ex.StatusCode == HttpStatusCode.Unauthorized) { // Handle authentication error Console.WriteLine(\"Authentication failed. Check your credentials.\"); } catch (HttpRequestException ex) when (ex.StatusCode == HttpStatusCode.TooManyRequests) { // Handle rate limiting Console.WriteLine(\"Rate limit exceeded. Retry later.\"); } catch (Exception ex) { Console.WriteLine($\"Error: {ex.Message}\"); } Examples The Core.Examples test project in the FoundationaLLM repository contains comprehensive examples demonstrating: Session management Completion requests Agent configuration Data source management Vectorization workflows Related Topics Core API Reference Management API Reference Python SDK"
  },
  "docs/apis-sdks/sdks/python/index.html": {
    "href": "docs/apis-sdks/sdks/python/index.html",
    "title": "Python SDK | FoundationaLLM",
    "summary": "Python SDK The FoundationaLLM Python SDK provides integration capabilities for Python applications. Overview The foundationallm Python package provides: Internal orchestration components (LangChain integration) Telemetry and logging utilities Configuration management Integration with FoundationaLLM platform services Installation pip install foundationallm Or from source: cd src/python/PythonSDK pip install -e . Package Structure foundationallm/ ├── config/ # Configuration management ├── hubs/ # Hub integrations (Agent, Prompt, Data Source) ├── langchain/ # LangChain orchestration components │ ├── agents/ # Agent implementations │ ├── data_sources/ # Data source connectors │ ├── message_history/ # Conversation history │ └── orchestration/ # Orchestration managers ├── models/ # Data models ├── plugins/ # Plugin system ├── storage/ # Storage integrations ├── telemetry/ # OpenTelemetry integration └── utils/ # Utility functions Configuration Environment Variables Variable Description FOUNDATIONALLM_APP_CONFIG_CONNECTION_STRING Azure App Configuration connection string FOUNDATIONALLM_INSTANCE_ID FoundationaLLM instance identifier APPLICATIONINSIGHTS_CONNECTION_STRING Application Insights telemetry Configuration Manager from foundationallm.config import Configuration # Initialize from environment config = Configuration() # Access configuration values api_url = config.get_value(\"FoundationaLLM:APIs:CoreAPI:APIUrl\") instance_id = config.get_value(\"FoundationaLLM:Instance:Id\") LangChain Integration The SDK provides LangChain components for building orchestration workflows. Orchestration Manager from foundationallm.langchain.orchestration import OrchestrationManager # Initialize orchestration orchestration = OrchestrationManager( config=config, agent_config=agent_configuration ) # Process a completion request result = await orchestration.run( user_prompt=\"What is FoundationaLLM?\", message_history=[] ) Message History from foundationallm.langchain.message_history import MessageHistoryManager # Manage conversation history history_manager = MessageHistoryManager( session_id=\"session-guid\", max_messages=10 ) # Add messages history_manager.add_user_message(\"What is AI?\") history_manager.add_assistant_message(\"AI stands for Artificial Intelligence...\") # Get history for context messages = history_manager.get_messages() Telemetry The SDK integrates with OpenTelemetry for distributed tracing: from foundationallm.telemetry import Telemetry # Initialize telemetry telemetry = Telemetry( service_name=\"my-service\", connection_string=app_insights_connection ) # Create spans with telemetry.create_span(\"process_request\") as span: span.set_attribute(\"user_id\", user_id) # Process request Plugin Development Create custom plugins for FoundationaLLM: Plugin Structure my_plugin/ ├── __init__.py ├── plugin.py ├── requirements.txt └── plugin.json Plugin Manifest (plugin.json) { \"name\": \"my-custom-plugin\", \"version\": \"1.0.0\", \"type\": \"AgentTool\", \"description\": \"Custom tool plugin\", \"entry_point\": \"plugin:MyCustomTool\" } Plugin Implementation from foundationallm.plugins import PluginBase class MyCustomTool(PluginBase): \"\"\"Custom tool implementation.\"\"\" def __init__(self, config): super().__init__(config) async def execute(self, parameters): \"\"\"Execute the tool.\"\"\" # Implementation return result REST API Client For direct API calls, use standard HTTP libraries: import httpx from azure.identity import DefaultAzureCredential # Get authentication token credential = DefaultAzureCredential() token = credential.get_token(\"api://your-api-client-id/.default\") # Call Core API async with httpx.AsyncClient() as client: response = await client.post( f\"{core_api_url}/instances/{instance_id}/completions\", headers={ \"Authorization\": f\"Bearer {token.token}\", \"Content-Type\": \"application/json\" }, json={ \"user_prompt\": \"Hello, what can you do?\", \"agent_name\": \"default-agent\" } ) result = response.json() print(result[\"completion\"]) Usage in FoundationaLLM Services The Python SDK powers several FoundationaLLM services: Service Description LangChainAPI LangChain-based orchestration service GatekeeperIntegrationAPI Content safety integrations Data Pipeline Workers Pipeline processing components Development Setup Development Environment # Create virtual environment python -m venv venv source venv/bin/activate # Linux/macOS # or: venv\\Scripts\\activate # Windows # Install dependencies pip install -r requirements.txt # Install package in development mode pip install -e . Running Tests pytest tests/ Requirements Python 3.11+ See requirements.txt for dependencies Related Topics Core API Reference Management API Reference .NET SDK Plugin Development"
  },
  "docs/archive/README.html": {
    "href": "docs/archive/README.html",
    "title": "Archived Documentation | FoundationaLLM",
    "summary": "Archived Documentation This folder contains documentation that has been superseded by the reorganized documentation structure. Contents Folder Original Location Archived Date Reason api/ docs/api/ 2024-12-14 Moved to apis-sdks/ concepts/ docs/concepts/ 2024-12-14 Moved to management-portal/reference/concepts/ deployment/ docs/deployment/ 2024-12-14 Moved to platform-operations/deployment/ how-to-guides/ docs/how-to-guides/ 2024-12-14 Moved to management-portal/how-to-guides/ operations/ docs/operations/ 2024-12-14 Moved to platform-operations/ role-based-access-control/ docs/role-based-access-control/ 2024-12-14 Moved to platform-operations/security-permissions/ setup-guides/ docs/setup-guides/ 2024-12-14 Distributed to new sections vectorization/ docs/setup-guides/vectorization/ 2024-12-14 Feature deprecated (replaced by Data Pipelines) knowledge-management-agent.md docs/setup-guides/agents/ 2024-12-14 Obsolete agent type New Documentation Locations API Documentation Old: api/ → New: apis-sdks/apis/ and apis-sdks/sdks/ Concepts Old: concepts/ → New: management-portal/reference/concepts/ Deployment Old: deployment/ → New: platform-operations/deployment/ How-To Guides Old: how-to-guides/ → New: management-portal/how-to-guides/ Operations Old: operations/ → New: platform-operations/ Role-Based Access Control Old: role-based-access-control/ → New: platform-operations/security-permissions/role-based-access-control/ Setup Guides Agents → management-portal/reference/concepts/ Branding → management-portal/reference/branding/ Exposed APIs → apis-sdks/ Management UI → management-portal/ Quickstart → chat-user-portal/quick-start/ Vectorization → Archived (deprecated) Notes This content is kept for historical reference only Do not link to these files from the main documentation For current documentation, refer to the corresponding files in the new documentation structure Consider removing archived content after sufficient time has passed"
  },
  "docs/archive/api/dotnet/index.html": {
    "href": "docs/archive/api/dotnet/index.html",
    "title": ".NET API | FoundationaLLM",
    "summary": ".NET API Use the navigation on the left to browse the .NET API documentation."
  },
  "docs/archive/api/index.html": {
    "href": "docs/archive/api/index.html",
    "title": "API documentation | FoundationaLLM",
    "summary": "API documentation FoundationaLLM has both .NET and Python and Python APIs: .NET API Python API"
  },
  "docs/archive/api/python/index.html": {
    "href": "docs/archive/api/python/index.html",
    "title": "Python API | FoundationaLLM",
    "summary": "Python API Use the navigation on the left to browse the Python API documentation."
  },
  "docs/archive/concepts/data-pipeline/data-pipeline.html": {
    "href": "docs/archive/concepts/data-pipeline/data-pipeline.html",
    "title": "Data Pipelines | FoundationaLLM",
    "summary": "Data Pipelines Overview The following diagram illustrates the high level structure of a FoundationaLLM data pipeline: Data pipelines have three main components: Data Pipeline Data Source: The source of the data that will be processed by the pipeline. Data Pipeline Stages: The stages that the data will go through in the pipeline. The stages are structure as a forest, with one or more starting stages. Data Pipeline Trigger: The trigger that will start the pipeline execution. Trigger can be a schedule, an event (e.g., new content added to the data source), or a manual action. The data source and the stages define parameters that are required for the pipeline execution. The parameters metadata is provided by the plugin that implements that particular data source or stage. Parameters can have default values, which can be overridden when the pipeline is executed. Note Triggers of type Schedule or Event must provide a complete set of parameter values. This is because the trigger will use these values to start the pipeline execution and there is no user interaction to provide missing values. In the case of a manual trigger, the user will be prompted to provide the missing values. Here is an example of a simple data pipeline that reads data from an Azure Data Lake storage account, extracts text from files, partitions the text into chunks of a certain size, embeds the chunks and deposits them into an Azure AI Search index: { \"type\": \"data-pipeline\", \"name\": \"DataPipeline01\", \"display_name\": \"Data Pipeline 01\", \"description\": \"Data Pipeline demo.\", \"active\": false, \"data_source\": { \"name\": \"VGDataLake\", \"description\": \"Victorious Ground data lake storage.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/AzureDataLakeDataSource\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"Folders\", \"type\": \"array\", \"description\": \"A list of strings defining data lake folders.\" }, \"default_value\": null } ], \"data_source_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/alchemy\" }, \"starting_stages\": [ { \"name\": \"Extract\", \"description\": \"Extract text from binary content items.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TextExtractionDataPipelineStage\", \"plugin_parameters\": null, \"next_stages\": [ { \"name\": \"Partition\", \"description\": \"Partition text into chunks.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TextPartitioningDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"PartitioningStrategy\", \"type\": \"string\", \"description\": \"The partitioning strategy to be used (can be Token or Semantic).\" }, \"default_value\": \"Token\" } ], \"plugin_dependencies\": [ { \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TokenContentTextPartitioning\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"PartitionSizeTokens\", \"type\": \"int\", \"description\": \"The size in tokens of the text partitions.\" }, \"default_value\": 400 }, { \"parameter_metadata\": { \"name\": \"PartitionOverlapTokens\", \"type\": \"int\", \"description\": \"The size in tokens of text partitions overlap.\" }, \"default_value\": 100 } ] } ], \"next_stages\": [ { \"name\": \"Embed\", \"description\": \"Embed chunks of text.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/GatewayTextEmbeddingDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"EmbeddingModel\", \"type\": \"string\", \"description\": \"The embedding model used for embedding.\" }, \"default_value\": \"text-embedding-3-large\" }, { \"parameter_metadata\": { \"name\": \"EmbeddingDimensions\", \"type\": \"int\", \"description\": \"The number of dimensions used for embedding.\" }, \"default_value\": 2048 } ], \"next_stages\": [ { \"name\": \"Index\", \"description\": \"Persist embeddings to a vector store.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/AzureAISearchIndexingDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"APIEndpointConfigurationObjectId\", \"type\": \"resource-object-id\", \"description\": \"The FoundationaLLM resource object identifier of the API Endpoint Configuration resource that represents the target Azure AI Search instance.\" }, \"default_value\": \"instances/{{instanceId}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\" }, { \"parameter_metadata\": { \"name\": \"IndexName\", \"type\": \"string\", \"description\": \"The name of the Azure AI Search index.\" }, \"default_value\": \"demo-index\" }, { \"parameter_metadata\": { \"name\": \"IndexPartitionName\", \"type\": \"string\", \"description\": \"The name of the index partition (to be added as a metadata entry and used for logical separation within a given physical index.\" }, \"default_value\": \"Dune\" } ] } ] } ] } ] } ], \"triggers\": [ { \"name\": \"Default pipeline schedule\", \"trigger_type\": \"Schedule\", \"trigger_cron_schedule\": \"0 6 * * *\", \"parameter_values\": { \"DataSource.VGDataLake.Folders\": [ \"vectorization-input/Dune\" ], \"Stage.Partition.PartitioningStrategy\": \"Token\", \"Stage.Partition.Dependency.TokenContentTextPartitioning.PartitionSizeTokens\": 400, \"Stage.Partition.Dependency.TokenContentTextPartitioning.PartitionOverlapTokens\": 100, \"Stage.Embed.EmbeddingModel\": \"text-embedding-3-large\", \"Stage.Embed.EmbeddingDimensions\": 2048, \"Stage.Index.APIEndpointConfigurationObjectId\": \"instances/{{instanceId}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\", \"Stage.Index.IndexName\": \"demo-index\", \"Stage.Index.IndexPartitionName\": \"Dune\" } } ] } The most important rules that govern the structure of data pipelines are: Both the data source and the stages must specify the configuration of the plugins that implement them. This is done by using the following properties: plugin_object_id: The FoundationaLLM resource identifier of the plugin that implements the data source or stage. plugin_parameters: The parameters that are required by the plugin. The parameters are defined by the plugin and can have default values. The values in the parameter_metadata are provided by the plugin definition. For more details about plugin parameters, see Plugins. plugin_dependencies: The dependencies that the plugin has. Dependencies are other plugins that the plugin requires to function properly. Dependencies can have their own parameters. The metadata required to select dependency plugins is provided by the plugin definition. For more details about plugin dependencies, see Plugins. The active flag indicates whether the pipeline can be triggered or not. If the pipeline is not active, it will not be executed. The data source must specify the FoundationaLLM resource identifier of the data source that it represents. This is done by using the data_source_object_id property. The data source object must be compatible with the pluging specified by the plugin_object_id property. The data source name and the stage names must be unique within the data pipeline pipeline. The data source name and the stage names can only contain alphanumerical characters, underlines, or hyphens. The stages must specify the next stages that the data will go through. This is done by using the next_stages property. The stages can have multiple next stages, forming a forest of stages. The stages must specify the configuration of the plugins that implement them, as described above. The triggers must specify the parameter values that are required by the data source and the stages. The parameter values are provided as a dictionary where the key is the parameter name and the value is the parameter value. The parameter values must be provided for all the parameters that are required by the data source and the stages. The parameter values are used to start the pipeline execution. Trigger parameter values naming convention The trigger parameter values are named using the following convention: For the data source, the naming structure is DataSource.{DataSourceName}.{ParameterName}. In the example above, the data source is named VGDataLake, so the parameter values for the data source are named DataSource.VGDataLake.{ParameterName}. For the stages, the naming structure is Stage.{StageName}.{ParameterName}. In the example above, the stages are named Extract, Partition, Embed, and Index, so the parameter values for the stages are named Stage.{StageName}.{ParameterName}. For stages that have dependency plugins, the naming structure is Stage.{StageName}.Dependency.{DependencyPluginName}.{ParameterName}. In the example above, the Partition stage has a dependency plugin named TokenContentTextPartitioning, so the parameter values for the dependency plugin are named Stage.Partition.Dependency.TokenContentTextPartitioning.{ParameterName}."
  },
  "docs/archive/concepts/plugin/plugin-package.html": {
    "href": "docs/archive/concepts/plugin/plugin-package.html",
    "title": "Plugin Packages | FoundationaLLM",
    "summary": "Plugin Packages Managing plugin packages using the FoundationaLLM Management API The management of plugin packages is done using the FoundationaLLM.Plugin resource provider through the Management API. At this time, only .NET NuGet plugin packages are supported by the resource provider. Create or update a plugin package Management API endpoint: POST /instances/{instanceId}/providers/FoundationaLLM.Plugin/pluginPackages/{packageName} Request body: Must be of type form-data. Must contain a file with the key file that represents the plugin package. The file must be a .NET NuGet package. Must contain a text with the key resource that represents the plugin package resource. The resource must be a valid JSON object with the following content: { \"type\": \"plugin-package\", \"name\": \"{packageName}\" } Note FoundationaLLM currently provides one .NET plugin package, FoundationaLLM.DataPipelinePlugins. An example of a file for this package is FoundationaLLM.DataPipelinePlugins.1.0.0.nupkg. Important Plugin package names must follow a strict naming convention. The name must be in the format {platform}-{name}, where {platform} can be one of Dotnet or Python, and {name} can only contain alphanumerical characters, underlines, or hyphens. For example, the package name for the FoundationaLLM data pipeline plugins package is Dotnet-FoundationaLLMDataPipelinePlugins. As part of the create or update flow, the package is inspected and all plugins contained within it are registered in the system. List plugin packages Management API endpoint: GET /instances/{instanceId}/providers/FoundationaLLM.Plugin/pluginPackages"
  },
  "docs/archive/concepts/plugin/plugin.html": {
    "href": "docs/archive/concepts/plugin/plugin.html",
    "title": "Plugins | FoundationaLLM",
    "summary": "Plugins Overview Plugin naming convention All plugins must follow a strict naming convention. The name must be in the format {platform}-{packageName}-{pluginName}, where: {platform} can be one of Dotnet or Python. {packageName} is the name of the plugin package that provides the plugin and can only contain alphanumerical characters, underlines, or hyphens. {pluginName} is the name of the plugin and can only contain alphanumerical characters, underlines, or hyphens. For example, the plugin name of the FoundationaLLM Azure AI Search data pipeline stage plugin is Dotnet-FoundationaLLMDataPipelinePlugins-AzureAISearchIndexingDataPipelineStage. Plugin properties Here is an example of a plugin definition: { \"type\": \"plugin\", \"name\": \"Dotnet-FoundationaLLMDataPipelinePlugins-AzureAISearchIndexingDataPipelineStage\", \"object_id\": \"instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/Dotnet-FoundationaLLMDataPipelinePlugins-AzureAISearchIndexingDataPipelineStage\", \"display_name\": \"Azure AI Search Indexing Data Pipeline Stage (FoundationaLLM)\", \"description\": \"Provides the FoundationaLLM standard implementation for indexing data pipeline stages that use Azure AI Search.\", \"cost_center\": null, \"category\": \"Data Pipeline Stage\", \"parameters\": [ { \"name\": \"APIEndpointConfigurationObjectId\", \"type\": \"resource-object-id\", \"description\": \"The FoundationaLLM resource identifier of the API Endpoint Configuration resource that identifies the Azure AI Search instance.\" }, { \"name\": \"IndexName\", \"type\": \"string\", \"description\": \"The name of the Azure AI Search index.\" }, { \"name\": \"IndexPartitionName\", \"type\": \"string\", \"description\": \"The name of the Azure AI Search index partition (to be added as a metadata entry and used for logical separation within a given physical index).\" } ], \"parameter_selection_hints\": { \"APIEndpointConfigurationObjectId\": { \"resourcePath\": \"providers/FoundationaLLM.Configuration/apiEndpointConfigurations\", \"filterActionPayload\": { \"Category\": \"General\", \"Subcategory\": \"Indexing\" } } }, \"dependencies\": [], \"properties\": null, \"created_on\": \"2025-03-09T19:59:41.110155+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"ciprian@foundationaLLM.ai\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Parameters Each plugin has zero, one, or more parameters that are required for its execution. Parameters can be of the following types: Type Description string A single string value. int A single integer value. float A single floating-point value. bool A single boolean value. datetime A single date and time value. array An array of values (individual items in the array can be of any type). resource-object-id A FoundationaLLM resource identifier that references another resource in the system. Note When working with resource-object-id parameters, the parameter_selection_hints property can be used to provide additional information about the resource that the parameter references. This information is used by the Management Portal to provide a user-friendly selection experience when configuring the plugin. The parameter_selection_hints property contains a dictionary where the keys are parameter names of type resource-object-id and the values are objects with the following properties: resourcePath: The path to the resource type that the parameter references. filterActionPayload: A JSON object that can be used to filter the resources of the specified type. The object can contain any number of properties that are used to filter the resources and it depends on the specific resource type. User interface developers can use the resourcePath and filterActionPayload properties to build a resource filtering request for the Management API. Using the exemple above, the following filtering request can be built: POST /instances/{instanceId}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/filter. The request body must be the JSON object specified in the filterActionPayload property. Dependencies Each plugin can have zero, one, or more dependencies. A dependency is a reference to another plugin that must be installed in the system in order for the plugin to work correctly. When configuring plugins in the Management Portal, the dependency plugins must also be selected and configured. The dependencies section provides the information needed to determine which plugins are required. The section contains a list of plugin selections that are required for the plugin to work correctly. Plugin selections can be either Single (meaning exactly one dependency plugin must be selected) or Multiple (meaning one or more dependency plugins must be selected). Here is an example of a plugin with single selection dependencies: { \"type\": \"plugin\", \"name\": \"Dotnet-FoundationaLLMDataPipelinePlugins-TextPartitioningDataPipelineStage\", \"object_id\": \"instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/Dotnet-FoundationaLLMDataPipelinePlugins-TextPartitioningDataPipelineStage\", \"display_name\": \"Text Partitioning Data Pipeline Stage (FoundationaLLM)\", \"description\": \"Provides the FoundationaLLM standard implementation for text partitioning data pipeline stages.\", \"cost_center\": null, \"category\": \"Data Pipeline Stage\", \"parameters\": [ { \"name\": \"PartitioningStrategy\", \"type\": \"string\", \"description\": \"Strategy used to partition text (can be Token or Semantic).\" } ], \"parameter_selection_hints\": {}, \"dependencies\": [ { \"selection_type\": \"Single\", \"dependency_plugin_names\": [ \"Dotnet-FoundationaLLMDataPipelinePlugins-TokenContentTextPartitioning\", \"Dotnet-FoundationaLLMDataPipelinePlugins-SemanticContentTextPartitioning\" ] } ], \"properties\": null, \"created_on\": \"2025-03-09T19:59:36.9258434+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"ciprian@foundationaLLM.ai\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Here is an example of a plugin with multiple selection dependencies: { \"type\": \"plugin\", \"name\": \"Dotnet-FoundationaLLMDataPipelinePlugins-AzureDataLakeDataSource\", \"object_id\": \"instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.Plugin/plugins/Dotnet-FoundationaLLMDataPipelinePlugins-AzureDataLakeDataSource\", \"display_name\": \"Azure Data Lake Data Source (FoundationaLLM)\", \"description\": \"Provides the FoundationaLLM standard implementation for Azure Data Lake data sources.\", \"cost_center\": null, \"category\": \"Data Source\", \"parameters\": [ { \"name\": \"Folders\", \"type\": \"array\", \"description\": \"List of strings defining data lake folders (the first part identifies the container name).\" } ], \"parameter_selection_hints\": {}, \"dependencies\": [ { \"selection_type\": \"Multiple\", \"dependency_plugin_names\": [ \"Dotnet-FoundationaLLMDataPipelinePlugins-PDFContentTextExtraction\", \"Dotnet-FoundationaLLMDataPipelinePlugins-DOCXContentTextExtraction\", \"Dotnet-FoundationaLLMDataPipelinePlugins-PPTXContentTextExtraction\", \"Dotnet-FoundationaLLMDataPipelinePlugins-XSLXContentTextExtraction\", \"Dotnet-FoundationaLLMDataPipelinePlugins-ImageContentTextExtraction\", \"Dotnet-FoundationaLLMDataPipelinePlugins-ImageMetadataTextExtraction\" ] } ], \"properties\": null, \"created_on\": \"2025-03-09T19:59:32.3378998+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"ciprian@foundationaLLM.ai\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Managing plugins using the FoundationaLLM Management API List all plugins Management API endpoint: GET /instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins Filter plugins by category Management API endpoint: POST /instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/filter. The request body must contain a JSON object with the following content: { \"categories\": [ \"Data Source\", \"Data Pipeline Stage\" ] } Note The following plugin categories are supported for use in the filter: Data Source, Data Pipeline Stage, Context Text Extraction, Content Text Partitioning."
  },
  "docs/archive/concepts/quota/agent-request-rate.html": {
    "href": "docs/archive/concepts/quota/agent-request-rate.html",
    "title": "Agent Request Rate | FoundationaLLM",
    "summary": "Agent Request Rate The agent request rate is a quota that limits the number of completion requests made to specific agents. These calls are made to the Core API Completions controller and the context of the quota definition must be set to CoreAPI:Completions:<agent_name>, where <agent_name> is the name of the agent. This quota can be enforced per agent for all users or per specific user principal name (UPN) or user identifier. In most cases, the relationship between user principal name and user identifier is one-to-one, so the quota would be effectively the same if metric_partition were set to UserPrincipalName or UserIdentitifer. In these cases, the recommended value for metric_partition is UserPrincipalName. However, in some more advanced scenarios (e.g., when calls are made by another service that authenticates against the Core API using a managed identity), the user principal name may not be available, and the quota should be enforced by user identifier. In these cases, the recommended value for metric_partition is UserIdentifier."
  },
  "docs/archive/concepts/quota/api-raw-request-rate.html": {
    "href": "docs/archive/concepts/quota/api-raw-request-rate.html",
    "title": "API Raw Request Rate | FoundationaLLM",
    "summary": "API Raw Request Rate The API raw request rate is a quota that limits the number of raw requests made to API controllers. Currently, only controllers from the Core API are supported. This quota can be enforced per API controller for all users or per specific user principal name (UPN) or user identifier. In most cases, the relationship between user principal name and user identifier is one-to-one, so the quota would be effectively the same if metric_partition were set to UserPrincipalName or UserIdentitifer. In these cases, the recommended value for metric_partition is UserPrincipalName. However, in some more advanced scenarios (e.g., when calls are made by another service that authenticates against the Core API using a managed identity), the user principal name may not be available, and the quota should be enforced by user identifier. In these cases, the recommended value for metric_partition is UserIdentifier. The following table lists the supported controllers and their contexts: Controller Context Completions CoreAPI:Completions CompletionsStatus CoreAPI:CompletionsStatus * Branding CoreAPI:Branding Configuration CoreAPI:Configuration Files CoreAPI:Files OneDriveWorkSchool CoreAPI:OneDriveWorkSchool Sessions CoreAPI:Sessions UserProfiles CoreAPI:UserProfiles Status CoreAPI:Status Before FoundationaLLM v0.9.7-rc158, the CompletionsStatus controller was not available, and the Completions controller was used to check the status of completions. Starting with FoundationaLLM v0.9.7-rc158, the CompletionsStatus controller is available, and it is used to check the status of completions. Certain client applications (the User Portal being the most notable example) require extensive use of the completions status endpoint, in order to provide a better user experience. Since the number of requests asking for the status of completions is significantly higher than the number of requests asking for completions defining an effective API raw request rate limit for the Completions controller is not practical prior to version v0.9.7-rc158 and is not recommended. Starting with v0.9.7-rc158, separate raw request rate limits can be defined for the Completions and CompletionsStatus controllers, allowing for a more effective quota enforcement. When defining a quota for the CompletionsStatus controller, you need to take into account the value of the FoundationaLLM:APIEndpoints:CoreAPI:Configuration:CompletionResponsePollingIntervalMilliseconds configuration setting, which defines the polling interval for the completions status endpoint in the User Portal. The default value is 100 milliseconds, meaning that the User Portal will poll the completions status endpoint every 100 milliseconds to check the status of the completion. This means that if a user has 2 active completions, they will make 20 requests per second to the completions status endpoint, which can quickly add up to a significant number of requests. Therefore, it is recommended to define a separate quota for the CompletionsStatus controller with a higher limit than the Completions controller."
  },
  "docs/archive/concepts/quota/quota-definition.html": {
    "href": "docs/archive/concepts/quota/quota-definition.html",
    "title": "Quota Definition | FoundationaLLM",
    "summary": "Quota Definition The quota definitions are stored in the main FoundationaLLM storage account, in the quota container in a file named quota-store.json. If the file does not exist, the file is automatically created. The file contains a list of quota definitions with the following structure: { \"name\": \"CoreAPICompletionsUPNRawRequestRateLimit\", \"description\": \"Defines a per UPN raw request rate limit on the Core API Completions controller.\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 120, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } The example above defines a quota for the Core API Completions controller that limits the number of raw API requests per user principal name (UPN) to 120 requests in a 60-second window. If the user principal exceeds this limit, they are locked out for 60 seconds before the quota is reset. Note A user principal name corresponds to either a user or an agent access token. If the same agent access token is shared by multiple applications calling the Core API, the quota is enforced across all applications using that token. Note FoundationaLLM using a smoothing time window of 20 seconds for the quota enforcement. This means that the quota is enforced every 20 seconds, and the number of requests is averaged over that time window. This helps to smooth out spikes in traffic and provides a more consistent experience for users. In the example above, this means that the specified user principal name can make up to 40 requests every 20 seconds. The first time the user principal name exceeds the limit, they are locked out for 60 seconds. After the lockout period, the user principal name can make requests again, and the quota is reset. It is recommended to set the metric_window_seconds to a multiple of 20 seconds to align with the smoothing time window. The standard practice is to set the metric_window_seconds to 60 seconds, which makes it easy to understand for the consumers of the API. Also, it is recommended to set the metric_limit to a multiple of metric_window_seconds divided by 20 seconds, so that you avoid misinterpretation of the quota limits due to roundings. In the example above, since metric_window_seconds is set to 60 seconds, metric_limit should be set to a multiple of 3 (60 seconds / 20 seconds). Therefore, the value of metric_limit is set to 120. If metric_limit were set to 3, the user principal name would be allowed to make only 1 request every 20 seconds. It is also important to note that setting metric_limit to a value below metric_windows_seconds divided by 20 seconds would result in the user principal name being locked out immediately after the first request, which is not a desirable behavior. The following table provides details about the quota definition properties: Name Description Notes name The name of the quota definition. description A description of the quota definition. context The context of the quota definition. The format of the context is <service_name>:<controller_name> or <service_name>:<controller_name>:<agent_name>. Currently the following contexts can be used: CoreAPI:<controller_name>, CoreAPI:Completions:<agent_name> where <agent_name> must be a valid agent name. For more details on the available controllers and their contexts, see API Raw Request Rate. type The type of the quota enforcement applied. The following types are supported: RawRequestRateLimit and AgentRequestRateLimit. RawRequestRateLimt defines the quota metric to be raw API requests and requires a context of <service_name>:<controller_name>. AgentRequestRateLimit defines the quota metric to be agent completion requests and requires a context of <service_name>:<controller_name>:<agent_name>. For more details, see API Raw Request Rate or Agent Request Rate. metric_partition The metric partition used to enforce the quota. The following partitions are supported: None (the metric is not partitioned) UserPrincipalName (the metric is partitioned by user principal name) and UserIdentifier (the metric is partitioned by user identifier). In the example above, the metric is partitioned by user principal name, meaning that each user principal name has its own quota limit. If metric_partition were set to None, the quota would be enforced globally across all calls, regardless of user principal names, meaning that the limit would apply to all users collectively. If metric_partition were set to UserIdentifier, the quota would be enforced per user identifier, which is a unique identifier for each user. In most cases, the relationship between user principal name and user identifier is one-to-one, so the quota would be effectively the same as if metric_partition were set to UserPrincipalName. In these cases, the recommended value for metric_partition is UserPrincipalName. However, in some more advanced scenarios (e.g., when calls are made by another service that authenticates against the Core API using a managed identity), the user principal name may not be available, and the quota should be enforced by user identifier. In these cases, the recommended value for metric_partition is UserIdentifier. metric_limit The limit of the metric. The limit is enforced over the metric_window_seconds. In the example above, a maximum number of 120 raw API requests are allowed per user principal name in a 60-second window. metric_window_seconds The time window in seconds over which the limit is enforced. In the example above, a maximum number of 120 raw API requests are allowed per user principal name in a 60-second window. lockout_duration_seconds The duration in seconds for which the caller is locked out after exceeding the quota. The lockout duration is applied after the user exceeds the quota limit. The user is locked out for the specified duration before the quota is reset. distributed_enforcement Indicates whether the quota is enforced across multiple instances of the same API. If true, the quota is enforced across all the instances of the API. If false, the quota is enforced individually on every single instance. In the example above, if the platform in running 5 instances of the Core API, one user principal name can make up to 5 x 120 = 600 raw API requests in a 60-second window. If distributed_enforcement were set to true, the user principal name would be allowed to make only 120 raw API requests in a 60-second window across all instances of the Core API."
  },
  "docs/archive/deployment/app-configuration-values.html": {
    "href": "docs/archive/deployment/app-configuration-values.html",
    "title": "Azure App Configuration values | FoundationaLLM",
    "summary": "Azure App Configuration values FoundationaLLM uses Azure App Configuration to store configuration values, Key Vault secret references, and feature flags. Doing so helps reduce duplication and provides a convenient way to manage these settings in one place. It also allows you to change the settings without having to redeploy the solution. Since several settings can be shared by multiple projects, we do not specify the project name in the configuration key names. Configuration values Key Default Value Description FoundationaLLM:Instance:Id Generated GUID The value should be a GUID represents a unique instance of the FoundationaLLM instance. FoundationaLLM:Configuration:KeyVaultURI Enter the URL to the service. The URL of the Key Vault used to store secrets. FoundationaLLM:AgentHub:AgentMetadata:StorageContainer agents FoundationaLLM:Agent:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:APIs:OrchestrationAPI:APIKey Key Vault secret name: foundationallm-apis-orchestrationapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:OrchestrationAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:OrchestrationAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:OrchestrationAPI:ForceHttpsRedirection false FoundationaLLM:APIs:AgentHubAPI:APIKey Key Vault secret name: foundationallm-apis-agenthubapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:AgentHubAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:AgentHubAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:CoreAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:CoreAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:CoreAPI:BypassGatekeeper true By default, the Core API bypasses the Gatekeeper API. To override this behavior and enable the Gatekeeper API, set this value to true. Bypassing the Gatekeeper means that you bypass content protection and filtering in favor of improved performance. FoundationaLLM:APIs:DataSourceHubAPI:APIKey Key Vault secret name: foundationallm-apis-datasourcehubapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:DataSourceHubAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:DataSourceHubAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:GatekeeperAPI:APIKey Key Vault secret name: foundationallm-apis-gatekeeperapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:GatekeeperAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:GatekeeperAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableAzureContentSafety true By default, the Gatekeeper API has Azure Content Safety integration enabled. To disable this feature, set this value to false. FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableLakeraGuard true By default, the Gatekeeper API has Lakera Guard integration enabled. To disable this feature, set this value to false. FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableEnkryptGuardrails true By default, the Gatekeeper API has Enkrypt Guardrails integration enabled. To disable this feature, set this value to false. FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableMicrosoftPresidio true By default, the Gatekeeper API has Microsoft Presidio integration enabled. To disable this feature, set this value to false. FoundationaLLM:APIs:GatekeeperAPI:ForceHttpsRedirection false FoundationaLLM:APIs:GatekeeperIntegrationAPI:APIKey Key Vault secret name: foundationallm-apis-gatekeeperintegrationapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:GatekeeperIntegrationAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:GatekeeperIntegrationAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:LangChainAPI:APIKey Key Vault secret name: foundationallm-apis-langchainapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:LangChainAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:LangChainAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:ManagementAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:ManagementAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:Prompt:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:APIs:PromptHubAPI:APIKey Key Vault secret name: foundationallm-apis-prompthubapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:PromptHubAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:PromptHubAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:SemanticKernelAPI:APIKey Key Vault secret name: foundationallm-apis-semantickernelapi-apikey This is a Key Vault reference. FoundationaLLM:APIs:SemanticKernelAPI:APIUrl Enter the URL to the service. FoundationaLLM:APIs:SemanticKernelAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:APIs:SemanticKernelAPI:ForceHttpsRedirection false FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:APIKey Key Vault secret name:foundationallm-apis-gatekeeper-azurecontentsafety-apikey This is a Key Vault reference. FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:APIUrl Enter the URL to the service. FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:HateSeverity 2 FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SelfHarmSeverity 2 FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SexualSeverity 2 FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:ViolenceSeverity 2 FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName completions FoundationaLLM:AzureOpenAI:API:Completions:MaxTokens 8096 FoundationaLLM:AzureOpenAI:API:Completions:ModelName gpt-35-turbo FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion 0301 FoundationaLLM:AzureOpenAI:API:Completions:Temperature 0 FoundationaLLM:AzureOpenAI:API:Embeddings:DeploymentName embeddings FoundationaLLM:AzureOpenAI:API:Embeddings:MaxTokens 8191 FoundationaLLM:AzureOpenAI:API:Embeddings:ModelName text-embedding-ada-002 FoundationaLLM:AzureOpenAI:API:Embeddings:Temperature 0 FoundationaLLM:AzureOpenAI:API:Endpoint Enter the URL to the service. FoundationaLLM:AzureOpenAI:API:Key Key Vault secret name: foundationallm-azureopenai-api-key This is a Key Vault reference. FoundationaLLM:AzureOpenAI:API:Version 2023-05-15 FoundationaLLM:BlobStorageMemorySource:BlobStorageContainer memory-source FoundationaLLM:BlobStorageMemorySource:ConfigFilePath BlobMemorySourceConfig.json FoundationaLLM:Branding:AccentColor #fff FoundationaLLM:Branding:AccentTextColor #131833 FoundationaLLM:Branding:BackgroundColor #fff FoundationaLLM:Branding:CompanyName FoundationaLLM FoundationaLLM:Branding:FavIconUrl favicon.ico FoundationaLLM:Branding:KioskMode false FoundationaLLM:Branding:LogoText FoundationaLLM:Branding:LogoUrl foundationallm-logo-white.svg FoundationaLLM:Branding:PageTitle FoundationaLLM Chat Copilot FoundationaLLM:Branding:PrimaryColor #131833 FoundationaLLM:Branding:PrimaryTextColor #fff FoundationaLLM:Branding:SecondaryColor #334581 FoundationaLLM:Branding:SecondaryTextColor #fff FoundationaLLM:Branding:PrimaryButtonBackgroundColor #5472d4 FoundationaLLM:Branding:PrimaryButtonTextColor #fff FoundationaLLM:Branding:SecondaryButtonBackgroundColor #70829a FoundationaLLM:Branding:SecondaryButtonTextColor #fff FoundationaLLM:Chat:Entra:CallbackPath /signin-oidc FoundationaLLM:Chat:Entra:ClientId FoundationaLLM:Chat:Entra:ClientSecret Key Vault secret name: foundationallm-chat-entra-clientsecret This is a Key Vault reference. FoundationaLLM:Chat:Entra:Instance Enter the URL to the service. FoundationaLLM:Chat:Entra:Scopes api://FoundationaLLM-Auth/Data.Read FoundationaLLM:Chat:Entra:TenantId FoundationaLLM:CoreAPI:Entra:CallbackPath /signin-oidc FoundationaLLM:CoreAPI:Entra:ClientId FoundationaLLM:CoreAPI:Entra:ClientSecret Key Vault secret name: foundationallm-coreapi-entra-clientsecret This is a Key Vault reference. FoundationaLLM:CoreAPI:Entra:Instance Enter the URL to the service. FoundationaLLM:CoreAPI:Entra:Scopes Data.Read FoundationaLLM:CoreAPI:Entra:TenantId FoundationaLLM:CoreWorker:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string This is a Key Vault reference. FoundationaLLM:CosmosDB:ChangeFeedLeaseContainer leases FoundationaLLM:CosmosDB:Containers Sessions, UserSessions FoundationaLLM:CosmosDB:Database database FoundationaLLM:CosmosDB:Endpoint Enter the URL to the service. FoundationaLLM:CosmosDB:MonitoredContainers Sessions FoundationaLLM:DataSourceHub:DataSourceMetadata:StorageContainer data-sources FoundationaLLM:LangChain:CSVFile:URL Key Vault secret name: foundationallm-langchain-csvfile-url This is a Key Vault reference. FoundationaLLM:LangChain:SQLDatabase:TestDB:Password Key Vault secret name: foundationallm-langchain-sqldatabase-testdb-password This is a Key Vault reference. FoundationaLLM:LangChain:Summary:MaxTokens 4097 FoundationaLLM:LangChain:Summary:ModelName gpt-35-turbo FoundationaLLM:LangChainAPI:Key Key Vault secret name: foundationallm-langchainapi-key This is a Key Vault reference. FoundationaLLM:Management:Entra:CallbackPath /signin-oidc FoundationaLLM:Management:Entra:ClientId FoundationaLLM:Management:Entra:ClientSecret Key Vault secret name: foundationallm-management-entra-clientsecret This is a Key Vault reference. FoundationaLLM:Management:Entra:Instance Enter the URL to the service. FoundationaLLM:Management:Entra:Scopes api://FoundationaLLM-Management-Auth/Data.Manage FoundationaLLM:Management:Entra:TenantId FoundationaLLM:ManagementAPI:Entra:CallbackPath /signin-oidc FoundationaLLM:ManagementAPI:Entra:ClientId FoundationaLLM:ManagementAPI:Entra:ClientSecret Key Vault secret name: foundationallm-managementapi-entra-clientsecret This is a Key Vault reference. FoundationaLLM:ManagementAPI:Entra:Instance Enter the URL to the service. FoundationaLLM:ManagementAPI:Entra:Scopes Data.Manage FoundationaLLM:ManagementAPI:Entra:TenantId FoundationaLLM:OpenAI:API:Endpoint Enter the URL to the service. FoundationaLLM:OpenAI:API:Key Key Vault secret name: foundationallm-openai-api-key This is a Key Vault reference. FoundationaLLM:OpenAI:API:Temperature 0 FoundationaLLM:PromptHub:PromptMetadata:StorageContainer system-prompt FoundationaLLM:Refinement FoundationaLLM:SemanticKernelAPI:OpenAI:Key Key Vault secret name: foundationallm-semantickernelapi-openai-key This is a Key Vault reference. FoundationaLLM:SemanticKernelAPI:OpenAI.ChatCompletionPromptName RetailAssistant.Default FoundationaLLM:SemanticKernelAPI:OpenAI.CompletionsDeployment completions FoundationaLLM:SemanticKernelAPI:OpenAI.CompletionsDeploymentMaxTokens 8096 FoundationaLLM:SemanticKernelAPI:OpenAI.EmbeddingsDeployment embeddings FoundationaLLM:SemanticKernelAPI:OpenAI.EmbeddingsDeploymentMaxTokens 8191 FoundationaLLM:SemanticKernelAPI:OpenAI.Endpoint Enter the URL to the service. FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.CompletionsMaxTokens 300 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.CompletionsMinTokens 50 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.MemoryMaxTokens 3000 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.MemoryMinTokens 1500 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.MessagesMaxTokens 3000 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.MessagesMinTokens 100 FoundationaLLM:SemanticKernelAPI:OpenAI.PromptOptimization.SystemMaxTokens 1500 FoundationaLLM:SemanticKernelAPI:OpenAI.ShortSummaryPromptName Summarizer.TwoWords FoundationaLLM:APIs:VectorizationAPI:APIUrl The URL of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:APIKey Key Vault secret name: foundationallm-apis-vectorizationapi-apikey The API key of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization API. FoundationaLLM:APIs:VectorizationAPI:ForceHttpsRedirection false FoundationaLLM:APIs:VectorizationWorker:APIUrl The URL of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:APIKey Key Vault secret name: foundationallm-apis-vectorizationworker-apikey The API key of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization worker API. FoundationaLLM:Vectorization:VectorizationWorker The settings used by each instance of the vectorization worker service. For more details, see default vectorization worker settings FoundationaLLM:Vectorization:Queues:Embed:AccountName The account name of the Azure Storage account used for the embed vectorization queue. FoundationaLLM:Vectorization:Queues:Extract:AccountName The account name of the Azure Storage account used for the extract vectorization queue. FoundationaLLM:Vectorization:Queues:Index:AccountName The account name of the Azure Storage account used for the index vectorization queue. FoundationaLLM:Vectorization:Queues:Partition:AccountName The account name of the Azure Storage account used for the partition vectorization queue. FoundationaLLM:Vectorization:StateService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Events:AzureEventGridEventService:APIKey Key Vault secret name: foundationallm-events-azureeventgrid-apikey The API key used to access FLLM's Event Grid namespace. FoundationaLLM:Events:AzureEventGridEventService:AuthenticationType The authentication type used to connect to the underlying Event Grid namespace. Currently supports APIKey. FoundationaLLM:Events:AzureEventGridEventService:Endpoint The endpoint for the Azure Event Grid namespace. FoundationaLLM:Events:AzureEventGridEventService:NamespaceId The Azure resource ID of the Event Grid namespace. FoundationaLLM:Events:AzureEventGridEventService:Profiles:OrchestrationAPI FLLM eventing infrastructure configuration for the Orchestration API. FoundationaLLM:Events:AzureEventGridEventService:Profiles:CoreAPI FLLM eventing infrastructure configuration for the Core API. FoundationaLLM:Events:AzureEventGridEventService:Profiles:ManagementAPI FLLM eventing infrastructure configuration for the Management API. FoundationaLLM:Events:AzureEventGridEventService:Profiles:VectorizationAPI FLLM eventing infrastructure configuration for the Vectorization API. FoundationaLLM:Events:AzureEventGridEventService:Profiles:VectorizationWorker FLLM eventing infrastructure configuration for the Vectorization Worker. FoundationaLLM:Configuration:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint Azure AI Search service endpoint. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIVersion 2023-05-15 FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName embeddings FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint Enter the URL to the service. FoundationaLLM:VectorizationAPI:Entra:CallbackPath /signin-oidc FoundationaLLM:VectorizationAPI:Entra:ClientId FoundationaLLM:VectorizationAPI:Entra:ClientSecret Key Vault secret name: foundationallm-apis-management-api-entra-clientsecret This is a Key Vault reference. FoundationaLLM:VectorizationAPI:Entra:Instance Enter the URL to the service. FoundationaLLM:VectorizationAPI:Entra:Scopes api://FoundationaLLM-Vectorization/Data.Manage FoundationaLLM:VectorizationAPI:Entra:TenantId"
  },
  "docs/archive/deployment/authentication-authorization/authorization-setup-entra.html": {
    "href": "docs/archive/deployment/authentication-authorization/authorization-setup-entra.html",
    "title": "Authorization setup: Microsoft Entra ID | FoundationaLLM",
    "summary": "Authorization setup: Microsoft Entra ID FoundationaLLM comes with out-of-the-box support for Microsoft Entra ID authorization. This means that you can use your Microsoft Entra ID to setup authorization access to FoundationaLLM's platform. Create the API application Register the API application in the Microsoft Entra ID admin center Sign in to the Microsoft Entra ID admin center as at least a Cloud Application Administrator. Browse to Identity > Applications > App registrations. On the page that appears, select + New registration. When the Register an application page appears, enter the name FoundationaLLM-Authorization-API for your application. Under Supported account types, select Accounts in this organizational directory only. Select Register. The application's Overview pane displays upon successful registration. Record the Application (client) ID and Directory (tenant) ID to add to your App Configuration settings later. Expose an API for the API application Under Manage, select Expose an API > Add a scope. For Application ID URI, enter api://FoundationaLLM-Authorization, then select Save and continue, and then enter the following details: Scope name: Authorization.Manage Who can consent?: Admins and users Admin consent display name: Manage Authorization Admin consent description: Allows the app to manage data on behalf of the signed-in user. User consent display name: Manage data on behalf of the user User consent description: Allows the app to manage data on behalf of the signed-in user. State: Enabled Select Add scope to complete the scope addition. Copy the Scope name value to add to your App Configuration settings later. It should look like api://FoundationaLLM-Authorization/Authorization.Manage. Update the client application manifest Under Manage, select Manifest. Locate the accessTokenAcceptedVersion property and set its value to 2. Select Save at the top of the page to save the changes. Configure Authentication Flows Navigate to the Authentication tab. Select Add a platform. On the Configure platforms pane, select Web. Enter http://localhost under Redirect URIs. Then, below Implicit grant and hybrid flows, select both Access tokens and ID tokens. Finally, select Configure."
  },
  "docs/archive/deployment/authentication-authorization/core-authentication-setup-entra.html": {
    "href": "docs/archive/deployment/authentication-authorization/core-authentication-setup-entra.html",
    "title": "Core API and User Portal authentication setup: Microsoft Entra ID | FoundationaLLM",
    "summary": "Core API and User Portal authentication setup: Microsoft Entra ID FoundationaLLM comes with out-of-the-box support for Microsoft Entra ID authentication. This means that you can use your Microsoft Entra ID account to log in to the chat interface. Create the Microsoft Entra ID applications To enable Microsoft Entra ID authentication for the Core API and user portal, you need to create two applications in the Microsoft Azure portal: A client application that will be used by the user portal chat interface to authenticate users. An API application that will be used by the Core API to authenticate users. Create the client application Register the client application in the Microsoft Entra ID admin center Sign in to the Microsoft Entra ID admin center as at least a Cloud Application Administrator. Browse to Identity > Applications > App registrations. On the page that appears, select + New registration. When the Register an application page appears, enter the following name for your application FoundationaLLM-User-Portal. Under Supported account types, select Accounts in this organizational directory only. Select Register. The application's Overview pane displays upon successful registration. Record the Application (client) ID and Directory (tenant) ID to add to your App Configuration settings later. Add a redirect URI to the client application Under Manage, select Authentication. Under Platform configurations, select Add a platform. In the pane that opens, select Single-page application. This is for the Vue.js chat application. Add a Redirect URI under Single-page application for your deployed Vue.js application. Enter <YOUR_CHAT_APP_URL>/signin-oidc, replacing <YOUR_CHAT_APP_URL> with the chat UI application URL obtained in the Pre-requisites section above. For example, it should look something like https://d85a09ce067141d5807a.eastus.aksapp.io/signin-oidc for an AKS deployment, or https://fllmaca002chatuica.graybush-c554b849.eastus.azurecontainerapps.io/signin-oidc for an ACA deployment. Add a Redirect URI under Single-page application for local development of the Vue.js application: http://localhost:3000/signin-oidc. If you wish to configure authentication in Postman for executing calls against the Core API, you will need to add a Redirect URI under Mobile and desktop applications for Postman. Enter https://oauth.pstmn.io/v1/callback for the URI. To do this, complete the following steps: Under Platform configurations, select Add a platform. In the pane that opens, select Mobile and desktop applications. Enter https://oauth.pstmn.io/v1/callback for the Custom redirect URIs value. Select Configure to apply the changes. Implicit grant and hybrid flows for the client application Check Access tokens and ID tokens under Implicit grant. Select Configure to apply the changes (if the button is present). Select Save at the bottom of the page to save the changes. Update the client application manifest Under Manage, select Manifest. Locate the accessTokenAcceptedVersion property and set its value to 2. Select Save at the top of the page to save the changes. Create the API application Register the API application in the Microsoft Entra ID admin center Return to the Microsoft Entra ID admin center. Browse to Identity > Applications > App registrations and select + New registration. For Name, enter the name FoundationaLLM-Core-API for the application. Under Supported account types, select Accounts in this organizational directory only. Select Register. The application's Overview pane displays upon successful registration. Record the Application (client) ID and Directory (tenant) ID to add to your App Configuration settings later. Implicit grant and hybrid flows for the API application Select Authentication under Manage in the left-hand menu. Select + Add a platform under Platform configurations. In the pane that opens, select Web. Under \"Redirect URIs\", enter http://localhost and select Configure. Please note that this value is not used in the FoundationaLLM solution, but is required in order to be able to select the access and ID tokens in the next step. Check Access tokens and ID tokens under Implicit grant. Select Configure to apply the changes. Select Save at the bottom of the page to save the changes. Expose an API for the API application Under Manage, select Expose an API > Add a scope. For Application ID URI, make sure to use api://FoundationaLLM-Core, then enter the following details: Scope name: Data.Read Who can consent?: Admins and users Admin consent display name: Read data on behalf of users Admin consent description: Allows the app to read data on behalf of the signed-in user. User consent display name: Read data on behalf of the user User consent description: Allows the app to read data on behalf of the signed-in user. State: Enabled Select Add scope to complete the scope addition. Copy the Scope name value to add to your App Configuration settings later. For example, it should look something like api://FoundationaLLM-Core/Data.Read. Add authorized client application While still in the Expose an API section, select + Add a client application. Paste the Application (client) ID of the client application that you created earlier. Check the Data.Read authorized scope that you created. Select Add application to complete the client application addition. Update the API application manifest Under Manage, select Manifest. Locate the accessTokenAcceptedVersion property and set its value to 2. Select Save at the top of the page to save the changes. Add API permissions for the client application Browse to Identity > Applications > App registrations. Select the FoundationaLLM-Client application that you created earlier. Select API permissions. Select + Add a permission under the \"Configured permissions\" section. In the \"Request API permissions\" pan, select the My APIs tab, then select the FoundationaLLM API application. Select the Data.Read scope that you created earlier, then select Add permissions. The client application's configured permissions should now look like the following:"
  },
  "docs/archive/deployment/authentication-authorization/index.html": {
    "href": "docs/archive/deployment/authentication-authorization/index.html",
    "title": "Authentication | FoundationaLLM",
    "summary": "Authentication FoundationaLLM uses the Microsoft Entra ID service to authenticate users and applications. Check back for additional authentication providers in the future. Microsoft Entra ID Starting release 0.8.0 there are 2 options of completing the app registrations for the 6 apps required for FoundationaLLM. Option #1 is to run a script that will register all 6 applications for you. Option #2 is to manually register the 6 applications. The steps for both options are below. Option #1 - Run the script to register all 6 applications The script is available in the \\deploy\\common\\scripts\\ folder. The script is called Create-FllmEntraIdApps.ps1. The script will register the 6 required applications in the Entra ID tenant that you are logged into. After the completion of the script execution, you will see the 6 applications registered in the Entra ID tenant under App registrations Option 2 - Manually registering the 6 applications Important The following steps are to set up authentication and authorization for the solution. You will need to create app registrations in the Entra ID tenant in the Azure portal manually if you choose not to run the automatic script for any reason. There are currently five app registrations required for the solution as listed below. After you complete the 6 app registrations, you will need to finish the deployment process of the solution and revisit these app registrations to fill in some missing values that are generated during the deployment itself. Steps to perform before the deployment Core API and user portal authentication pre-deployment - Microsoft Entra ID Management API and portal authentication pre-deployment - Microsoft Entra ID Authorization pre-deployment - Microsoft Entra ID Steps to perform after the deployment Pre-requisites for post-deployment configuration Core API and user portal authentication post-deployment - Microsoft Entra ID Management API and portal authentication post-deployment - Microsoft Entra ID Authorization post-deployment - Microsoft Entra ID"
  },
  "docs/archive/deployment/authentication-authorization/management-authentication-setup-entra.html": {
    "href": "docs/archive/deployment/authentication-authorization/management-authentication-setup-entra.html",
    "title": "Management API and portal authentication setup: Microsoft Entra ID | FoundationaLLM",
    "summary": "Management API and portal authentication setup: Microsoft Entra ID FoundationaLLM comes with out-of-the-box support for Microsoft Entra ID authentication. This means that you can use your Microsoft Entra ID account to log in to the management portal. Create the Microsoft Entra ID applications To enable Microsoft Entra ID authentication for the Management API and portal, you need to create two applications in the Microsoft Azure portal: A client application that will be used by the management portal to authenticate users. An API application that will be used by the Management API to authenticate users. Create the client application Register the client application in the Microsoft Entra ID admin center Sign in to the Microsoft Entra ID admin center as at least a Cloud Application Administrator. Browse to Identity > Applications > App registrations. On the page that appears, select + New registration. When the Register an application page appears, enter the name FoundationaLLM-Management-Portal for your application. Under Supported account types, select Accounts in this organizational directory only. Select Register. The application's Overview pane displays upon successful registration. Record the Application (client) ID and Directory (tenant) ID to add to your App Configuration settings later. Add a redirect URI to the client application Under Manage, select Authentication. Under Platform configurations, select Add a platform. In the pane that opens, select Single-page application. This is for the Vue.js management application. Add a Redirect URI under Single-page application for your deployed Vue.js management application. Enter <YOUR_MANAGEMENT_APP_URL>/signin-oidc, replacing <YOUR_MANAGEMENT_APP_URL> with the management UI application URL. For example, it should look something like https://d85a09ce067141d5807a.eastus.aksapp.io/signin-oidc for an AKS deployment, or https://fllmaca002managementuica.graybush-c554b849.eastus.azurecontainerapps.io/signin-oidc for an ACA deployment. Add a Redirect URI under Single-page application for local development of the management portal Vue.js application: http://localhost:3001/signin-oidc. If you wish to configure authentication in Postman for executing calls against the Core API, you will need to add a Redirect URI under Mobile and desktop applications for Postman. Enter https://oauth.pstmn.io/v1/callback for the URI. To do this, complete the following steps: Under Platform configurations, select Add a platform. In the pane that opens, select Mobile and desktop applications. Enter https://oauth.pstmn.io/v1/callback for the Custom redirect URIs value. Select Configure to apply the changes. Implicit grant and hybrid flows for the client application Check Access tokens and ID tokens under Implicit grant. Select Configure to apply the changes (if the button is present). Select Save at the bottom of the page to save the changes. Update the client application manifest Under Manage, select Manifest. Locate the accessTokenAcceptedVersion property and set its value to 2. Select Save at the top of the page to save the changes. Create the API application Register the API application in the Microsoft Entra ID admin center Return to the Microsoft Entra ID admin center. Browse to Identity > Applications > App registrations and select + New registration. For Name, enter a name for the application. For example, enter FoundationaLLM-Management. Users of the app will see this name, and can be changed later. Under Supported account types, select Accounts in this organizational directory only. Select Register. The application's Overview pane displays upon successful registration. Record the Application (client) ID and Directory (tenant) ID to add to your App Configuration settings later. Implicit grant and hybrid flows for the API application Select Authentication under Manage in the left-hand menu. Select + Add a platform under Platform configurations. In the pane that opens, select Web. Under \"Redirect URIs\", enter http://localhost and select Configure. Please note that this value is not used in the FoundationaLLM solution, but is required in order to be able to select the access and ID tokens in the next step. Check Access tokens and ID tokens under Implicit grant. Select Configure to apply the changes. Select Save at the bottom of the page to save the changes. Expose an API for the API application Under Manage, select Expose an API > Add a scope. For Application ID URI, accept the default or specify a custom one, then select Save and continue, and then enter the following details: Scope name: Data.Manage Who can consent?: Admins and users Admin consent display name: Manage data on behalf of users Admin consent description: Allows the app to manage data on behalf of the signed-in user. User consent display name: Manage data on behalf of the user User consent description: Allows the app to manage data on behalf of the signed-in user. State: Enabled Select Add scope to complete the scope addition. Copy the Scope name value to add to your App Configuration settings later. It should look like api://FoundationaLLM-Management/Data.Manage. Add authorized client application While still in the Expose an API section, select + Add a client application. Paste the Application (client) ID of the client application that you created earlier. Check the Data.Manage authorized scope that you created. Select Add application to complete the client application addition. Update the API application manifest Under Manage, select Manifest. Locate the accessTokenAcceptedVersion property and set its value to 2. Select Save at the top of the page to save the changes. Add API permissions for the client application Browse to Identity > Applications > App registrations. Select the FoundationaLLM-Management-Portal application that you created earlier. Select API permissions. Select + Add a permission under the \"Configured permissions\" section. In the \"Request API permissions\" pan, select the My APIs tab, then select the FoundationaLLM-Management-API application. Select the Data.Manage scope that you created earlier, then select Add permissions. The client application's configured permissions should now look like the following:"
  },
  "docs/archive/deployment/authentication-authorization/post-authorization-deployment.html": {
    "href": "docs/archive/deployment/authentication-authorization/post-authorization-deployment.html",
    "title": "| FoundationaLLM",
    "summary": "Post Deployment Follow the instruction below to complete the setup of Microsoft Entra ID authentication for the Authorization API application after the deployment is complete. Update App Configuration settings Sign in to the Azure portal as at least a Contributor. Navigate to the resource group that was created as part of the deployment. Select the App Configuration resource and select Configuration explorer to view the values. Enter authorization in the search box to filter the results. Check the box next to Key in the header to select all items. Find the key for FoundationaLLM:APIs:AuthorizationAPI:APIScope and click on edit. Replace the value with the value from the scope we created earlier, as api://FoundationaLLM-Authorization Select Apply to save the changes. Next steps Now that Entra authorization is fully configured, navigate to your Entra ID management console and make sure you completed all app registrations for all the other apps mentioned in the deployment documentation."
  },
  "docs/archive/deployment/authentication-authorization/post-core-deployment.html": {
    "href": "docs/archive/deployment/authentication-authorization/post-core-deployment.html",
    "title": "| FoundationaLLM",
    "summary": "Post Deployment Follow the instruction below to complete the setup of Microsoft Entra ID authentication for the Core API and Chat applications after the deployment is complete. Update App Configuration settings Sign in to the Azure portal as at least a Contributor. Navigate to the resource group that was created as part of the deployment. Select the App Configuration resource and select Configuration explorer to view the values. Enter entra in the search box to filter the results. Check the box next to Key in the header to select all items. Select Edit to open a JSON editor for the selected items. Replace the values for the following settings with the values that you recorded earlier: FoundationaLLM:Chat:Entra:ClientId: The Application (client) ID of the client application that you created earlier. FoundationaLLM:Chat:Entra:Scopes: The fully-qualified scopes path for the API application that you created earlier. For example, it should look something like api://FoundationaLLM-Core/Data.Read. FoundationaLLM:Chat:Entra:TenantId: The Directory (tenant) ID of the client application that you created earlier. FoundationaLLM:CoreAPI:Entra:ClientId: The Application (client) ID of the API application that you created earlier. FoundationaLLM:CoreAPI:Entra:TenantId: The Directory (tenant) ID of the API application that you created earlier. Validate the following values while reviewing the settings: FoundationaLLM:Chat:Entra:CallbackPath: Should be /signin-oidc. FoundationaLLM:Chat:Entra:Instance: Should be https://login.microsoftonline.com/. FoundationaLLM:CoreAPI:Entra:CallbackPath: Should be /signin-oidc. FoundationaLLM:CoreAPI:Entra:Instance: Should be https://login.microsoftonline.com/. FoundationaLLM:CoreAPI:Entra:Scopes: Should be Data.Read. Select Apply to save the changes. Next steps Now that Entra authentication is fully configured, restart the Core API and chat applications to apply the changes. Navigate to your chat application or refresh the page if it is already open. It should automatically prompt you to sign in with your Microsoft Entra ID account. Restart Core API and Chat UI applications in an ACA Deployment To restart the Core API and Chat applications in an Azure Container Apps (ACA) deployment, you will need to navigate to the Core API and Chat applications and restart their container revisions, as indicated in the following Azure Portal screenshot: From the Revisions blade in the left navigation panel of the Core API or Chat UI container app detail page in Azure Portal, select the name of the running revision. A dialog panel titled Revision details should appear on the right side of the browser with a Restart button at the top. Select the Restart button to restart the running container. Restarting in this manner will need to be performed for both the Core API container app and the Chat UI container app. Restart Core API and Chat UI applications in an AKS Deployment To restart the Core API and Chat applications in an Azure Kubernetes Service (AKS) deployment, you will need to navigate to the AKS detail page in Azure Portal and perform the following: Select the Workloads blade from the left navigation panel. Select the Pods tab from the Workloads detail page. Select the Core API and Chat UI pods from the list (it helps if you select default in the Filter by namespace dropdown first). Select the Delete button to terminate the currently running pods. New pods will be instantiated to take their place."
  },
  "docs/archive/deployment/authentication-authorization/post-management-deployment.html": {
    "href": "docs/archive/deployment/authentication-authorization/post-management-deployment.html",
    "title": "| FoundationaLLM",
    "summary": "Post Deployment Follow the instruction below to complete the setup of Microsoft Entra ID authentication for the Management API and Management Portal applications after the deployment is complete. Update App Configuration settings Sign in to the Azure portal as at least a Contributor. Navigate to the resource group that was created as part of the deployment. Select the App Configuration resource and select Configuration explorer to view the values. Enter entra in the search box to filter the results. Check the box next to Key in the header to select all items. Select Edit to open a JSON editor for the selected items. Replace the values for the following settings with the values that you recorded earlier: FoundationaLLM:Management:Entra:ClientId: The Application (client) ID of the client application that you created earlier. FoundationaLLM:Management:Entra:Scopes: The fully-qualified scopes path for the API application that you created earlier. It should look like api://FoundationaLLM.Management-API/Data.Manage. FoundationaLLM:Management:Entra:TenantId: The Directory (tenant) ID of the client application that you created earlier. FoundationaLLM:ManagementAPI:Entra:ClientId: The Application (client) ID of the API application that you created earlier. FoundationaLLM:ManagementAPI:Entra:TenantId: The Directory (tenant) ID of the API application that you created earlier. Validate the following values while reviewing the settings: FoundationaLLM:Management:Entra:CallbackPath: Should be /signin-oidc. FoundationaLLM:Management:Entra:Instance: Should be https://login.microsoftonline.com/. FoundationaLLM:ManagementAPI:Entra:Instance: Should be https://login.microsoftonline.com/. FoundationaLLM:ManagementAPI:Entra:Scopes: Should be Data.Manage. Select Apply to save the changes. Next steps Now that Entra authentication is fully configured, restart the Management API and management portal applications to apply the changes. Navigate to your management portal application or refresh the page if it is already open. It should automatically prompt you to sign in with your Microsoft Entra ID account."
  },
  "docs/archive/deployment/authentication-authorization/pre-requisites.html": {
    "href": "docs/archive/deployment/authentication-authorization/pre-requisites.html",
    "title": "| FoundationaLLM",
    "summary": "Pre-requisites for post-deployment configuration Important Be aware that after completing this registration and the other app registrations in Entra ID as instructed in the docs you will complete the deployment steps outlined at deploy the solution then you will be revisiting your app registrations to complete some of the settings that require the solution to be deployed before the entire app registration is completed successfully. Setup App Configuration access Sign in to the Azure portal as at least a Contributor. Navigate to the Resource Group that was created as part of the deployment. Note If you performed an Azure Container Apps (ACA) or Azure Kubernetes Service (AKS) deployment, you will see an extra Resource Group that starts with ME_ or MC_ in addition to the Resource Group defined during the deployment. You will need to navigate to the Resource Group that does not start with ME_ or MC_ to access the App Configuration resource. 3. Select the App Configuration resource and select Configuration explorer to view the values. If you cannot access the configurations, add your user account as an App Configuration Data Owner through Access Control (IAM). You need this role in order to update the configurations as a required part of the authentication setup. To add your user account to the appropriate role, follow the instructions in the Configure access control for services document. Obtain the URL for the chat UI application You need this URL to assign the redirect URI for the client application. If you performed an Azure Container Apps (ACA) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Container App resource whose name ends with chatuica. Within the Overview pane, copy the Application Url value. This is the URL for the chat application. If you performed an Azure Kubernetes Service (AKS) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Kubernetes Service resource. Select Properties in the left-hand menu and copy the HTTP application routing domain value. This is the URL for the chat application."
  },
  "docs/archive/deployment/azure-resource-providers-requirements.html": {
    "href": "docs/archive/deployment/azure-resource-providers-requirements.html",
    "title": "| FoundationaLLM",
    "summary": "Azure Resource Providers Requirements to successfully deploy FoundationaLLM To successfully deploy FoundationaLLM, you need to have the following Azure Resource Providers accessible in your Azure Subscription with permissions to create them. For Quick Start Deployment microsoft.alertsmanagement/smartDetectorAlertRules Microsoft.App/containerApps Microsoft.App/managedEnvironments Microsoft.AppConfiguration/configurationStores Microsoft.CognitiveServices/accounts Microsoft.DocumentDB/databaseAccounts Microsoft.EventGrid/namespaces Microsoft.EventGrid/systemTopics Microsoft.Insights/components Microsoft.KeyVault/vaults Microsoft.ManagedIdentity/userAssignedIdentities Microsoft.OperationalInsights/workspaces Microsoft.Portal/dashboards Microsoft.Search/searchServices Microsoft.Storage/storageAccounts For Standard Deployment microsoft.alertsmanagement/smartDetectorAlertRules Microsoft.AppConfiguration/configurationStores Microsoft.CognitiveServices/accounts Microsoft.Compute/virtualMachineScaleSets Microsoft.ContainerService/managedClusters Microsoft.DocumentDB/databaseAccounts Microsoft.EventGrid/namespaces Microsoft.EventGrid/systemTopics Microsoft.Insights/actiongroups Microsoft.Insights/components Microsoft.Insights/metricalerts microsoft.insights/privateLinkScopes Microsoft.Insights/scheduledqueryrules Microsoft.KeyVault/vaults Microsoft.ManagedIdentity/userAssignedIdentities Microsoft.Network/loadBalancers Microsoft.Network/networkInterfaces Microsoft.Network/networkSecurityGroups Microsoft.Network/privateEndpoints Microsoft.Network/publicIPAddresses Microsoft.Network/virtualNetworks Microsoft.OperationalInsights/workspaces Microsoft.OperationsManagement/solutions Microsoft.Search/searchServices Microsoft.Storage/storageAccounts"
  },
  "docs/archive/deployment/configure-access-control-for-services.html": {
    "href": "docs/archive/deployment/configure-access-control-for-services.html",
    "title": "Configure access control for services | FoundationaLLM",
    "summary": "Configure access control for services FoundationaLLM takes a least privilege approach to access control. This means that by default, users have no access to any resources. You must explicitly grant access to users for each resource they need to access. This guide walks you through the process of granting access to users as needed. Prerequisites You have a FoundationaLLM solution deployed and running. You have a user account with the Contributor role on the Azure resource group or subscription where the solution is deployed. Azure App Configuration service The Azure App Configuration service provides FoundationaLLM solution components with a centralized location to store and manage application settings and feature flags. Role-based access controls (RBAC) are used to control access to the App Configuration service for managing settings and feature flags, as well as accessing them. The deployment scripts assign access controls to service accounts to allow them to read application settings and feature flags. You can use the same approach to grant access to users as needed. App Configuration roles The following table summarizes the roles FoundationaLLM uses for the App Configuration service: Role Description Target Users App Configuration Data Reader Allows users to read settings and feature flags. The system-assigned managed identities for the api services and any developers or admins that need to run the solution locally. App Configuration Data Owner Allows users to read and write settings and feature flags. Administrators that need to manage settings and feature flags. Grant access to the App Configuration service Use the steps below to grant access to the App Configuration service: Sign in to the Azure portal as at least a Contributor. Navigate to the resource group where the solution is deployed. Note If you performed an Azure Container Apps (ACA) or Azure Kubernetes Service (AKS) deployment, you will see an extra Resource Group that starts with ME_ or MC_ in addition to the Resource Group defined during the deployment. You will need to navigate to the Resource Group that does not start with ME_ or MC_ to access the App Configuration resource. 3. Select the App Configuration resource. The name should end with -appconfig. 4. Select Access Control (IAM) in the left-hand menu. ![The Access Control (IAM) menu item is highlighted.](media/appconfig-access-control-link.png) Select + Add and then Add role assignment. Select the App Configuration Data Reader or the App Configuration Data Owner role and then select Next. Select the + Select members link, search for user or group you want to grant access to, select the member to add them to the Selected members list, and then select Next. Select Review + assign to complete the assignment. Azure Key Vault service The Azure Key Vault service provides FoundationaLLM solution components with a centralized location to store and manage secrets. Role-based access controls (RBAC) are used to control access to the Key Vault service for managing secrets, as well as accessing them. The deployment scripts assign access controls to service accounts to allow them to read secrets. You can use the same approach to grant access to users as needed. Key Vault roles The following table summarizes the roles FoundationaLLM uses for the Key Vault service: Role Description Target Users Key Vault Secrets User Allows users to read secrets. The system-assigned managed identities for the api services and any developers or admins that need to run the solution locally. Key Vault Secrets Officer Allows users to read and write secrets. Administrators that need to manage secrets. Grant access to the Key Vault service Use the steps below to grant access to the Key Vault service: Sign in to the Azure portal as at least a Contributor. Navigate to the resource group where the solution is deployed. Note If you performed an Azure Container Apps (ACA) or Azure Kubernetes Service (AKS) deployment, you will see an extra Resource Group that starts with ME_ or MC_ in addition to the Resource Group defined during the deployment. You will need to navigate to the Resource Group that does not start with ME_ or MC_ to access the App Configuration resource. 3. Select the Key Vault resource. The name should end with -kv. 4. Select Access Control (IAM) in the left-hand menu. ![The Access Control (IAM) menu item is highlighted.](media/keyvault-access-control-link.png) Select + Add and then Add role assignment. Select the Key Vault Secrets User or the Key Vault Secrets Officer role and then select Next. Select the + Select members link, search for user or group you want to grant access to, select the member to add them to the Selected members list, and then select Next. Select Review + assign to complete the assignment."
  },
  "docs/archive/deployment/custom-domains.html": {
    "href": "docs/archive/deployment/custom-domains.html",
    "title": "Custom Domains | FoundationaLLM",
    "summary": "Custom Domains FoundationaLLM uses Azure Container Apps (ACA) or Azure Kubernetes Services (AKS) to deploy the various services needed to support the GenAI platform. Both of these support the ability to add custom domains. Azure Container Apps To add a custom domain to your ACA environments, perform the following: Open the Azure Portal. Browse to the subscription and resource group that contains the target FLLM instance. Select the ACA instance that you want to add a custom domain too. Select Settings, select Custom domains. Select Add custom domain. Type the domain you would like to add. In the dialog, notice the DNS entries you will need to add/modify in order to validate your environment. Once you have validated the domain, select Add. If you selected a managed certificate, after a few moments, an Azure based SSL certificate will be bound to your custom domain. For more information, reference Custom domain names and bring your own certificates in Azure Container Apps. Kubernetes For more information, reference Set up a custom domain name and SSL certificate with the application routing add-on. Application Registration Redirects URIs Two of the ACA instances (management and chat) will require you to add redirect urls in order for the custom domain to function properly. Open the Azure Portal. Browse to Microsoft Entra. Select Application Registrations. Search for the chat UI application, then select it. Under Manage, select Authentication. In the Single-page application Redirect URIs section, add your custom domain appended with signin-oidc. Select Save For the management UI, perform the following: Select Application Registrations. Search for the chat UI application, then select it. Under Manage, select Authentication. In the Single-page application Redirect URIs section, add your custom domain appended with /management/signin-oidc. Select Save."
  },
  "docs/archive/deployment/deployment-configuration.html": {
    "href": "docs/archive/deployment/deployment-configuration.html",
    "title": "Configuration for deployment | FoundationaLLM",
    "summary": "Configuration for deployment Configuration settings used by the FoundationaLLM platform Name Default Type Area Used by Description foundationallm-core-api-url Environment variable User Portal ChatThread.vue, index.vue The URL of the FoundationaLLM Core API. foundationallm-core-api-keyvault-name Environment variable Core API The name of the Azure Key Vault used by the Core API. foundationallm-core-api-entra-instance https://login.microsoftonline.com/ Environment variable Core API The Entra instance used by the Core API. foundationallm-core-api-entra-tenant-id View the Entra setup document for instructions Environment variable Core API The Entra tenant ID used by the Core API. foundationallm-core-api-entra-client-id Environment variable Core API The Entra client ID used by the Core API. foundationallm-core-api-entra-client-secret-name Key Vault secret Core API Name of the Entra client secret used by the Core API. foundationallm-core-api-entra-callback-path /signin-oidc Environment variable Core API The Entra callback path used by the Core API. foundationallm-core-api-entra-scopes Environment variable Core API The Entra scopes used by the Core API. foundationallm-core-api-gatekeeper-api-url Environment variable Core API The URL of the Gatekeeper API used by the Core API. foundationallm-gatekeeper-api-keyvault-name Environment variable Gatekeeper API The name of the Azure Key Vault used by the Gatekeeper API. foundationallm-gatekeeper-api-key Key Vault secret Gatekeeper API The first Gatekeeper API key. foundationallm-gatekeeper-api-orchestration-api-url Environment variable Gatekeeper API The URL of the Orchestration API used by the Gatekeeper API. foundationallm-orchestration-api-keyvault-name Environment variable Orchestration API The name of the Azure Key Vault used by the Orchestration API. foundationallm-orchestration-api-key Key Vault secret Orchestration API The first Orchestration API key. foundationallm-orchestration-api-agenthub-api-url Environment variable Orchestration API The URL of the Agent Hub API used by the Orchestration API. foundationallm-orchestration-api-prompthub-api-url Environment variable Orchestration API The URL of the Prompt Hub API used by the Orchestration API. foundationallm-orchestration-api-datasourcehub-api-url Environment variable Orchestration API The URL of the Data Source Hub API used by the Orchestration API. foundationallm-orchestration-api-langchain-api-url Environment variable Orchestration API The URL of the LangChain API used by the Orchestration API. foundationallm-orchestration-api-semantickernel-api-url Environment variable Orchestration API The URL of the Semantic Kernel API used by the Orchestration API. foundationallm-agenthub-api-key Key Vault secret Agent Hub API APIKeyValidator (SDK) The Agent Hub API key. foundationallm-prompthub-api-key Key Vault secret Prompt Hub API APIKeyValidator (SDK) The Prompt Hub API key. foundationallm-datasourcehub-api-key Key Vault secret Data Source Hub API APIKeyValidator (SDK) The Data Source Hub API key. foundationallm-langchain-api-key Key Vault secret LangChain API APIKeyValidator (SDK) The first LangChain API key. foundationallm-langchain-sqldb-testdb-server-name Environment variable LangChain API SqlDbConfig (SDK) The name of the Azure SQL Server used by the LangChain testdb SQL agent. foundationallm-langchain-sqldb-testdb-database-name Environment variable LangChain API SqlDbConfig (SDK) The name of the database used by the LangChain testdb SQL agent. foundationallm-langchain-sqldb-testdb-username Environment variable LangChain API SqlDbConfig (SDK) The user name used by the LangChain testdb SQL agent. foundationallm-langchain-sqldb-testdb-database-password Key Vault Secret LangChain API SqlDbConfig (SDK) The user password used by the LangChain testdb SQL agent. foundationallm-azure-openai-api-url Environment variable LangChain API AzureChatLLM (SDK) The URL of the Azure OpenAI API. foundationallm-azure-openai-api-completions-deployment Environment variable LangChain API AzureChatLLM (SDK) The name of the completions Azure Open AI deployment used by LangChain API. foundationallm-azure-openai-api-completions-model-version Environment variable LangChain API AzureChatLLM (SDK) The version of the completions model used by LangChain API. foundationallm-azure-openai-api-version Environment variable LangChain API AzureChatLLM (SDK) The version of the Azure Open AI API used by LangChain API. foundationallm-azure-openai-api-key Key Vault secret LangChain API AzureChatLLM (SDK) The key of the Azure Open AI API used by LangChain API. foundationallm-langchain-summary-model-name gpt-35-turbo Environment variable LangChain API SummaryAgent (SDK) The name of the summary model used by the LangChain summary agent. foundationallm-langchain-summary-max-tokens 4097 Environment variable LangChain API SummaryAgent (SDK) The maximum number of input tokens used by the LangChain summary agent. foundationallm-keyvault-name Environment variable LangChain API, PythonSDK AgentHub(SDK), DataSourceHub(SDK),PromptHub(SDK) The name of the Azure Key Vault used by the FoundationaLLM platform. foundationallm-configuration-allow-environment-variables Environment variable PythonSDK Configuration(SDK) When True checks environment first then key vault, otherwise checks App config (not yet implemented) then key vault foundationallm-prompt-metadata-storage-container Environment variable PythonSDK PromptHubStorageManager(SDK) The name of the Azure Blob Storage container where prompt metadata is stored. foundationallm-datasource-metadata-storage-container Environment variable PythonSDK DataSourceHubStorageManager(SDK) The name of the Azure Blob Storage container where data source metadata is stored. foundationallm-agent-metadata-storage-container Environment variable PythonSDK AgentHubStorageManager(SDK) The name of the Azure Blob Storage container where agent metadata is stored. Temporary configuration settings used by the FoundationaLLM platform NOTE: These configuration settings are temporary and will be removed in the future. Name Type Area Used by Description foundationallm-langchain-csv-file-url Environment variable LangChain API CSVAgent (SDK) The URL (including the SAS token) of the CSV file used by the LangChain CSV agent."
  },
  "docs/archive/deployment/deployment-quick-start.html": {
    "href": "docs/archive/deployment/deployment-quick-start.html",
    "title": "Deployment - Quick Start | FoundationaLLM",
    "summary": "Deployment - Quick Start FoundationaLLM is designed for seamless deployment within your Azure Subscription. It initially utilizes Azure Container Apps (ACA) for rapid deployment and streamlined development. For scaling up to production environments, FoundationaLLM also supports deployment on Azure Kubernetes Service (AKS), offering robust scalability and management features. Be mindful of the Azure OpenAI regional quota limits on the number of Azure OpenAI Service instances. To optimize resource usage, FoundationaLLM offers the flexibility to connect to an existing Azure OpenAI Service resource, thereby avoiding the creation of additional instances during deployment. This feature is particularly useful for managing resource allocation and ensuring efficient Azure OpenAI Service quota utilization. Prerequisites You will need the following resources and access to deploy the solution: Azure Subscription: An Azure Subscription is a logical container in Microsoft Azure that links to an Azure account and is the basis for billing, resource management, and allocation. It allows users to create and manage Azure resources like virtual machines, databases, and more, providing a way to organize access and costs associated with these resources. Subscription access to Azure OpenAI service: Access to Azure OpenAI Service provides users with the ability to integrate OpenAI's advanced AI models and capabilities within Azure. This service combines OpenAI's powerful models with Azure's robust cloud infrastructure and security, offering scalable AI solutions for a variety of applications like natural language processing and generative tasks. Start here to Request Access to Azure OpenAI Service Minimum quota of 65 CPUs across all VM family types: Azure CPU quotas refer to the limits set on the number and type of virtual CPUs that can be used in an Azure Subscription. These quotas are in place to manage resource allocation and ensure fair usage across different users and services. Users can request quota increases if their application or workload requires more CPU resources. Start here to Manage VM Quotas App Registrations created in the Entra ID tenant (formerly Azure Active Directory): Azure App Registrations is a feature in Entra ID that allows developers to register their applications for identity and access management. This registration process enables applications to authenticate users, request and receive tokens, and access Azure resources that are secured by Entra ID. Follow the instructions in the Authentication and Authorization setup document to configure authentication for the solution. User with the proper role assignments: Azure Role-Based Access Control (RBAC) roles are a set of permissions in Azure that control access to Azure resource management. These roles can be assigned to users, groups, and services in Azure, allowing granular control over who can perform what actions within a specific scope, such as a subscription, resource group, or individual resource. Owner on the target subscription Owner on the App Registrations described in the Authentication setup document You will use the following tools during deployment: Azure Developer CLI (v1.6.1 or greater) Azure CLI (v2.51.0 or greater): Latest Git PowerShell 7 (7.4.1 or greater) Optional To run or debug the solution locally, you will need to install the following dependencies: .NET 8 SDK Visual Studio 2022 Optional To build or test container images, you will need to install the following dependencies: Docker Desktop Important The perception of the main branch in GitHub, or any version control system, can vary depending on the development workflow adopted by a particular team or organization. The FoundationaLLM team uses the main branch as the primary development branch. In this case, the main branch might indeed be considered a work in progress, with developers regularly pushing changes and updates directly to it. It is where ongoing development work happens. So for deployment purposes, it is recommended to use the latest release branch, which is considered stable and tested. The release branch is a snapshot of the main branch at a specific point in time, where the code is considered stable and ready for deployment. The release branch is tagged with a version number, such as 0.6.0, and is the recommended branch for deployment. Please find our latest releases here Deployment steps Follow the steps below to deploy the solution to your Azure subscription. If you are upgrading from a previous version, like 0.5.0, please refer to the changes in the breaking changes notes. Important Follow the instructions in the Authentication and Authorization setup document to finalize authentication and authorization for the solution. Bear in mind that creating the app registrations in the Entra ID tenant is a prerequisite for the deployment, but you will have to revisit some of these settings after the deployment is complete later to fill in some missing values that are generated during the deployment. Ensure all the prerequisites are met and you have installed the tools required to complete the deployment. From a PowerShell prompt, execute the following to clone the repository: git clone https://github.com/foundationallm/foundationallm.git cd foundationallm/deploy/quick-start git checkout release/0.8.3 Run the following script to install the deployment utilities, including AzCopy, locally. cd .\\deploy\\common\\scripts .\\Get-AzCopy.ps1 Run the following commands to log into Azure CLI, Azure Developer CLI and AzCopy (the instance you just installed above): cd .\\deploy\\quick-start az login # Log into Azure CLI azd auth login # Log into AZD ..\\common\\tools\\azcopy\\azcopy login # Log into AzCopy Set up an azd environment targeting your Azure subscription and desired deployment region: # Set your target Subscription and Location azd env new --location <Supported Azure Region> --subscription <Azure Subscription ID> Run the following commands to set the appropriate application registration settings for OIDC authentication. cd .\\deploy\\quick-start ..\\common\\scripts\\Set-AzdEnvEntra.ps1 Optional: Bring Your Own Azure OpenAI Instance If you have an existing Azure OpenAI instance, you can use it by setting the following environment variables: azd env set OPENAI_NAME <OpenAI Name> azd env set OPENAI_RESOURCE_GROUP <OpenAI Resource Group> azd env set OPENAI_SUBSCRIPTION_ID <OpenAI Subscription ID> Important Deploying with Bring Your Own Azure OpenAI, customers need to make sure that the relevant Managed Identities (LangChain API, Semantic Kernel API, and Gateway API) are assigned the Open AI reader role on the Azure OpenAI account object. Deploy the solution After setting the OIDC-specific settings in the AZD environment above, run azd up in the same folder location to provision the infrastructure, update the App Configuration entries, deploy the API and web app services, and import files into the storage account. azd up Running script to allow MS Graph access through Role Permissions After the deployment is complete, you will need to run the following script to allow MS Graph access through Role Permissions. (See below) Important The user running the script will need to have the appropriate permissions to assign roles to the managed identities. The user will need to be a Global Administrator or have the Privileged Role Administrator role in the Entra ID tenant. The syntax for running the script from the deploy\\quick-start\\common\\scripts folder is: cd .\\deploy\\quick-start ..\\common\\scripts\\Set-FllmGraphRoles.ps1 -resourceGroupName rg-<azd env name> Finally, you will need to update the Authorization Callbacks in the App Registrations created in the Entra ID tenant by running the following script: cd .\\deploy\\quick-start ..\\common\\scripts\\Update-OAuthCallbackUris.ps1 Teardown To tear down the environment, execute azd down in the same folder location. azd down --purge Note The --purge argument in the command above. This ensures that resources that would otherwise be soft-deleted are instead completely purged from your Azure subscription."
  },
  "docs/archive/deployment/deployment-standard.html": {
    "href": "docs/archive/deployment/deployment-standard.html",
    "title": "Deployment - Standard with AKS | FoundationaLLM",
    "summary": "Deployment - Standard with AKS Compared to the quick start deployment using Azure Container Apps (ACA), the FoundationaLLM Standard Deployment with AKS is tailored for scaling up to production environments. It leverages Azure Kubernetes Service (AKS) for robust scalability and management, requiring an Azure Subscription with Azure OpenAI access. Be mindful of the Azure OpenaAI regional quota limits on the number of Azure OpenAI Service instances. This deployment option for FoundationaLLM uses Azure Kubernetes Service (AKS) to host the applications. Compared to Azure Container Apps (ACA) deployment, AKS provides more advanced orchestration and scaling capabilities suitable for larger workloads. The Standard Deployment will configure OpenAI instances to use the maximum quota available. If existing OpenAI resources are already deployed in the subscription, the Standard Deployment will not be able to deploy. The Standard Deployment should be deployed to a subscription with no existing OpenAI resources, or a new subscription should be created for the Standard Deployment. As a final option, the template can be updated to allocate a smaller quota. Prerequisites You will need the following resources and access to deploy the solution: Azure Subscription: An Azure Subscription is a logical container in Microsoft Azure that links to an Azure account and is the basis for billing, resource management, and allocation. It allows users to create and manage Azure resources like virtual machines, databases, and more, providing a way to organize access and costs associated with these resources. Subscription access to Azure OpenAI service: Access to Azure OpenAI Service provides users with the ability to integrate OpenAI's advanced AI models and capabilities within Azure. This service combines OpenAI's powerful models with Azure's robust cloud infrastructure and security, offering scalable AI solutions for a variety of applications like natural language processing and generative tasks. Start here to Request Access to Azure OpenAI Service Minimum quota of vCPUs across all VM family types: Azure CPU quotas refer to the limits set on the number and type of virtual CPUs that can be used in an Azure Subscription. For Azure Kubernetes Service (AKS) deployment, you will need a quota of around 24 vCPUs across all VM family types to ensure that the solution can be deployed and run successfully as the system deploys 2 clusters with 2 node pools each, each pool is set to scale between 1 and 3 instances where each instance has 2 vCPUS which brings the total to 24 vCPUS. These quotas are in place to manage resource allocation and ensure fair usage across different users and services. Users can request quota increases if their application or workload requires more CPU resources. Start here to Manage VM Quotas App Registrations created in the Entra ID tenant (formerly Azure Active Directory): Azure App Registrations is a feature in Entra ID that allows developers to register their applications for identity and access management. This registration process enables applications to authenticate users, request and receive tokens, and access Azure resources that are secured by Entra ID. Follow the instructions in the Authentication setup document to configure authentication for the solution. User with the proper role assignments: Azure Role-Based Access Control (RBAC) roles are a set of permissions in Azure that control access to Azure resource management. These roles can be assigned to users, groups, and services in Azure, allowing granular control over who can perform what actions within a specific scope, such as a subscription, resource group, or individual resource. Owner on the target subscription Owner on the app registrations described in the Authentication setup document You will use the following tools during deployment: Azure CLI (v2.51.0 or greater) git PowerShell 7 (7.4.1 or greater) Helm kubectl kubelogin Optional To run or debug the solution locally, you will need to install the following dependencies: .NET 8 SDK Visual Studio 2022 Optional To build or test container images, you will need to install the following dependencies: Docker Desktop Pre-Deployment steps Follow the steps below to deploy the solution to your Azure subscription. Ensure all the prerequisites are met and gather the following information (you will be prompted for it during the deployment): Azure Service Principal Credentials for Deployment Requires Directory Reader role on the Azure Tenant Requires Owner role on the target resource groups If the target resource groups do not already exist, requires Owner role on the target subscription Requires DNS Zone Contributor role on the DNS resource group (resource group containing private DNS zones necessary for private endpoint provisioning) Requires Network Contributor role on the HUB VNET resource (virtual network resource to which the FoundationaLLM VNET will be peered) Either Owner of the AD Group specified for the FoundationaLLM Administrator role or a member of said AD Group FoundationaLLM Project ID - a 3 to 8 character identifier for a FoundationaLLM deployment Target Azure Location - default is eastus2 Target Resource Groups for the specified workloads APP - defaults to rg-(azd environment name)-(azure location)-app-(project id) AUTH - defaults to rg-(azd environment name)-(azure location)-auth-(project id) DATA - defaults to rg-(azd environment name)-(azure location)-data-(project id) JBX - defaults to rg-(azd environment name)-(azure location)-jbx-(project id) NET - defaults to rg-(azd environment name)-(azure location)-net-(project id) OAI - defaults to rg-(azd environment name)-(azure location)-oai-(project id) OPS - defaults to rg-(azd environment name)-(azure location)-ops-(project id) STORAGE - defaults to rg-(azd environment name)-(azure location)-storage-(project id) VEC - defaults to rg-(azd environment name)-(azure location)-vec-(project id) Pre-created AD Group for the FoundationaLLM Administrator role (default name is FLLM-Admins) Pre-created AD Group for the FoundationaLLM Users role (default name is FLLM-Users) Pre-created and configured FoundationaLLM specific App Registrations necessary to facilitate RBAC (see here and the Entra setup script in deploy/common/scripts/Create-FllmEntraIdApps.ps1) FoundationaLLM-Core-API FoundationaLLM-Reader FoundationaLLM-Core-Portal FoundationaLLM-Management-API FoundationaLLM-Authorization-API FoundationaLLM-Management-Portal Private DNS Zone resource group, subscription Id (if it is in a different subscription), and tenant Id HUB virtual network resource, resource group, subscription Id (if it is in a different subscription), and tenant Id FoundationaLLM Virtual Network and AKS configurations AKS Service CIDR range (default is 10.100.0.0/16) FoundationaLLM VNET CIDR range (default is 10.220.128.0/20 - must be a /20 netmask) Allowed External CIDRs or IPs (default is 192.168.100.0/24,192.168.101.0/28) Backend AKS User Node Pool VM SKU (default is Standard_D8_v5) Backend AKS System Node Pool VM SKU (default is Standard_D2_v5) Frontend AKS User Node Pool VM SKU (default is Standard_D2_v5) Frontend AKS System Node Pool VM SKU (default is Standard_D2_v5) AKS Node Pool VM Availability Zones (default is 1,2,3) Ensure you have adequate quota and availability for selected SKUs in target region and availability zones - a minimal configuration using the same family requires a quota of 64 vCPUs Hostnames and associated SSL certificates in PFX format for the following endpoints User Portal (i.e chat.example.com) Management Portal (i.e. management.example.com) Core API (i.e. api.example.com) Management API (i.e. management-api.example.com) The SSL certificates will need to be copied to the corresponding folders in deploy/standard/certs Desired Azure Container Registry endpoint and credentials if you are escrowing container images and helm charts From a PowerShell prompt, execute the following to clone the repository: git clone https://github.com/foundationallm/foundationallm.git cd foundationallm git checkout release/0.9.6 Set up an azd environment targeting your Azure subscription and desired deployment region: # Set your target Subscription and Location azd env new --location <Supported Azure Region> --subscription <Azure Subscription ID> Provision SSL certificates for the appropriate domains and package them in PFX format. Place the PFX files in foundationallm/deploy/standard/certs following the naming convention below. The values for Host Name and Domain Name should match the values you provided in your deployment manifest: Service Name Host Name Domain Name File Name coreapi api example.com api.example.com.pfx managementapi management-api example.com management-api.example.com.pfx chatui chat example.com chat.example.com.pfx managementui management example.com management.example.com.pfx Provision Infrastructure Provision platform infrastructure with AZD: cd .\\deploy\\standard azd provision The deployment process will take some time. The AZD post-provisioning hook script will generate a hosts file in the .\\deploy\\standard\\config folder describing all the private endpoint IPs and the associated hostnames. These values can be used to populate your computer's local hosts file, or may assist with configuring your organization's DNS system. This guide will assume that you have taken the contents of the generated file and added them to your local hosts file. Configure and Deploy Ensure that you have network access to the deployed resources and that DNS resolution to deployed resources is configured (this is environment specific). Deploy to platform infrastructure with AZD: cd .\\deploy\\standard azd deploy The deployment process will take some time. The process will: Generate the configuration for the system. Load the configuration into App Configuration. Load default system files into Azure Storage. Configure the backend cluster. Create the FLLM namespace in the backend cluster Deploy the backend services to the cluster in the FLLM namespace Create the gateway-system namespace Deploy the secret class provider to the gateway-system namespace Deploy ingress-nginx Deploy Ingress Configurations and External Services Configure the frontend cluster. Create the FLLM namespace in the frontend cluster Deploy the frontend services to the cluster in the FLLM namespace Create the gateway-system namespace Deploy the secret class provider to the gateway-system namespace Deploy ingress-nginx Deploy Ingress Configurations and External Services Generate host file entries for the deployed services on AKS that you can add to your host file or DNS server. The AZD deploy hook script will generate a hosts.ingress file in the .\\deploy\\standard\\config folder describing the api and frontend endpoints and the associated hostnames. These values can be used to populate your computer's local hosts file, or may assist with configuring your organization's DNS system. This guide will assume that you have taken the contents of the generated file and added them to your local hosts file.16. Update your local hosts file with the entries from the generated host file. Running script to allow MS Graph access through Role Permissions After the deployment is complete, you will need to run the following script to allow MS Graph access through Role Permissions. (See below) Important The user running the script will need to have the appropriate permissions to assign roles to the managed identities. The user will need to be a Global Administrator or have the Privileged Role Administrator role in the Entra ID tenant. The syntax for running the script from the deploy\\standard folder is: cd .\\deploy\\standard ..\\common\\scripts\\Set-FllmGraphRoles.ps1 -resourceGroupName <APP workload resource group name> Important The user running the following script will need to have the appropriate permissions to update app registration configuration for the FoundationaLLM specific app registrations used to enable RBAC. This means either Owner role on the aforementioned app registrations or the Application Administrator role in the Azure tenant. Update the Authorization Callbacks in the App Registrations created in the Entra ID tenant by running the following script: cd .\\deploy\\standard ..\\common\\scripts\\Update-OAuthCallbackUris.ps1 Connect and Test Visit the chat UI in your browser and send a message to verify the deployment. The message can be very simple like \"Who are you?\". The default agent should respond with a message explaining it's persona."
  },
  "docs/archive/deployment/index.html": {
    "href": "docs/archive/deployment/index.html",
    "title": "Deployment | FoundationaLLM",
    "summary": "Deployment Deployment choices The following table summarizes the deployment choices available for the solution: Deployment type Description When to use CloudShell Coming Soon - Use Azure CloudShell to deploy the solution using only a browser. Best suited for situations where you want to deploy without needing to install anything in your local development environment. Quick Start Use your local development environment to deploy the solution to your Azure subscription. Best suited for situations where you need the flexibility of a full development environment (e.g. to customize the solution) and you have a local development environment available. Select the links in the table above to learn more about each deployment choice. Post-deployment configuration Authentication setup Follow the instructions in the Authentication setup document to configure authentication for the solution."
  },
  "docs/archive/deployment/soft-delete.html": {
    "href": "docs/archive/deployment/soft-delete.html",
    "title": "| FoundationaLLM",
    "summary": "Which resources in FoundationaLLM are soft-deleted? In FoundationaLLM, the following resources are soft-deleted: Azure OpenAI Resources Azure Key Vault Resources Azure AI Search Resources Azure Content Safety Resources Note If you do not use the azd down --purge command when you delete your resources, you will need to do so manually in the portal to purge (delete permenantly) these resources. Otherwise, you will have to make sure to name the resources differently when you redeploy the FoundationaLLM platform. Another concern is that you may exceed the capacity or tokens allowed for your subscription if you do not purge the resources before creating another one. So, be aware of these issues when you deploy and delete resources from your subscription."
  },
  "docs/archive/deployment/standard/manifest.html": {
    "href": "docs/archive/deployment/standard/manifest.html",
    "title": "Deployment Manifest Setup | FoundationaLLM",
    "summary": "Deployment Manifest Setup The Deployment Manifest is a JSON file that defines the configuration for a FoundationaLLM Standard deployment. You will find an empty Deployment Manifest template in foundationallm/deploy/standard/Deployment-Manifest.template.json. Once you have filled in the required values, you can use the Deployment Manifest to deploy the solution to your Azure subscription using the provided deployment scripts. Create the Deployment Manifest To create the Deployment Manifest, first copy the template file to a new file, for example Deployment-Manifest.json. Then, fill in the required values for your deployment. The following sections describe the different parts of the Deployment Manifest and the values you need to provide. Note You may create several deployment manifests for different environments, such as development, testing, and production. Each manifest should have the appropriate values for the environment in which you are deploying the solution. Deployment-Manifest.json is the default name expected by scripts. You can name your manifest files as you see fit and pass the file name as an argument to the deployment scripts. Property Values General The root section of the Deployment Manifest defines the general properties of the deployment. The following table describes each property and provides an example value. Name Description Value Example adminObjectId The Azure AD Group's Object ID designated as the deployment's admin. Entra Group ID 995a549b-067e-4fe3-9f90-98d78b9ed086 baseDomain The base domain for the deployment. Internet Domain Name example.com createVpnGateway Whether to create a VPN Gateway for the deployment. Boolean true environment A token for naming deployment resources in the environment. String dev, test, prod instanceId The unique ID for the deployment instance. GUID 5d40d2ee-aeb5-4391-95a0-1fd9045d7720 k8sNamespace The Kubernetes namespace for the FLLM Helm deployments. String fllm letsEncryptEmail The email address for Let's Encrypt notifications. Email Address admin@example.com location The Azure region where the deployment resources will be created. Azure Region eastus2, francecentral networkName The name of the network pre-provisioned before the deployment. String fllm-network project A token for naming deployment resources in the environment. String ai, fllm, rd, fred, sally subscription The Azure subscription ID for the deployment. GUID ad82622e-458a-4a48-8023-6b18eed1cf79 Notes createVpnGateway is a boolean value that determines whether a VPN Gateway should be created as part of the deployment. Set this value to true if you want to create a VPN Gateway. You do not need to create a VPN gateway if your networking environment already has a VPN gateway that you want to use or a similar solution like Express Route. instanceId is a GUID that uniquely identifies the deployment instance. You can generate a GUID using PowerShell or other tools. Each deployment instance should have a unique instanceId, this value is used by the authorization system when determining access to resources. This is similar to the subscription ID in Azure. letsEncryptEmail is the email address that will be used for Let's Encrypt notifications. Let's Encrypt is used to generate SSL certificates for the deployment. You do not need to provide this value unless you plan to use the optional pre-deployment script to generate certificates. If you already have certificates, or you plan to use a different certificate provider, you can leave this value blank. The deployment instructions will cover how to provide certificates during deployment. location is the Azure region where the deployment resources will be created. You should choose a region that supports OpenAI and the models needed by FoundationaLLM. The standard deployment supports automatically deploying the following models, not all models are available in every region, the template will configure the models supported in the specified location. Consult the Azure documentation and choose a region supporting the models you would like to use: gpt-35-turbo (0613) gpt-35-turbo (1106) gpt-4 (1106-Preview) gpt-4o (2024-05-13) text-embedding-ada-002 (2) text-embedding-3-large text-embedding-3-small networkName is the name of the network pre-provisioned before the deployment. The deployment will create the requird subnets and other networking resources in this network. If you do not have a pre-provisioned network, the template will create one for you. The network should be created in the networking resource group described later in the manifest. Entra Client IDs The entraClientIds section of the Deployment Manifest defines the client IDs for the different parts of the FoundationaLLM system. These client IDs are used by the authentication system to determine access to resources. The client IDs are unique to each deployment and should be kept secure. See the Authentication setup document for more information on the authentication system. Name Description Value See Also authorization The client ID for the authorization API. GUID API Application Setup chat The client ID for the chat service. GUID Client Application Setup core The client ID for the core API. GUID API Application Setup managementapi The client ID for the management API. GUID Management API Application Setup managementui The client ID for the management UI. GUID Management UI Application Setup vectorizationapi The client ID for the vectorization API. GUID Vectorization API Application Setup Entra Client Secrets The entraClientSecrets section of the Deployment Manifest provides the secrets use for authorization. Name Description Value authorization The client secret (password) for the authorization Application Registration A client secret value generated in the Entra portal Entra Instances The entraInstances section of the Deployment Manifest defines the cloud that can be used for authorization. In most cases this value will be https://login.microsoftonline.com/. Name Description Value authorization The login URL for the Entra cloud https://login.microsoftonline.com/ Entra Scopes The entraScopes section of the Deployment Manifest defines the scopes for the different parts of the FoundationaLLM system. These scopes are used by the authentication system to determine access to resources. See the Authentication setup document for more information on the authentication system. Name Description Example See Also authorization The scope for the authorization API api://FoundationaLLM-Authorization-Auth API Application Setup chat The scope for the chat service api://FoundationaLLM-Auth/Data.Read Client Application Setup core The scope for the core API Data.Read API Application Setup managementapi The scope for the management API Data.Manage Management API Application Setup managementui The scope for the management UI api://FoundationaLLM-Management-Auth/Data.Manage Management UI Application Setup vectorizationapi The scope for the vectorization API Data.Manage Vectorization API Application Setup Ingress Configuration The ingress section of the Deployment Manifest defines the configuration for the Ingress resources that route traffic to the different parts of the FoundationaLLM system. The Ingress resources are used to route traffic from the internet to the different services in the deployment. The following table describes the properties of the ingress section. Section Service Name Description Example apiIngress Ingress Configuration for the APIs in the backend AKS cluster coreapi The Ingress configuration for the Core API host The host name for the Ingress resource api.fllm.example.com path The path for the Ingress resource /core/ pathType The path type for the Ingress resource ImplementationSpecific serviceName The name of the service that the Ingress routes to core-api sslCert The SSL certificate to use for the Ingress resource coreapi managementapi The Ingress configuration for the Management API host The host name for the Ingress resource management-api.fllm.example.com path The path for the Ingress resource /management/ pathType The path type for the Ingress resource ImplementationSpecific serviceName The name of the service that the Ingress routes to management-api sslCert The SSL certificate to use for the Ingress resource managementapi vectorizationapi The Ingress configuration for the Vectorization API host The host name for the Ingress resource vectorization-api.fllm.example.com path The path for the Ingress resource /vectorization/ pathType The path type for the Ingress resource ImplementationSpecific serviceName The name of the service that the Ingress routes to vectorization-api sslCert The SSL certificate to use for the Ingress resource vectorizationapi frontendIngress Ingress Configuration for the portals in the frontend AKS cluster chatui The Ingress configuration for the Chat UI host The host name for the Ingress resource chat.fllm.example.com path The path for the Ingress resource / pathType The path type for the Ingress resource ImplementationSpecific serviceName The name of the service that the Ingress routes to chat-ui sslCert The SSL certificate to use for the Ingress resource chatui managementui The Ingress configuration for the Management UI host The host name for the Ingress resource management.fllm.example.com path The path for the Ingress resource / pathType The path type for the Ingress resource ImplementationSpecific serviceName The name of the service that the Ingress routes to management-ui sslCert The SSL certificate to use for the Ingress resource managementui Resource Group Configuration The resourceGroups section of the Deployment Manifest defines the names of the resource groups that will be created as part of the deployment. The following table describes the properties of the resourceGroups section. Name Description Example app The resource group for the application hosting resources (AKS). rg-ai-dev-eastus2-app auth The resource group for the authorization API storage resources. rg-ai-dev-eastus2-auth data The resource group for the customer source data resources. rg-ai-dev-eastus2-data dns The resource group for the Private DNS resources. rg-ai-dev-eastus2-dns jbx The resource group for the Jumpbox resources. rg-ai-dev-eastus2-jbx net The resource group for the networking resources. rg-ai-dev-eastus2-net oai The resource group for the OpenAI resources. rg-ai-dev-eastus2-oai ops The resource group for the operations resources. rg-ai-dev-eastus2-ops storage The resource group for the FLLM internal storage resources. rg-ai-dev-eastus2-storage vec The resource group for the vectorization resources. rg-ai-dev-eastus2-vec External Resource Group Configuration The externalResourceGroups section of the Deployment Manifest defines the names of the resource groups that contain resources that are external to the deployment. When pre-provisioning resources for FLLM, be sure to remove the corresponding entry from the resourceGroups section. The following table describes the properties of the externalResourceGroups section. Name Description Example dns The resource group containing pre-provisioned Private DNS resources. rg-ai-shared-eastus2-dns Next Return to the standard deployment steps."
  },
  "docs/archive/how-to-guides/create-model-agnostic-agent-claude.html": {
    "href": "docs/archive/how-to-guides/create-model-agnostic-agent-claude.html",
    "title": "Overview | FoundationaLLM",
    "summary": "Overview In this step-by-step guide, you will create a model agnostic agent using Claude, a code interpreter tool that uses Python custom containers and a knowledge search tool that uses the uploaded files as a data source. Creating a model agnostic agent Navigate to the Management Portal. Select Create New Agent from the menu bar. In the Agent Name provide a unique name for the agent. In the Agent Display Name provide a user friendly name for the agent. In the Description, provide a description of the agent. Under User Portal Experience, change Would you like to allow the uder to upload files? to Yes. In the Knowledge Source section, under does this agent have an inline context, select Yes. In the Workflow section, and provide the following values: What workflow should the agent use? Select ExternalAgentWorkflow in the drop down. Workflow name: MAA-Workflow Workflow package name: foundationallm_agent_plugins Workflow class name: FoundationaLLMFunctionCallingWorkflow Workflow host: select LangChain Workflow main model: select a Claude based model from the list. Under workflow main model parameters, select Add Property and in the dialog that appears provide these values: Property Key: temperature Property Type: number Property Value: 0.5 Select Save to create the property. Under what is the main workflow prompt, copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Main.txt. Create the Workflow Prompts Open a new browser window to the Management Portal. Select Prompts. First you will create the main workflow prompts. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Files. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions that are specific for the identification of the files that are relevant to the question. Category: Workflow Prompt Prefix: Copy and paste the prompt from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Files.txt Select Create Prompt Next, create the Workflow Final prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Final. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions to build the final response based on the results provided by tools. Category: Workflow Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Final.txt. Select Create Prompt Next, create the Workflow Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions that are specific for the selection of tools. Category: Workflow Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Router.txt. Select Create Prompt Create the Tool Prompts First, create the Tool Code Main prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Code-Main. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Main.txt. Select Create Prompt Next, create the Tool Code Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Code-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides additional instructions for the selection of this tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Router.txt. Select Create Prompt Next, create the Tool Knowledge Main prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Knowledge-Main. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Main.txt. Select Create Prompt Next, create the Tool Knowledge Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Knowledge-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Router.txt. Select Create Prompt Configure the Prompt Resources Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Router prompt (e.g., Your-Agent-Name--Workflow-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save to add the prompt resource. Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Files prompt (e.g., Your-Agent-Name--Workflow-Files) you previously created for this agent. Resource Role: Enter files_prompt. Select Save to add the prompt resource. Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Final prompt (e.g., Your-Agent-Name--Workflow-Final) you previously created for this agent. Resource Role: Enter final_prompt. Select Save to add the prompt resource. Configure the Tools First, you will add a code interpreter tool. Under the Tools sections select Add New Tool. Enter the following values: Tool name enter Code-01. This MUST be called Code-01 to match the name used in the prompts. Tool description: Answers questions that require dynamic generation of code. Tool package name: foundationallm_agent_plugins Tool class name: FoundationaLLMCodeInterpreterTool Under Tool resources, select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Model Resource: Select your Claude model Resource Role: Enter main_model. Select Save. Expand the newly created AI model object and select Add Property. Property Key: model_parameters Property Type: Select Object / Array Property Value: Select text and then enter the following and select Save: { \"temperature\": 0.2 } Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Code Interpreter Main prompt (e.g., Your-Agent-Name-Tool-Code-Main) you previously created for this agent. Resource Role: Enter main_prompt. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Code Interpreter Router prompt (e.g., Your-Agent-Name-Tool-Code-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save. Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_required Property Type: Boolean Property Value: True Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_endpoint_provider Property Type: String Property Value: AzureContainerAppsCustomContainer Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_language Property Type: String Property Value: Python In the Configure Tool dialog, select Save. Next, you will add Knowledge Conversation Files tool. Under the Tools sections select Add New Tool. Enter the following values: Tool name enter Knowledge-Conversation-Files. This MUST be called Knowledge-Conversation-Files to match the name used in the prompts. Tool description: Retrieves content from files uploaded to conversations. Tool package name: foundationallm_agent_plugins Tool class name: FoundationaLLMKnowledgeTool Under Tool resources, select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Model Resource: Select your Claude model Resource Role: Enter main_model. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Knowledge Main prompt (e.g., Your-Agent-Name-Tool-Knowledge-Main) you previously created for this agent. Resource Role: Enter main_prompt. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Knowledge Router prompt (e.g., Your-Agent-Name-Tool-Knowledge-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Data Pipeline Resource: Select DefaultFileUpload Resource Role: Enter file_upload_data_pipeline. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Vector Database Resource: Select ConversationFiles Resource Role: Enter vector_database. Select Save. Under Tool properties, select Add Property and provide these values, then select Save: Property Key: embedding_model Property Type: String Property Value: text-embedding-3-large Under Tool properties, select Add Property and provide these values, then select Save: Property Key: embedding_dimensions Property Type: Number Property Value: 2048 In the Configure Tool dialog, select Save. Select Create Agent to save the new agent."
  },
  "docs/archive/how-to-guides/create-model-agnostic-agent-gpt4o.html": {
    "href": "docs/archive/how-to-guides/create-model-agnostic-agent-gpt4o.html",
    "title": "Overview | FoundationaLLM",
    "summary": "Overview In this step-by-step guide, you will create a model agnostic agent using GPT-4o, a code interpreter tool that uses Python custom containers and a knowledge search tool that uses the uploaded files as a data source. Creating a model agnostic agent Navigate to the Management Portal. Select Create New Agent from the menu bar. In the Agent Name provide a unique name for the agent. In the Agent Display Name provide a user friendly name for the agent. In the Description, provide a description of the agent. Under User Portal Experience, change Would you like to allow the uder to upload files? to Yes. In the Knowledge Source section, under does this agent have an inline context, select Yes. In the Workflow section, and provide the following values: What workflow should the agent use? Select ExternalAgentWorkflow in the drop down. Workflow name: MAA-Workflow Workflow package name: foundationallm_agent_plugins Workflow class name: FoundationaLLMFunctionCallingWorkflow Workflow host: select LangChain Workflow main model: select a GPT4o based model from the list. Under workflow main model parameters, select Add Property and in the dialog that appears provide these values: Property Key: temperature Property Type: number Property Value: 0.5 Select Save to create the property. Under what is the main workflow prompt, copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Main.txt. Create the Workflow Prompts Open a new browser window to the Management Portal. Select Prompts. First you will create the main workflow prompts. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Files. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions that are specific for the identification of the files that are relevant to the question. Category: Workflow Prompt Prefix: Copy and paste the prompt from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Files.txt Select Create Prompt Next, create the Workflow Final prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Final. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions to build the final response based on the results provided by tools. Category: Workflow Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Final.txt. Select Create Prompt Next, create the Workflow Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Workflow-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides instructions that are specific for the selection of tools. Category: Workflow Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Router.txt. Select Create Prompt Create the Tool Prompts First, create the Tool Code Main prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Code-Main. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Main.txt. Select Create Prompt Next, create the Tool Code Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Code-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides additional instructions for the selection of this tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Router.txt. Select Create Prompt Next, create the Tool Knowledge Main prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Knowledge-Main. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Main.txt. Select Create Prompt Next, create the Tool Knowledge Router prompt. Select Create Prompt. Provide the values as follows: Prompt Name: Your-Agent-Name-Tool-Knowledge-Router. Replace Your-Agent-Name with the name of your agent. Description: Provides the main instructions for the tool. Category: Tool Prompt Prefix: Copy and paste the prompt from https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Router.txt. Select Create Prompt Configure the Prompt Resources Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Router prompt (e.g., Your-Agent-Name--Workflow-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save to add the prompt resource. Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Files prompt (e.g., Your-Agent-Name--Workflow-Files) you previously created for this agent. Resource Role: Enter files_prompt. Select Save to add the prompt resource. Under the additional workflow resources, select Add Workflow Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Final prompt (e.g., Your-Agent-Name--Workflow-Final) you previously created for this agent. Resource Role: Enter final_prompt. Select Save to add the prompt resource. Configure the Tools First, you will add a code interpreter tool. Under the Tools sections select Add New Tool. Enter the following values: Tool name enter Code-01. This MUST be called Code-01 to match the name used in the prompts. Tool description: Answers questions that require dynamic generation of code. Tool package name: foundationallm_agent_plugins Tool class name: FoundationaLLMCodeInterpreterTool Under Tool resources, select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Model Resource: Select your GPT4o model Resource Role: Enter main_model. Select Save. Expand the newly created AI model object and select Add Property. Property Key: model_parameters Property Type: Select Object / Array Property Value: Select text and then enter the following and select Save: { \"temperature\": 0.2 } Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Code Interpreter Main prompt (e.g., Your-Agent-Name-Tool-Code-Main) you previously created for this agent. Resource Role: Enter main_prompt. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Code Interpreter Router prompt (e.g., Your-Agent-Name-Tool-Code-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save. Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_required Property Type: Boolean Property Value: True Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_endpoint_provider Property Type: String Property Value: AzureContainerAppsCustomContainer Under Tool properties, select Add Property and provide these values, then select Save: Property Key: code_session_language Property Type: String Property Value: Python In the Configure Tool dialog, select Save. Next, you will add Knowledge Conversation Files tool. Under the Tools sections select Add New Tool. Enter the following values: Tool name enter Knowledge-Conversation-Files. This MUST be called Knowledge-Conversation-Files to match the name used in the prompts. Tool description: Retrieves content from files uploaded to conversations. Tool package name: foundationallm_agent_plugins Tool class name: FoundationaLLMKnowledgeTool Under Tool resources, select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Model Resource: Select your GPT4o model Resource Role: Enter main_model. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Knowledge Main prompt (e.g., Your-Agent-Name-Tool-Knowledge-Main) you previously created for this agent. Resource Role: Enter main_prompt. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Prompt Resource: Select the Knowledge Router prompt (e.g., Your-Agent-Name-Tool-Knowledge-Router) you previously created for this agent. Resource Role: Enter router_prompt. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Data Pipeline Resource: Select DefaultFileUpload Resource Role: Enter file_upload_data_pipeline. Select Save. Select Add Tool Resource. In the Add Resource dialog, provides these values: Resource Type: Vector Database Resource: Select ConversationFiles Resource Role: Enter vector_database. Select Save. Under Tool properties, select Add Property and provide these values, then select Save: Property Key: embedding_model Property Type: String Property Value: text-embedding-3-large Under Tool properties, select Add Property and provide these values, then select Save: Property Key: embedding_dimensions Property Type: Number Property Value: 2048 In the Configure Tool dialog, select Save. Select Create Agent to save the new agent."
  },
  "docs/archive/knowledge-management-agent.html": {
    "href": "docs/archive/knowledge-management-agent.html",
    "title": "Knowledge management agent | FoundationaLLM",
    "summary": "Knowledge management agent The FoundationaLLM (FLLM) Knowledge Management agent type supports the following scenarios: With an Inline Context: Knowledge Management agents with an Inline Context pass the user's prompt directly to the Large Language Model (LLM). Without an Inline Context: Knowledge Management agents without an Inline Context implement the Retrieval Augmented Generation (RAG) design pattern. RAG augments the user prompt with additional context to generate a more accurate response. The RAG flow uses a retrieval model to retrieve relevant documents from a knowledge base, such as a vector store, and then uses the retrieved documents to augment the user prompt before sending it to the LLM. The creation of a Knowledge Management agent without an Inline Context requires an existing knowledge base, such as a vector store. Use the Vectorization API to create a vector store prior to the creation of the agent. Knowledge Management Agent Configuration The Knowledge Management agent configuration may reference the following resources: Vectorization text embedding profile: The text embedding profile contains the configuration of the text embedding model used to embed the user prompt and perform a vector search in the knowledge base. This must match the text embedding profile used to populate the knowledge base. Vectorization indexing profile: The indexing profile contains the configuration of the service hosting the index. Prompt: The system prompt of the agent, describing the persona of the agent. Note: The Knowledge Management agent implementation currently supports the AzureAISearchIndexer indexing profile. The structure of a Knowledge Management agent is the following: { \"type\": \"knowledge-management\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Agent/agents/<name>\", \"description\": \"<description>\", \"display_name\": \"<display_name>\", \"inline_context\": true, \"vectorization\": { \"dedicated_pipeline\": \"\", \"data_source_object_id\": \"<data_source_object_id>\", \"indexing_profile_object_id\": \"<indexing_profile_object_id>\", \"text_embedding_profile_object_id\": \"<text_embedding_profile_object_id>\", \"text_partitioning_profile_object_id\": \"<text_partitioning_profile_object_id>\", \"vectorization_data_pipeline_object_id\": \"\", \"trigger_type\": \"\", \"trigger_cron_schedule\": \"\" }, \"prompt_object_id\": \"<prompt_resource_objectid>\", \"language_model\": { \"type\": \"openai\", \"provider\": \"microsoft\", \"temperature\": 0.0, \"use_chat\": true, \"api_endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"version\": \"FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion\", \"deployment\": \"FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName\" }, \"sessions_enabled\": true, \"conversation_history\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper\": { \"use_system_setting\": false, \"options\": [ \"ContentSafety\", \"Presidio\" ] }, \"orchestration_settings\": { \"orchestrator\": \"LangChain\", \"endpoint_configuration\": { \"endpoint\": \"\", \"api_version\": \"\", \"api_key\": \"\", \"auth_type\": \"\", \"api_key\": \"\", \"provider\": \"\", \"operation_type\": \"chat\" }, \"model_parameters\": { \"deployment_name\": \"\" } } } where: <name> is the name of the agent. <instance_id> is the instance ID of the deployment. <description> is the description of the agent. Ensure that this description details the purpose of the agent. <display_name> controls the title of the agent in the Chat UI dropdown menu. <data_source_object_id> is the object ID of the Data Source resource. <indexing_profile_object_id> is the object ID of the indexing profile resource. <text_embedding_profile_object_id> is the object ID of the text embedding profile resource. <text_partitioning_profile_object_id> is the object ID of the text partitioning profile resource. <prompt_resource_objectid> is the object ID of the prompt resource. Parameter Description type The type of the agent - will always be knowledge-management. type must be the first key in the request body. name The name of the agent. object_id The object ID of the agent. Remove this element when creating an agent as this is generated by the Management API. description The description of the agent, ensure this description details the purpose of the agent. display_name The title of the agent in the Chat UI dropdown menu. This field is optional. inline_context Whether or not the agent has an Inline Context. vectorization The vectorization object is only required for Knowledge Management agents without an Inline Context (inline_context is false). If the vectorization object is included, the indexing_profile_object_id and text_embedding_profile_object_id keys are required. vectorization.dedicated_pipeline A boolean indicating whether or not the agent has a dedicated Vectorization pipeline (implemented in an upcoming release). vectorization.data_source_object_id The object ID of the Data Source resource. vectorization.indexing_profile_object_id The object ID of the indexing profile resource. vectorization.text_embedding_profile_object_id The object ID of the text embedding profile resource. vectorization.text_partitioning_profile_object_id The object ID of the text partitioning profile resource. vectorization.vectorization_data_pipeline_object_id The resource ID of the agent's Vectorization pipeline (implemented in an upcoming release). vectorization.trigger_type The trigger type of the agent's Vectorization pipeline (implemented in an upcoming release). Permissible values are Manual, Schedule, and Event. vectorization.trigger_cron_schedule The schedule of the trigger in Cron format (implemented in an upcoming release). This property is valid only when trigger_type is Schedule. prompt_object_id The object ID of the prompt resource. language_model The language model configuration. The language_model object has been deprecated as of release 0.6.0. language_model.type The type of the language model. Currently supporting OpenAI based langauge models. language_model.provider The provider of the language model. Currently supporting microsoft or openai. language_model.temperature The temperature value for the language model. A value between 0 and 1. Values closer to 0 return more factual information whereas values closer to 1 yield more creative responses. language_model.use_chat Determines the type of language model to use, as an example, when using Microsoft's Azure OpenAI, specifying use_chat equal to true will use the AzureChatOpenAI model vs. the AzureOpenAI model in LangChain. language_model.api_endpoint The configuration setting key that houses the API endpoint of the language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.api_key The configuration setting key that houses a reference to a key vault value containing the API key for the language model service. Ensure these values are populated in key vault and app configuration. language_model.api_version The configuration setting key that houses the API version of the language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.version The configuration setting key that houses the version of the language model deployment. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.deployment The configuration setting key that houses the name given to the deployed language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. sessions_enabled A boolean value that indicates whether the agent is session-less (false) or supports sessions(true). conversation_history The conversation history configuration. conversation_history.enabled Indicates if conversation history is retained for subsequent agent interactions(true). conversation_history.max_history indicates the number of messages to be retained. gatekeeper The gatekeeper configuration. gatekeeper.use_system_setting Indicates if the system settings are used for the gatekeeper. gatekeeper.options Contains the list of gatekeeper options. The sample provided overrides the system setting for gatekeeper and enables Azure Content Safety and MS Presidio in the messaging pipeline. orchestration_settings The settings for the agent orchestrator. orchestration_settings.orchestrator FoundationaLLM currently supports LangChain and SemanticKernel for both types of Knowledge Management agents; however, Knowledge Management agents with an Inline Context can also use the AzureOpenAIDirect and AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration The endpoint configuration of the hosted LLM. FoundationaLLM currently supports Azure OpenAI and OpenAI. orchestration_settings.endpoint_configuration.endpoint The endpoint URL of the hosted LLM. The URL should be provided directly for the LangChain or SemanticKernel orchestrators; it should be provided as an Azure App Configuration key reference for the AzureOpenAIDirect or AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration.api_version The API version of the hosted LLM. For Azure OpenAI, this value should be set to the latest GA version. The API version should be provided directly for the LangChain or SemanticKernel orchestrators; it should be provided as an Azure App Configuration key reference for the AzureOpenAIDirect or AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration.auth_type The authentication method of the hosted LLM. This value can either be token or key. For Azure OpenAI deployments, this value should be token, which configures the orchestrator to use Managed Identities for authentication. key-based authentication uses API keys. orchestration_settings.endpoint_configuration.api_key The name of the Azure App Configuration key storing the LLM endpoint API key. This parameter is required if auth_type is set to key. orchestration_settings.endpoint_configuration.provider The provider of the hosted LLM. FoundationaLLM currently supports microsoft (Azure OpenAI) or openai. orchestration_settings.endpoint_configuration.operation_type This field is set to chat by default and can be omitted. orchestration_settings.model_parameters Endpoint-specific model parameters. This field must be non-null if the provider is microsoft. orchestration_settings.model_parameters.deployment_name This field should be set to the name of the Azure OpenAI model deployment if the provider is microsoft. AzureOpenAIDirect Orchestrator The AzureOpenAIDirect orchestrator passes the user's prompt to an LLM deployed in an instance of Azure OpenAI Service, bypassing LangChain or Semantic Kernel. Example Configuration: { \"orchestration_settings\": { \"orchestrator\": \"AzureOpenAIDirect\", \"endpoint_configuration\": { \"endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"auth_type\": \"key\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"operation_type\": \"chat\" }, \"model_parameters\": { \"deployment_name\": \"completions\" } } } Note: AzureOpenAIDirect is only compatible with Knowledge Management agents with an Inline Context. AzureAIDirect Orchestrator The AzureAIDirect orchestrator passes the user's prompt to an LLM deployed as an Azure AI Studio real-time endpoint. This orchestrator allows customers to use a wider range of LLMs with FLLM agents. Example Configuration: { \"orchestration_settings\": { \"orchestrator\": \"AzureAIDirect\", \"endpoint_configuration\": { \"endpoint\": \"<AZURE APP CONFIGURATION KEY>\", \"api_key\": \"<AZURE APP CONFIGURATION KEY>\" }, \"model_parameters\": { \"temperature\": 0.8, \"max_new_tokens\": 1000, \"deployment_name\": \"<AZURE AI STUDIO DEPLOYMENT NAME>\" } } } Note: AzureAIDirect is only compatible with Knowledge Management agents with an Inline Context. Managing Knowledge Management Agents This section describes how to manage knowledge management agents using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name> Content-Type: application/json BODY <agent_configuration> where <agent_configuration> is the JSON agent configuration structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name> Note FLLM currently implements logical deletes for Knowledge Management agents. This means that users cannot create a Knowledge Management agent with the same name as a deleted Knowledge Management agent. Support for purging Knowledge Management agents will be added in a future release. Validating a Knowledge Management Agent Once configured, the knowledge management agent can be validated using an API call to the Core API or via the User Portal. Note It can take up to 5 minutes for a new Knowledge Management agent to appear in the User Portal or be accessible for requests from the Core API. Overriding agent parameters The agent parameters can be overridden at the time of the API call. Refer to the Core API documentation for more information."
  },
  "docs/archive/operations/backups.html": {
    "href": "docs/archive/operations/backups.html",
    "title": "FoundationaLLM Backups & data resiliency | FoundationaLLM",
    "summary": "FoundationaLLM Backups & data resiliency Before implementing any backup strategy, it's important to carefully plan and consider factors such as recovery time objectives (RTO), recovery point objectives (RPO), and compliance requirements. Choose the method or combination of methods that best align with your specific backup and recovery needs. CosmosDB Ensuring regular backups for Azure Cosmos DB is crucial to protect data such as private agents, user profiles, and chat history. Backups play a vital role in safeguarding your data against accidental deletions, data corruption, or other unforeseen issues. Here are some key points to consider when planning your Cosmos DB backup strategy: Backup and Restore: Automated Backups: Azure Cosmos DB includes automated backups that are taken at regular intervals. These backups are used to provide point-in-time restore capabilities. Retention Period: Backups are retained for a specific retention period, allowing you to restore your data to a previous state within that time frame. The options for retention period are 7 or 30 days. The Standard deployment configures 30 days retention. How to Configure Backup Policy: Backup policies can be configured at the Cosmos DB account level, specifying the frequency and duration of backups. Restore from Backup: You can initiate a point-in-time restore using the Azure Portal, Azure PowerShell, or Azure SDKs. Data Resiliency: Global Distribution: Distributing your Cosmos DB data globally across multiple regions ensures that your data is available even in the face of regional outages. This enhances data resiliency and availability. The Standard Deployment does not currently enable global data distribution. Data is replicated within a single region by the Cosmos DB service. Consistency Levels: Azure Cosmos DB offers various consistency levels, allowing you to balance between consistency and availability based on your application's requirements. The Standard Deployment uses Session consistency by default. Manual Backups and Data Migration: Export and Import: You can manually export your Cosmos DB data to Azure Storage or another Cosmos DB account, providing an additional layer of backup and migration capability. The Standard Deployment does not configure this capability by default. Storage Accounts Backing up the storage account where your prompts, agents, and data sources are defined is crucial to ensure the integrity and availability of your conversational data. Here are steps you can take to back up an Azure Storage account: Azure Storage Account Replication: Azure Storage offers built-in redundancy options like Locally Redundant Storage (LRS), Zone-Redundant Storage (ZRS), Geo-Redundant Storage (GRS), and Read-Access Geo-Redundant Storage (RA-GRS). These options replicate your data across different locations for high availability and durability. By default, LRS is enabled for all new storage accounts in the Standard Deployment. You can change the replication option for an existing storage account by navigating to the Replication tab in the Azure portal. Azure Backup: Azure Backup service allows you to create backups of your virtual machines, files, and databases, including Azure Storage accounts. You can configure backup policies and retention rules to meet your data protection requirements. Configuring Azure Backup is not currently enabled in the Standard Deployment, but you can use the Azure portal to manually configure back up for your storage account. Azure Blob Storage Versioning: Azure Blob Storage versioning is a feature that allows you to enable versioning on your storage account. When versioning is enabled, any update or deletion of a blob results in the creation of a new version of that blob. This helps you maintain a historical record of changes made to your data. This feature is enabled in the Standard Deployment. Azure Blob Storage Soft Delete: Soft delete is a feature in Azure Blob Storage that provides an extra layer of protection against accidental data deletion. When soft delete is enabled, deleted blobs are retained for a specified retention period before being permanently deleted. In the Standard Deployment, soft delete is enabled for 30 days for blobs and containers. Key Vault Azure Key Vault provides several features to help you protect and manage your keys and secrets effectively. Purge Protection: Purge protection is a feature in Azure Key Vault that helps prevent the permanent deletion of a key vault. When purge protection is enabled, the key vault cannot be permanently deleted immediately after deletion. Instead, there is a retention period during which the key vault is retained, and it can be recovered. Key Vaults in the Standard Deployment have purge protection enabled by default and deleted Key Vaults are retained for 7 days. Soft Delete: Soft delete is a feature that protects keys, secrets, and certificates in Azure Key Vault from immediate and irreversible deletion. When soft delete is enabled, deleted items are retained for a specified retention period before they are permanently deleted. Soft delete is enabled by default for all new key vaults in the Standard Deployment. Deleted keys, secrets, and certificates are retained for 7 days. Secret Versioning: Secret versioning is a feature that allows you to store multiple versions of a secret within a key vault. Each time you create or update a secret, a new version is generated. Secret versions help you maintain a history of changes and facilitate rollbacks if needed. This feature is enabled on all Key Vaults in Azure. Backups: Azure Key Vault provides a backup and restore capability, allowing you to create backups of your key vault's keys, secrets, and certificates. These backups can be used for data recovery and protection against accidental data loss. There is no way to backup the entire Key Vault or to schedule regular backups. App Config Azure App Configuration provides features related to backup, versioning, and data resiliency to help you effectively manage and deploy application configuration settings. Backup in Azure App Configuration: Azure App Configuration allows you to back up your configuration settings, including feature flags, connection strings, and other key-value pairs using the Import/Export feature. Backups can be sent to another App Configuration instance, an App Service, or a local file. Versioning in Azure App Configuration: Azure App Configuration automatically version-controls your configuration settings. Each change to a key-value pair creates a new version. Versioning helps track changes to configuration settings over time. You can access and roll back to previous versions of a key-value pair. Data Resiliency in Azure App Configuration: Azure App Configuration is designed with built-in redundancy across multiple regions to ensure high availability and data resiliency. App Configuration supports multi-region replication, allowing you to replicate your configuration settings to different regions for additional resilience. This feature is not currently enabled in the Standard Deployment."
  },
  "docs/archive/operations/graph-api-permissions.html": {
    "href": "docs/archive/operations/graph-api-permissions.html",
    "title": "Why FoundationaLLM needs Graph API permissions | FoundationaLLM",
    "summary": "Why FoundationaLLM needs Graph API permissions Summary A fully functional, robust, and scalable Role-Based Access Control (RBAC) implementation is critical for the success of any enterprise software platform. This is especially true with Generative AI platforms where the new ways in which data is transported, processed, and transformed must be secured and governed. A fully functional RBAC implementation relies on capabilities provided by the underlying identity management platform like: Enumeration of users, groups, and security principals Mapping of object identifiers to their display names Identification of security groups membership. The RBAC implementation provided by FoundationaLLM is designed from the ground up to provide the necessary security layers required by a Generative AI platform deployed in the enterprise. For transparency and auditing purposes, FoundationaLLM keeps all its source code out in the open (in a public GitHub repository) allowing all of the claims made about the behavior of the code to be validated and confirmed by any interested party. Context Graph API permissions are required for the managed identities of the two surface APIs of the platform: Core API Management API These APIs are involved in authenticating their clients (the FoundationaLLM portals and any other actor calling them). They are also involved in enforcing and managing the underlying Role-Based Access Control model of the platform. Rationale behind the architectural decision When authenticating against Graph API, any custom-built API has two choices: Using an Entra ID application registration with a client secret or using a managed identity In both cases, the underlying service principal (the enterprise application associated with the application registration or the managed identity itself) will need the proper permissions to the Graph API to fulfill its tasks. According to Microsoft's official documentation: \"Managed identities provide an automatically managed identity in Microsoft Entra ID for applications to use when connecting to resources that support Microsoft Entra ID authentication. Applications can use managed identities to obtain Microsoft Entra ID tokens without having to manage any credentials.\" The recommended path forward is using managed identities as opposed to any approach that involves using secrets that need to be managed (plain or certificates). This is the underlying justification for our architectural decision to assign permissions to the managed identities. Justification of the need Any standard Role-Based Access Control capability requires the following core features (none of them being optional): Based on the object ID of a security principal, retrieve the security groups it belongs to (either directly or transitively). Based on a list of object IDs of security principals, retrieve their display names (to display meaningful information in the RBAC management experience). Select users, groups, or service principals that are targets for role assignments in the RBAC model. Note \"Entry-level\" approach is to ask for group membership in access tokens. This approach has significant limitations as it is limited to a small number of groups which, if exceeded, will result in a Graph API reference added to the access token, forcing the code to call into Graph API. This limit is hit with almost certainty in enterprise scenarios, hance our decision to use the robust approach (directly calling Graph API) by default. Based on all of the above, the justification for the requested roles is as follows: Group.Read.All - required to get the group membership for security principals authenticated against the APIs which is in turn used to evaluate their role membership and level of access to the capabilities of the platform. Also required to list security groups when FoundationaLLM RBAC roles are assigned to them. User.Read.All - required for group membership retrieval and to list users when FoundationaLLM RBAC roles are assigned to them. Application.Read.All - required for group membership retrieval and to list security principals when FoundationaLLM RBAC roles are assigned to them. Note From the Graph API documentation (https://learn.microsoft.com/en-us/graph/permissions-reference): \"In some cases, an app might need extra permissions to read some group properties like member and memberOf. For example, if a group has one or more service principals as members, the app also needs permissions to read service principals, otherwise Microsoft Graph returns an error or limited information.\" Transparency and auditing The FoundationaLLM source code is fully available in a public GitHub repo: https://github.com/foundationallm/foundationallm. We took this critical decision precisely for cases like this, where our customers need to understand (and also fully audit if needed) the security implications of the permissions granted to various FoundationaLLM components during deployment. For the specific case of Graph API interactions, the behavior is fully encapsulated in one service implementation It can be immediately verified that: Our code performs only the operations mentioned above. Our code uses the smallest possible subset of properties to fulfill its required functionality."
  },
  "docs/archive/operations/index.html": {
    "href": "docs/archive/operations/index.html",
    "title": "Standard platform administration documentation | FoundationaLLM",
    "summary": "Standard platform administration documentation Environment Setup: Initial environment setup (Quick Start Deployment) Initial environment setup (Standard Deployment) Configuration settings for different components: App Configuration Settings Configuration for deployment Configure Core API Settings Configure Management API Settings User Management: Creating, modifying, and deleting user accounts: Authentication setup Assigning roles and permissions: Access control for azure services FoundationaLLM Role-Based Access Control System Maintenance: FoundationaLLM Backups & Data Resiliency Purge user conversations Security Measures: Platform Security Features & Best Practices Logging and Auditing: Accessing System Logs & Audit Trails Troubleshooting: Troubleshooting & Issue Reporting Guide Platform Features: Use cases and best practices for utilizing specific features Update Process: Release Notes: Guidance for creating release notes Update Procedure: Updating container versions in the Standard Deployment Vulnerabilities: Vulnerabilities: Identification, Communication, and Remediation"
  },
  "docs/archive/operations/logs.html": {
    "href": "docs/archive/operations/logs.html",
    "title": "Accessing system logs & audit trails | FoundationaLLM",
    "summary": "Accessing system logs & audit trails Maintaining visibility into system activities is paramount for ensuring security, troubleshooting issues, and monitoring overall system health. Our system generates various logs that provide valuable insights into events, errors, and user activities, and these logs are centrally stored in an Azure Log Analytics Workspace. Accessing these logs is crucial for effective system management. Below, we outline the procedures for accessing logs within our system. 1. Log location Our system logs are stored in the Azure Log Analytics Workspace. This centralized location within Azure ensures ease of access and management. After deployment, customers can redirect these logs to an existing Log Analytics Workspace within their Azure environment, if desired. 2. Log types Different types of logs are generated, including: Security Logs: Capturing security-related events and potential threats. System Logs: Detailing system-level activities and performance metrics. Application Logs: Recording application-specific events and errors. 3. Access permissions Access to logs is restricted to authorized personnel with the appropriate permissions using Azure RBAC. Ensure that only individuals with a legitimate need for log access have the required Azure Roles. 4. Access methods Azure Portal: Navigate to the Azure Portal and access the Log Analytics Workspace for log retrieval. Azure Command-Line Interface (CLI): Use Azure CLI commands for programmatic access to logs. 5. Log retention For long term retention, export logs to a storage account or archive them to a data lake. This is not configured by default but can be added by customers after the initial deployment. The default retention period for logs is 30 days. This can be changed by customers after the initial deployment. 6. Monitoring Tools Utilize Azure Monitor and Log Analytics tools to receive real-time alerts for critical events. Integration with Azure monitoring solutions enhances proactive incident response and system stability. Additional tools like Azure Sentinel can be used for advanced security monitoring and threat detection but are not configured by default. 8. Audit Trails All diagnostics are enabled by default in the standard deployment, including audit trails for Key Vault and similar resources. Audit trails are stored in the Azure Log Analytics Workspace. Regularly review audit trails to ensure the integrity and security of log data."
  },
  "docs/archive/operations/network-security-groups.html": {
    "href": "docs/archive/operations/network-security-groups.html",
    "title": "Network Security Group Configurations | FoundationaLLM",
    "summary": "Network Security Group Configurations FoundationaLLM uses Azure Virtual Networks for network segmentation. The Standard Deployment uses Network Security Groups (NSGs) to control inbound and outbound traffic. The following table lists the NSGs used in the Standard Deployment and the ports that are open by default. NSG Rules Application Gateway Rule Name Access DestinationAddressPrefix DestinationPortRange Direction Priority Protocol ProvisioningState SourceAddressPrefix SourcePortRange Notes allow-internet-http-inbound Allow VirtualNetwork 80 Inbound 128 Tcp Succeeded Internet * Customers may restrict inbound connectivity as desired. allow-internet-https-inbound Allow VirtualNetwork 443 Inbound 132 Tcp Succeeded Internet * Customers may restrict inbound connectivity as desired. allow-gatewaymanager-inbound Allow * 65200-65535 Inbound 148 Tcp Succeeded GatewayManager * This rule is required by Azure and cannot be changed.1 allow-loadbalancer-inbound Allow * * Inbound 164 * Succeeded AzureLoadBalancer * This rule is required by Azure and cannot be changed.1 deny-all-inbound Deny * * Inbound 4096 * Succeeded * * Customers may modify this rule if needed (not reccomended) 1: For further information regarding required NSG rules for Application Gateway, please see this article."
  },
  "docs/archive/operations/purge-conversations.html": {
    "href": "docs/archive/operations/purge-conversations.html",
    "title": "Cosmos DB stored procedure to anonymize deleted sessions (conversations) | FoundationaLLM",
    "summary": "Cosmos DB stored procedure to anonymize deleted sessions (conversations) User conversations are stored in Cosmos DB. When a user deletes a conversation, the session document and associated documents (CompletionPrompt and Message) are marked as deleted (soft-deleted). As opposed to hard-deleting the documents, which would remove them from the database, soft-deleting allows for the possibility of restoring the conversation later. However, in some cases, it may be necessary to anonymize the deleted session to ensure that no sensitive information is retained in the database. Anonymizing the deleted session involves updating the associated documents to remove any potentially sensitive information, such as user input or analysis results. As opposed to hard-deleting the documents, which would remove them from the database, soft-deleting allows for the possibility of retaining statistical information and other metrics, such as token usage, user feedback, etc. This stored procedure anonymizes the deleted session by updating the associated documents to remove any potentially sensitive information. Creating the stored procedure To create the stored procedure, follow these steps: Open the Azure portal and navigate to your Cosmos DB account. Select the \"Data Explorer\" option from the left-hand menu. Right-click the Sessions container and select the New Stored Procedure option. Copy and paste the following code into the stored procedure editor: function anonymizeDeletedSession() { var collection = getContext().getCollection(); var response = getContext().getResponse(); // Retrieve Session document var sessionQuery = `SELECT * FROM c WHERE c.type = 'Session'`; var isSessionAccepted = collection.queryDocuments( collection.getSelfLink(), sessionQuery, {}, function (err, sessionDocuments, responseOptions) { if (err) throw err; if (sessionDocuments.length === 0 || !sessionDocuments[0].deleted) { response.setBody(\"Session cannot be anonymized because it was not deleted.\"); return; } // Session marked as deleted, proceed var query = `SELECT * FROM c WHERE c.type = 'CompletionPrompt' OR c.type = 'Message'`; var isAccepted = collection.queryDocuments( collection.getSelfLink(), query, {}, function (err, documents, responseOptions) { if (err) throw err; if (documents.length === 0) { response.setBody(\"No related documents found.\"); return; } var updatedCount = 0; var processedCount = 0; documents.forEach(function (doc) { if (doc.type === \"CompletionPrompt\") { doc.prompt = \"Deleted\"; } if (doc.type === \"Message\") { doc.text = \"Deleted\"; if (Array.isArray(doc.content)) { doc.content.forEach(function (contentItem) { contentItem.value = \"Deleted\"; }); } if (Array.isArray(doc.analysisResults)) { doc.analysisResults.forEach(function (analysisItem) { analysisItem.toolInput = \"Deleted\"; analysisItem.toolOutput = \"Deleted\"; }); } } // Update the document var acceptUpdate = collection.replaceDocument(doc._self, doc, function (err) { if (err) throw err; updatedCount++; processedCount++; // Check if all documents processed if (processedCount === documents.length) { response.setBody(\"Updated \" + updatedCount + \" documents.\"); } }); if (!acceptUpdate) throw new Error(\"Update not accepted, aborting\"); }); } ); if (!isAccepted) throw new Error(\"Query was not accepted by server.\"); } ); if (!isSessionAccepted) throw new Error(\"Session query was not accepted by server.\"); } Provide a name for the stored procedure (e.g., anonymizeDeletedSession). Select the Save button to create the stored procedure. Executing the stored procedure When you wish to anonymize a deleted session, you can execute the stored procedure by following these steps: In the Azure portal, navigate to your Cosmos DB account and select the Data Explorer option. Expand the Sessions container and expand the Stored Procedures option. Select the stored procedure you created (e.g., anonymizeDeletedSession). Click the Execute button to run the stored procedure. In the input parameters panel that appears, enter the sessionId of the session you want to anonymize into the Partition key value field, then select Execute. The stored procedure will run and anonymize the deleted session, updating the associated documents as necessary. The output will indicate the number of documents that were updated. If the session was not deleted, the output will indicate that the session cannot be anonymized because it was not deleted."
  },
  "docs/archive/operations/release-notes.html": {
    "href": "docs/archive/operations/release-notes.html",
    "title": "Guidance for creating release notes | FoundationaLLM",
    "summary": "Guidance for creating release notes Creating release notes is an essential part of the software development and release process. Release notes provide valuable information to end-users, stakeholders, and other team members about the changes, enhancements, and fixes introduced in a software release. Here's a structured process for creating release notes: 1. Define the release scope and content Understand the changes included in the release. Identify new features, enhancements, bug fixes, and any other significant changes. 2. Use a version control system Review the commit history and pull requests associated with the release. Identify the issues or features implemented during the release period. 3. Categorize changes Group changes into categories for clarity. Common categories include: New Features Enhancements Bug Fixes Deprecations Security Updates 4. Collect information Gather details for each change, including: Brief description of the change. Associated issue or feature request numbers. Names of contributors or teams involved. 5. Prioritize information Arrange the changes in order of importance or relevance to users. Highlight major features or critical bug fixes at the top. 6. Write Release Notes Craft concise and clear release notes. Use a consistent format and style. Include a header with the release version, date, and any important announcements. 7. Provide context Offer context for each change. Explain why a feature was added or why a bug fix was necessary. Include information that helps users understand the impact of the changes. 8. Include links Add links to relevant documentation, issue trackers, or pull requests for more details. This helps users and developers find additional information if needed. 9. Test Release Notes Review the release notes to ensure accuracy and completeness. Verify that all changes are appropriately documented. 10. Review with stakeholders Share the draft release notes with relevant stakeholders, such as product managers, developers, and quality assurance teams. Incorporate feedback and make necessary adjustments. 11. Publish release notes We will publish release notes as part of creating a new release in github. If applicable, notify users through email, social media, or other communication channels. 12. Archive previous release notes Maintain a historical record by archiving previous release notes. Relase notes for previous releases will remain visible with those releases in github. This helps users track changes over time and understand the evolution of the software. 13. Update documentation Ensure that other documentation, such as user guides or API documentation, is updated to reflect the changes introduced in the release. 14. Automate if possible Consider automating parts of the release notes generation process, especially if your project follows a structured development workflow with clear commit messages and pull request descriptions. 15. Seek user feedback Encourage users to provide feedback on the release notes. This can help you improve the clarity and usefulness of future release notes. By following this process, you can create comprehensive and user-friendly release notes that effectively communicate the changes made in each software release."
  },
  "docs/archive/operations/security.html": {
    "href": "docs/archive/operations/security.html",
    "title": "Platform security features & best practices | FoundationaLLM",
    "summary": "Platform security features & best practices Maintaining the security of the Azure platform is crucial for protecting sensitive data and ensuring the integrity of your infrastructure. Identity and Access Management (IAM) Ensure that only authorized users have access to your Azure resources. FoundationaLLM uses Azure Entra ID for centralized identity management. Enable multi-factor authentication (MFA) for all user accounts. Network Security The standard deployment: Uses Azure Virtual Networks for network segmentation. Implements Network Security Groups (NSGs) to control inbound and outbound traffic. Network Security Rule Details Data Encryption Wherever possible the Standard Deployment uses encryption at rest with system-managed keys. Customer managed keys can be enabled at your discretion. Threat Detection and Monitoring Wherever supported the Standard Deployment enables Azure Diagnostics on the resources it deploys. These logs are sent to a Log Analytics workspace. as part of the standard resources. Customers are encouraged to enable Azure Defender for additional threat detection and monitoring. Customers may use Azure Sentinel for advanced security information and event management (SIEM). Customers already using Azure Sentinel may redirect the logs from the Standard Deployment to their existing Azure Sentinel instance. Patch Management Regularly check the FoundationaLLM github repository for new image releases and update your deployment accordingly."
  },
  "docs/archive/operations/troubleshooting.html": {
    "href": "docs/archive/operations/troubleshooting.html",
    "title": "Troubleshooting & issue reporting guide | FoundationaLLM",
    "summary": "Troubleshooting & issue reporting guide There are three common issues that may arise when using the FoundationaLLM platform. This guide provides a structured approach to troubleshooting these issues. Troubleshooting Azure App Registration misconfiguration 1. Symptoms: Users unable to authenticate or access Azure services using the app. Error messages related to authentication failures. 2. Troubleshooting Steps: a. Verify App Registration Configuration: Check the Azure Portal for the App Registration settings. Ensure the correct redirect URIs, client secrets, and authentication settings are configured by reviewing (the setup guide)[../deployment/authentication/core-authentication-setup-entra.md#update-app-configuration-settings]. b. Error Logs Examination: Review logs for any authentication-related errors. Check for error details and correlate them with misconfigured settings. Troubleshooting missing Azure App Registration details in Azure App Configuration 1. Symptoms: Application unable to retrieve configuration settings. Errors related to missing or invalid configuration values. Login problems similar to those described in the previous section. 2. Troubleshooting Steps: a. Check Azure App Configuration: Verify that the App Registration details are correctly stored in Azure App Configuration using the setup guide. Ensure that keys, secrets, and connection strings are accurate. c. Azure App Configuration Logs: Inspect Azure App Configuration logs for any errors related to configuration retrieval. Look for issues such as key not found or invalid values. d. Azure Key Vault Integration: FoundationaLLM Azure Key Vault for sensitive configuration, verify the correct values are in Key Vault using [the setup guide][1]. Ensure the Azure App Configuration managed identity has the necessary permissions to access Key Vault secrets. Troubleshooting container crashing 1. Symptoms: Containers restarting frequently or failing to start. Application unavailability due to container issues. 2. Troubleshooting Steps: a. Container Logs Examination: Access container logs in Log Analytics to identify error messages or issues during startup. Look for any crashes, exceptions, or resource constraints. b. Resource Utilization: Check resource utilization metrics (CPU, memory) for the container. Ensure that the container has adequate resources allocated. c. Dependency Check: Examine dependencies within the containerized application. Verify that required services or components are accessible. d. Container Health Checks: Identify and address health check failures impacting container stability. e. Container Image Update: Review the container image version, update to the latest version to receive bug fixes and new features. Additional support and issue reporting If you encounter an issue that is not addressed by the troubleshooting steps outlined in this document, we encourage you to open a GitHub issue. This ensures that our team can provide tailored assistance and continuously improve our troubleshooting resources. Steps to open a GitHub issue Navigate to our GitHub Repository: Visit our GitHub repository at https://github.com/foundationallm/foundationallm. Check Existing Issues: Before creating a new issue, check the existing issues to see if the problem has already been reported or discussed. Create a New Issue: Click on the \"Issues\" tab in the repository. Select \"New Issue\" to open a new issue template. Provide Detailed Information: Clearly describe the issue, including symptoms, error messages, and steps to reproduce. Attach relevant logs or screenshots that can assist in understanding the problem. Tag the Issue Appropriately: Tag the issue with relevant labels, such as \"bug,\" \"enhancement,\" or \"question,\" to categorize it correctly. Monitor for Updates: After creating the issue, monitor it for updates and respond promptly to any requests for additional information. By opening a GitHub issue at https://github.com/foundationallm/foundationallm, you contribute to our collaborative effort in maintaining a robust and well-supported system. Our team values your feedback, and addressing issues through GitHub allows for a transparent and efficient resolution process. Thank you for your collaboration, and we look forward to assisting you with any challenges you may encounter. Your input is instrumental in enhancing the overall reliability and functionality of our system."
  },
  "docs/archive/operations/update.html": {
    "href": "docs/archive/operations/update.html",
    "title": "Updating container versions in the Standard Deployment | FoundationaLLM",
    "summary": "Updating container versions in the Standard Deployment The Standard Deployment uses the foundationallm/deploy/scripts/Deploy-Images-Aks.ps1 script to deploy latest version of the FoundationaLLM images during initial setup. However, you may want to update the images to a specific version. This can be done by updating the values file and redeploying the images. PREREQUISITES Azure CLI set to the correct tenant/subscription context AKS credentials located in the .kube/config directory To obtain AKS credentials, use the following Azure CLI command: az aks get-credentials --name \"[PREFIX]-aks\" --resource-group \"[DEPLOYMENT RESOURCE GROUP]\" Helm CLI DESCRIPTION This script deploys multiple images on an AKS (Azure Kubernetes Service) cluster using Helm charts. It takes various parameters such as the release name, AKS name, resource group, tag, charts to deploy, values file, namespace, TLS/SSL environment, TLS host, TLS secret name, and autoscale option. PARAMETER name The release name for the deployment. Default is \"foundationallm\". PARAMETER aksName The name of the AKS cluster. PARAMETER resourceGroup The resource group of the AKS cluster. PARAMETER tag The tag of the images to deploy. Default is \"latest\". PARAMETER charts The charts to deploy. Use \"\" to deploy all charts. Default is \"\". PARAMETER valuesFile The path to the values file for Helm charts. Default is \"gvalues.yaml\". PARAMETER namespace The namespace to deploy the charts. If empty, it uses the namespace specified in .kube/config. PARAMETER tlsEnv The TLS/SSL environment to enable. Valid values are \"prod\", \"staging\", \"none\", and \"custom\". Default is \"prod\". PARAMETER tlsHost The hostname of the AKS cluster. Required if tlsEnv is set to \"custom\". PARAMETER tlsSecretName The name of the TLS secret. Required if tlsEnv is set to \"custom\". PARAMETER autoscale Specifies whether to enable autoscaling for the core-job chart. Default is $false. EXAMPLE Deploy-Images-Aks.ps1 ` -aksName \"myAKS\" ` -resourceGroup \"myResourceGroup\" ` -tag \"v1.0\" ` -charts \"core-api,core-job\" ` -valuesFile \"myvalues.yaml\" ` -namespace \"myNamespace\" ` -tlsEnv \"prod\" ` -tlsHost \"myaks.example.com\" ` -tlsSecretName \"myTLSSecret\" ` -autoscale $true Note: The script will NOT generate a gvalues.yaml file for you. If you do not have a gvalues.yaml file locally, use the Helm CLI to obtain it. # Get all releases (e.g., foundationallm-web) helm list # Write gvalues.yaml for release helm get values \"foundationallm-web\" --all > gvalues.yaml"
  },
  "docs/archive/operations/vulnerabilities.html": {
    "href": "docs/archive/operations/vulnerabilities.html",
    "title": "Vulnerabilities: Identification, communication, and remediation | FoundationaLLM",
    "summary": "Vulnerabilities: Identification, communication, and remediation FoundationaLLM is committed to maintaining the security of our platform and protecting the integrity of your data. We conduct regular security testing to identify and address potential vulnerabilities in our platform. Vulnerability Identification: Regular Red-Team Exercises: Our security protocols include routine red-team exercises aimed at identifying potential vulnerabilities and misconfigurations within the Azure platform. Through these exercises, we simulate real-world attack scenarios to proactively identify and address any weaknesses in our security posture. Regular Builds and Container Image Scans: To stay ahead of emerging threats, we conduct regular builds and scans of our container images. This proactive approach involves identifying and addressing newly reported Common Vulnerabilities and Exposures (CVEs) promptly. By integrating security scans into our regular build processes, we ensure that our container images adhere to the latest security standards and mitigate potential risks effectively. Vulnerability Severity Categorization: Severity levels for vulnerabilities and other security findings are defined as follows: Minor: Vulnerabilities categorized as minor pose low or negligible risk to our system's security. These issues typically have minimal impact on operations and can be addressed during routine maintenance. Major: Major vulnerabilities signify a moderate level of risk and may have a noticeable impact on security if left unaddressed. Immediate attention is given to major vulnerabilities to mitigate potential security gaps and maintain a secure environment. High: Vulnerabilities classified as high represent a significant risk to the security and stability of our Azure platform. Urgent action is taken to address high-severity issues, often involving immediate patches or remediation steps to minimize potential threats. Critical: Critical vulnerabilities pose a severe and imminent threat to the integrity and confidentiality of our system. Immediate and comprehensive measures are implemented to address critical vulnerabilities, including rapid deployment of patches, configuration changes, or other necessary security controls. This severity categorization allows us to prioritize our response efforts based on the potential impact and urgency associated with each vulnerability. Regular assessments and adjustments are made to ensure the accuracy and relevance of the severity levels assigned to vulnerabilities. Communication of Patched Versions: We are committed to maintaining transparent and effective communication regarding security updates and patched versions. Our primary channel for disseminating information about patched versions is our official GitHub release page. This ensures that our users and stakeholders have immediate access to crucial details about the updates, including security enhancements and fixes. Key points related to the communication of patched versions on our GitHub release page: Release Notifications: Timely notifications about new releases, including security patches, will be posted on our GitHub release page. Users are encouraged to subscribe to release notifications to stay informed about the latest updates and security improvements. Detailed Release Notes: Each release on our GitHub page will include comprehensive release notes outlining the changes, enhancements, and specific security vulnerabilities addressed. This transparent approach provides our community with detailed insights into the updates and the importance of applying the latest patches. Vulnerability Disclosure: We adhere to responsible disclosure practices by openly acknowledging and crediting the individuals or organizations that report security vulnerabilities to us. Detailed information about the vulnerabilities, their potential impact, and the corresponding patches will be shared on our GitHub release page. Encouraging Regular Updates: Users are strongly encouraged to regularly check our GitHub release page for the latest updates and security patches. Proactive adoption of the latest releases ensures that users benefit from the most recent security enhancements and protection against potential threats. By leveraging our GitHub release page as a central hub for communication, we aim to streamline the process of disseminating critical information and empowering our user community to maintain a secure environment."
  },
  "docs/archive/role-based-access-control/agent-role-assignments.html": {
    "href": "docs/archive/role-based-access-control/agent-role-assignments.html",
    "title": "Automate the management of role assignments for agents | FoundationaLLM",
    "summary": "Automate the management of role assignments for agents Calling the Management REST API All calls to the FoundationaLLM Management REST API must use HTTPS and include the Authorization header with a valid Bearer token for the Microsoft Entra ID identity used to perform the call. The token must have the Data.Manage scope. The identity must have one of the following roles assigned at the scope of the FoundationaLLM agent: Owner User Access Administrator The URL of the Management REST API can be obtained from the Deployment information section of the FoundationaLLM Management Portal: Here is an example of how to get an access token for the FoundationaLLM Management REST API: az login az account get-access-token --scope api://FoundationaLLM-Management/Data.Manage Listing available agents To list all agents in your FoundationaLLM instance, you can use the following REST API call: HTTP GET {managementAPIUrl}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents where: {managementAPIUrl} is the URL of the FoundationaLLM Management REST API. {instanceId} is the ID of your FoundationaLLM instance. The response will be a list of objects representing agents. Each object has a property named resource that contains the agent's details as follows: [ { \"resource\": { \"type\": \"knowledge-management\", \"name\": \"FoundationaLLM\", \"object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.Agent/agents/FoundationaLLM\", \"display_name\": null, \"description\": \"Useful for answering questions about FoundationaLLM.\", \"cost_center\": \"\", \"vectorization\": { \"dedicated_pipeline\": true, \"data_source_object_id\": \"\", \"indexing_profile_object_ids\": [ \"\" ], \"text_embedding_profile_object_id\": \"\", \"text_partitioning_profile_object_id\": \"\", \"vectorization_data_pipeline_object_id\": \"\", \"trigger_type\": \"Event\", \"trigger_cron_schedule\": \"* * * * *\" }, \"inline_context\": true, \"sessions_enabled\": true, \"conversation_history_settings\": { \"enabled\": false, \"max_history\": 5 }, \"gatekeeper_settings\": { \"use_system_setting\": false, \"options\": [] }, \"orchestration_settings\": { \"orchestrator\": \"LangChain\", \"agent_parameters\": null }, \"prompt_object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.Prompt/prompts/FoundationaLLM\", \"ai_model_object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.AIModel/aiModels/GPT4oMiniCompletionAIModel\", \"capabilities\": [ \"OpenAI.Assistants\" ], \"tools\": { \"dalle-image-generation\": { \"name\": \"dalle-image-generation\", \"description\": \"Generates an image based on a prompt.\", \"ai_model_object_ids\": { \"main_model\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.AIModel/aiModels/DALLEImageGenerationModel\" }, \"api_endpoint_configuration_object_ids\": {}, \"properties\": {} } }, \"properties\": { \"welcome_message\": \"<p>You are chatting with an agent named FoundationaLLM, who can answer questions about the FoundationaLLM platform.</p><p>Additional capabilities include:</p><ul><li>Upload files and ask the agent to analyze them</li><li>Generate charts, files, and downloadable content</li><li>Generate images using DALL-E</li></ul><p><em>Please avoid sharing personally identifiable information (PII) while conversing with the agent.</em></p>\", \"Azure.OpenAI.Assistant.Id\": \"asst_wNLRX3klgprrg6ZFSbigfSJg\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"2024-11-13T16:50:28.6257554+00:00\", \"created_by\": null, \"updated_by\": \"ciprian@foundationaLLM.ai\", \"deleted\": false, \"expiration_date\": null } } ] Make a note of the object_id property for the agent you want to assign a role to. You will need this value when requesting to add or remove role assignments for the agent. Adding or removing role assignments for an agent To request adding or removing role assignments for an agent, you can use the following REST API call: HTTP POST {managementAPIUrl}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/{agentName}/externalRoleAssignments where: {managementAPIUrl} is the URL of the FoundationaLLM Management REST API. {instanceId} is the ID of your FoundationaLLM instance. {agentName} is the name of the agent you want to assign a role to. For the example above, the agentName would be FoundationaLLM and the request should be: HTTP POST {managementAPIUrl}/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.Agent/agents/FoundationaLLM/externalRoleAssignments The body of the request should contain the following JSON object: { \"roleAssignmentsToAdd\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [ \"user1@foundationallm.ai\", \"user2@foundationallm.ai\" ], \"expirationDate\": \"2024-12-31T23:59:59Z\" } ], \"roleAssignmentsToRemove\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [] } } where: The role definition identifier 00a53e72-f66e-4c03-8f81-7e885fd2eb35 is the well-known identifier for the Reader role. The identities array contains the list of user principal names (UPNs) that you want to assign or remove to or from the role. expirationDate is an optional property that specifies the date and time when the role assignment will expire."
  },
  "docs/archive/role-based-access-control/index.html": {
    "href": "docs/archive/role-based-access-control/index.html",
    "title": "FoundationaLLM RBAC documentation | FoundationaLLM",
    "summary": "FoundationaLLM RBAC documentation FoundationaLLM role-based access control (FoundationaLLM RBAC) is a system that provides fine-grained access control to FoundationaLLM resources. It is a system that allows you to control who can access what in your FoundationaLLM instance. Concepts Understand FoundationaLLM role definitions Understand FoundationaLLM role assignments Understand scope for FoundationaLLM RBAC Manage role assignments Automate the management of role assignments for agents"
  },
  "docs/archive/role-based-access-control/role-assignments.html": {
    "href": "docs/archive/role-based-access-control/role-assignments.html",
    "title": "Understand FoundationaLLM role assignments | FoundationaLLM",
    "summary": "Understand FoundationaLLM role assignments Role assignments enable you to grant a principal (such as a user, a group, a managed identity, or a service principal) access to a specific FoundationaLLM resource. Role assignment Acess to FoundationaLLM resources is granted through role assignments, and is reoked by removing a role assignment. A role assignment has several components: The principal, or who is being given access. The role definition (role), or what access is being granted. The scope at which the role is assigned, or where the access applies. The name of the role assignment. The description of the role assignment that helps explain why it exists. The following is an example of a FoundationaLLM role assignment: { \"RoleAssignmentName\": \"00000000-0000-0000-0000-000000000000\", \"RoleAssignmentId\": \"/instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.Authorization/roleAssignments/00000000-0000-0000-0000-000000000000\", \"Scope\": \"/instances/11111111-1111-1111-1111-111111111111\", \"RoleDefinitionName\": \"Contributor\", \"RoleDefinitionId\": \"e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"ObjectId\": \"22222222-2222-2222-2222-222222222222\", \"ObjectType\": \"User\", \"DisplayName\": \"Jack The Cat\", \"SignInName\": \"jackthecat@foundationallm.ai\", \"Description\": \"Jack The Cat has contributor access to the FoundationaLLM instance.\" } The following table describes the properties of a role assignment. Property Description RoleAssignmentName The name of the role assignment (is always a GUID). RoleAssignmentId The unique identifier of the role assignment which includes the name. Scope The FoundationaLLM resource identifier that the role assignment applies to. RoleDefinitionName The name of the role definition. RoleDefinitionId The unique identifier of the role definition. DisplayName The display name of the principal. ObjectId The unique identifier of the principal (can be a user, a group, a managed identity, or a service principal). ObjectType The type of the principal. Valid values include User, Group, and ServicePrincipal. DisplayName The display name of the principal. SignInName The unique principal name (UPN) of the principal. Description The description of the role assignment."
  },
  "docs/archive/role-based-access-control/role-definitions.html": {
    "href": "docs/archive/role-based-access-control/role-definitions.html",
    "title": "Understand FoundationaLLM role definitions | FoundationaLLM",
    "summary": "Understand FoundationaLLM role definitions Role definition A role defininition (or just role) is a collection of permissions. A role definition lists the actions that can be performed, such as read, write, and delete. The following table describes the propoerties of a role definition. Property Description Name The display name of the role definition. Id The unique identifier of the role definition. Description The description of the role definition. Actions An array of strings that lists the control plane actions that a role definition can perform. For example, FoundationaLLM.Agent/agents/create. NotActions An array of strings that lists the actions that are excluded from the actions listed in the Actions property. DataActions An array of strings that lists the data plane actions that a role definition can perform. For example, FoundationaLLM.Agent/agents/read. NotDataActions An array of strings that lists the data plane actions that are excluded from the actions listed in the DataActions property. AssignableScopes An array of strings that lists the scopes that the role definition can be assigned to. Actions format The string that represents an action has the following format: FoundationaLLM.{ProviderName}/{resourceType}/{action} Examples of actions include read, write, and delete. The wildcard character (*) can be used to match any resource type or action. For example, FoundationaLLM.Agent/*/read matches all read actions for all resource types in the FoundationaLLM.Agent provider. Role definition example The following example shows the Contributor role definition. The wildcard (*) character under Actions indicates that the principal assigned to the role can perform all actions (i.e., it can manage everything). This includes also actions defined in the future, as FoundationaLLM adds new resource types. The actions under NotActions are subtracted from Actions. In this specific case, NotActions removes the role's ability to manage access to resources. { \"Name\": \"Contributor\", \"Id\": \"e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"Description\": \"Allows you to manage everything except access to resources.\", \"Actions\": [ \"*\" ], \"NotActions\": [ \"FoundationaLLM.Authorization/*/delete\", \"FoundationaLLM.Authorization/*/write\" ], \"DataActions\": [], \"NotDataActions\": [], \"AssignableScopes\": [ \"/\" ] } Control and data actions Control plane actions are specified in the Actions and NotActions properties. Examples of control plane actions in FoundationaLLM include: Manage access to an agent Create a new data source Delete a prompt Data plane actions are specified in the DataActions and NotDataActions properties. NOTE: FoundationaLLM maintains a strict separation between the control and data planes. Control plane access is not inherited to the data plane. For example, if a user has the FoundationaLLM.Agent/agents/create permission, it does not mean that the user has the FoundationaLLM.Agent/agents/read permission."
  },
  "docs/archive/role-based-access-control/role-management.html": {
    "href": "docs/archive/role-based-access-control/role-management.html",
    "title": "Manage role assignments | FoundationaLLM",
    "summary": "Manage role assignments FoundationaLLM roles are assigned to users, groups, service principals, and managed identities through the Management API. As described in the role definitions article, administrators can apply fine-grained access control to features and resources to ensure the deployment adheres to least-privilege best practices when properly assigned. Management API Important The authenticated user must be assigned to the User Access Administrator role (/providers/FoundationaLLM.Authorization/roleDefinitions/fb8e0fd0-f7e2-4957-89d6-19f44f7d6618) on the FoundationaLLM instance to manage role assignments and call the identity endpoints through the Management API. The Management API provides endpoints for managing role assignments and retrieving identities. The API is secured with Microsoft Entra ID and requires a valid access token to access the endpoints. The access token must be obtained by authenticating with Entra ID and acquiring the required permissions to call the Management API endpoints. Role management endpoints The Management API provides the following role management endpoints: Method Endpoint Description GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions Returns a list of all role definitions. POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/filter Returns the role assignments for a given scope. Here is a sample request payload for retrieving all RBAC role assignments for an FLLM instance: {\"scope\":\"/instances/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\"}. To retrieve the role assignments for a specific resource, set the request body to the resource's scope: {\"scope\":\"/instances/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/providers/FoundationaLLM.Agent/agents/FoundationaLLM\"} POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{roleAssignmentName} Creates a new role assignment. Here is an example request payload: {\"name\":\"55555555-4444-3333-2222-111111111111\",\"description\":\"\",\"principal_id\":\"11111111-2222-3333-4444-555555555555\",\"role_definition_id\":\"/providers/FoundationaLLM.Authorization/roleDefinitions/a9f0020f-6e3a-49bf-8d1d-35fd53058edf\",\"type\":\"FoundationaLLM.Authorization/roleAssignments\",\"principal_type\":\"User\",\"scope\":\"/instances/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/providers/FoundationaLLM.Agent/agents/FoundationaLLM\"} DELETE /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{roleAssignmentName} Deletes a role assignment. Identity endpoints The Management API provides the following identity endpoints: Method Endpoint Description POST /instances/{instanceId}/identity/users/retrieve Returns a list of user identities. Filter and page through the identities with the following request payload structure: {\"name\":\"\",\"ids\":[],\"page_number\":1,\"page_size\":null} POST /instances/{instanceId}/identity/groups/retrieve Returns a list of group identities. Filter and page through the identities with the following request payload structure: {\"name\":\"\",\"ids\":[],\"page_number\":1,\"page_size\":null} POST /instances/{instanceId}/identity/objects/retrievebyids Returns user and group objects by the passed in list of IDs. Here is a sample request payload: {\"ids\":[\"11111111-2222-3333-4444-555555555555\",\"66666666-7777-8888-9999-000000000000\"]} Management Portal The Management Portal provides a graphical user interface over the Management API for managing role assignments, among other FLLM configuration settings. Role assignment management Important The authenticated user must be assigned to the User Access Administrator role (/providers/FoundationaLLM.Authorization/roleDefinitions/fb8e0fd0-f7e2-4957-89d6-19f44f7d6618) on the FoundationaLLM instance to manage role assignments and call the identity endpoints through the Management Portal. All role assignment changes are audited and can be viewed in the Management Portal. Auditing ensures that all changes to role assignments are tracked and can be reviewed at a later time, which is often required for compliance purposes. The following screenshot shows the role assignment management page in the Management Portal: You can expand and contract each role assignment name to view the role assignment details. Each role assignment name has an info icon next to it that displays a description within a tooltip. The role assignment details include the role assignment name, type (user or group), and scope (instance or resource). You can sort by each of these columns. The Delete column provides an option to delete the role assignment. Please note that you cannot edit a role assignment. You must delete the role assignment and create a new one to make changes. The following screenshot shows the role assignment management form: Use the Browse button to search for users, groups, service principals, and managed identities. Selecting a user, group, service principal, or managed identity will populate the Principal Type, Principal Name, Principal Email, and Principal ID fields with the selected identity's information. Assign access control to the FoundationaLLM instance There are two levels of access control in FoundationaLLM: instance level and resource level. The instance level is the highest level of access control and applies to all resources within the instance. The resource level is a more granular level of access control and applies to specific resources within the instance. When you assign a role to a user, group, service principal, or managed identity at the instance level, the role is inherited by all resources within the instance. To assign access control to the FoundationaLLM instance, select the Instance Access Control item under the Security group within the left-hand navigation menu of the Management Portal: Assign access control to a specific resource Most resources, such as Agents, provide an option to directly assign role-based access control to the resource. The following screenshot shows an Access Control button on top of a selected agent's form with the access control dialog displayed after selecting the button: Note that the Scope column of the list of role assignments shows whether the role assignment is at the instance level or at the resource level. A value of This resource indicates a direct role assignment on the resource. When the value is Instance (inherited), the role assignment is inherited from the instance level. This is similar to how Azure RBAC works. Role assignment data store The role-based access control (RBAC) engine uses a dedicated data store to manage role assignments. Doing so ensures isolation from the rest of the FoundationaLLM data platform, which is used to store data sources, prompts, and other resources accessible to users possessing varying levels of access. The isolation also supports the ability to scale the RBAC engine independently from the rest of the FoundationaLLM data platform. The data store is implemented as a dedicated Azure Cosmos DB account or Azure Data Lake Storage Gen2 account."
  },
  "docs/archive/role-based-access-control/scope.html": {
    "href": "docs/archive/role-based-access-control/scope.html",
    "title": "Understand scope for FoundationaLLM RBAC | FoundationaLLM",
    "summary": "Understand scope for FoundationaLLM RBAC Scope is the set of resources that a role assignment can access. When assigning roles, it is importan to understand how scope works so that you grant security principals only the level of access they need. Limiting scope also limits the potential damage that can be done if a security principal is compromised. Scope levels In FoundationaLLM, you can specify a scope at the following levels: Instance: The FoundationaLLM deployment itself. Resource: A specific resource in FoundationaLLM, such as an agent. The following rules apply to scope levels: Scopes are structured as a hierarchy. For example, a resource scope is always a child of an instance scope. Each level make the scope more specific. For example, a resource scope is more specific than an instance scope. Roles can be assigned at any of these levels of scope. Lower levels inherit the permissions of higher levels. For example, a role assignment at the instance level applies to all resources in the instance. Scope format Scope is a string that identifies the exact scope of the role assignment. The scope is usually referred to as the resource identifier or resource ID. The scope consists of a series of identifiers separated by the slash (/) character. You can think of this string as expressing the following hierarchy, where text without placeholders ({}) are fixed identifiers: /instances /{instanceId} /providers /{providerName} /{resourceType} /{resourceSubType1} /{resourceSubType2} /{resourceName} {instanceId} is unique identifier of the FoundationaLLM deployment (a GUID). {providerName} is the name of the FoundationaLLM resource provider (for example, FoundationaLLM.Agent). {resourceType} and {resourceSubType*} identify levels within the resource provider. {resourceName} is the name of a specific resource. Scope examples Scope Example Instance /instances/11111111-1111-1111-1111-111111111111 Resource /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.Agent/agents/agent1 Resource /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.DataSource/dataSources/datasource1 Resource /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.Agent/agents/agent1/models/gpt4."
  },
  "docs/archive/setup-guides/agents/Agent_AccessToken.html": {
    "href": "docs/archive/setup-guides/agents/Agent_AccessToken.html",
    "title": "Agent Access Token | FoundationaLLM",
    "summary": "Agent Access Token To allow the flexibility of using an agent without requiring the user to be authenticated using Entra ID credentials, you can create an Agent Access Token. This is particularly useful for public applications that want to provide access to the agent without requiring users to log in with their Entra ID credentials. How to create an Agent Access Token In the Security section of the agent configuration, click on the Create Access Token button to create a new access token. In the Create Access Token dialog, enter a description for the access token and an expiration date then click on the Create Access Token button. The access token will be created and displayed in a dialog, make sure to save it or copy it for future use. Assigning permission to the new Virtual Security Group ID of the agent Copy the GUID of the new Virtual Security Group ID of the agent. At the top of the page while editing the agent, click on the Access Control. In the Access Control page, click on the Add Role Assignment for this resource button. Verify that the Scope is set to providers/FoundationaLLM.Agent/agents/{agent_name}. Choose Group as the Prinicipal Type. Paste the GUID of the new Virtual Security Group ID of the agent in the Principal ID field. Choose Reader as the Role to assign. Accessing the agent using the Agent Access Token To share an example of how to use the Agent Access Token, we will use the API collection from POSTMAN that is publicly available and part of the FoundationaLLM documentation. There are two steps required to use the agent using the Agent Access Token in POSTMAN by accessing the FoundationaLLM.Core.API collection: Under sessions, pick the POST Creates a new Chat session with Agent Access Token Under completions, expand completion and pick POST Requests a completion with Agent Access Token You have to start with the Creates a new Chat session with Agent Access Token request to create a new chat session as the request for completion requires a sessionId. Start by setting the Authorization type to No Auth On the Headers tab, set a new key X-AGENT-ACCESS-TOKEN with the value of the Access Token that you were asked to save or copy previously On the Body tab, enter a name for your chat conversation session in the name key in raw JSON format Click on the Send button to send the request and save the sessionId from the response as it will be needed in the next step. The next step is to POST to the completion endpoint to get a response from the agent utilizing the sessionId received in the previous step. 1- Set the Authorization type to No Auth 2- On the Headers tab, set a new key X-AGENT-ACCESS-TOKEN with the value of the Access Token that you were asked to save or copy previously 3- On the Body tab, enter the sessionId received in the previous POST in the sessionId key in raw JSON format 4- Click on the Send button to send the request and receive a response from the agent explaining why the sky is blue."
  },
  "docs/archive/setup-guides/agents/agents_workflows.html": {
    "href": "docs/archive/setup-guides/agents/agents_workflows.html",
    "title": "Agents and Workflows | FoundationaLLM",
    "summary": "Agents and Workflows FoundationaLLM (FLLM) agents are the core of the solution. They are responsible for providing users with a customized experience based on its configuration. Creation of a new Agent To create a new agent, you can use the Create Agent hyperlink in the Agents section of the Management Portal. The creation of a new agent consists of 5 sections: General Agent Configuration Workflow Tools Security General Section In this section, you can define the name, description and welcome message of the agent. The Welcome message is what a user will see in the Chat portal as soon as they pick that agent from the dropdown to learn about the agent and its services that it provides before starting a chat conversation. Agent Configuration Section In this section, you can define the following configurations: Chat History: This setting allows you to enable or disable the chat history feature for the agent. When enabled, the agent will remember the context of previous conversations, allowing for more personalized and relevant responses. If disabled, the agent will not retain any memory of past interactions. It also allows you to define the number of messages to be stored in the chat history. The default is 5 messages. Gatekeeper: This setting allows you to enable or disable the gatekeeper feature for the agent. When enabled, the agent will have a gatekeeper that can filter and moderate the content of conversations, ensuring that inappropriate or harmful content is not generated. If disabled, the agent will not have any content moderation capabilities. You can choose from multiple options for the content safety: Azure Content Safety Azure Content Safety Prompt Shield Lakera Guard Enkrypt Guardrails The Gatekeeper also allows you to enable the Data Protection aspect of the agent, which currently uses Microsoft Presidio to filter sensitive data in the conversations. Cost Center: This setting allows you to define a cost center for the agent. A cost center is a department or unit within an organization that is responsible for its own expenses and budget. By assigning a cost center to the agent, you can track and manage the costs associated with its operations. Expiration: This setting allows you to define an expiration date for the agent. After this date, the agent will no longer be available for use. This is useful for managing the lifecycle of agents and ensuring that they are only active when needed. Chat Portal Displays: This setting allows you to turn on or off 4 valuable capabilities in the Chat Portal. The amount of tokens used in the conversation. (Questions and Responses) The prompt used by the agent for a specific question including history and context. The option to rate the response of the agent. The ability to allow the user to upload files to the agent in the conversation. Workflow Section In this section, you can define the workflow of the agent. The workflow is a sequence of steps that the agent follows to process user requests and provide responses. You can define the following aspects of the workflow: Workflow Type: This setting allows you to choose the type of workflow for the agent. You can choose from the following options: OpenAIAssistants: Gives your agent the ability to take advantage of Code Interpreter, File Search and Function Calling. LangGraphReactAgent: Gives your agent the ability to dynamically choose a tool from a predefined toolset in LangGraph ExternalAgentWorkflow: Gives your agent the ability to use external workflows developed in Python and registered to be used by your Agent. Workflow name: This setting allows you to define the name of the workflow for the agent. The name should be descriptive and reflect the purpose of the workflow. Workflow Package Name: This setting allows you to define the name of the workflow package for the agent. The package name should be descriptive and reflect the purpose of the workflow. Workflow Host: This setting allows you to define the host of the workflow for the agent. Currently the host is required to be Langchain for all OpenAIAssistants workflows. Workflow Main Model: This setting allows you to define the main model of the workflow for the agent. The main model is the primary large language model (LLM) that the agent uses to generate responses. You can choose from any of the models deployed as part of your instance. Workflow Main Model Parameters: This setting allows you to define the parameters of the main model for the agent. The parameters are the settings that control the behavior of the model, such as temperature, max tokens, and top_p. Workflow Main Prompt: This setting allows you to define the main prompt of the workflow for the agent. The main prompt is the definition of the persona of the agent and the instructions that it follows to generate responses. Tools Section In this section, you can define the tools that the agent can use to perform tasks and provide responses. The tools are external services or APIs that the agent can call to retrieve information or perform actions. Currently, the following tools are available out of the box: DALLE3 Image Generator Important The name of the tool HAS to be DALLEImageGeneration in order for the agent to be able to use it. The AI Model's Object role has to be main_model in the Tool Resources section. Security Section In this section, you can define an Agent Access Token to be used by the agent. The Agent Access Token is a security token that is used to authenticate and authorize access to the agent's resources and services. It is a unique identifier that is generated for each agent and is used to ensure that only authorized users can access the agent's capabilities without requiring the user to be authenticated using Entra ID credentials. This is particularly useful for public applications that want to provide access to the agent without requiring users to log in with their Entra ID credentials. Access Token scenario"
  },
  "docs/archive/setup-guides/agents/index.html": {
    "href": "docs/archive/setup-guides/agents/index.html",
    "title": "Agents | FoundationaLLM",
    "summary": "Agents FoundationaLLM (FLLM) agents are the core of the solution. They are responsible for providing users with a customized experience based on its configuration. Agents and Workflows Prompts Prompts are an important aspect for agents. A prompt defines the persona, instructions and guardrails provided to the large language model (LLM) so that it formulates accurate responses in desired formats. Prompt resources Access Tokens Access tokens are used to authenticate and authorize access to agents without the need for Entra ID credentials. This is useful for scenarios where you want to provide access to an agent without requiring users to log in with their Entra ID credentials. Agent Access Token"
  },
  "docs/archive/setup-guides/agents/knowledge-management-agent.html": {
    "href": "docs/archive/setup-guides/agents/knowledge-management-agent.html",
    "title": "Knowledge management agent | FoundationaLLM",
    "summary": "Knowledge management agent The FoundationaLLM (FLLM) Knowledge Management agent type supports the following scenarios: With an Inline Context: Knowledge Management agents with an Inline Context pass the user's prompt directly to the Large Language Model (LLM). Without an Inline Context: Knowledge Management agents without an Inline Context implement the Retrieval Augmented Generation (RAG) design pattern. RAG augments the user prompt with additional context to generate a more accurate response. The RAG flow uses a retrieval model to retrieve relevant documents from a knowledge base, such as a vector store, and then uses the retrieved documents to augment the user prompt before sending it to the LLM. The creation of a Knowledge Management agent without an Inline Context requires an existing knowledge base, such as a vector store. Use the Vectorization API to create a vector store prior to the creation of the agent. Knowledge Management Agent Configuration The Knowledge Management agent configuration may reference the following resources: Vectorization text embedding profile: The text embedding profile contains the configuration of the text embedding model used to embed the user prompt and perform a vector search in the knowledge base. This must match the text embedding profile used to populate the knowledge base. Vectorization indexing profile: The indexing profile contains the configuration of the service hosting the index. Prompt: The system prompt of the agent, describing the persona of the agent. Note: The Knowledge Management agent implementation currently supports the AzureAISearchIndexer indexing profile. The structure of a Knowledge Management agent is the following: { \"type\": \"knowledge-management\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Agent/agents/<name>\", \"description\": \"<description>\", \"display_name\": \"<display_name>\", \"inline_context\": true, \"vectorization\": { \"dedicated_pipeline\": \"\", \"data_source_object_id\": \"<data_source_object_id>\", \"indexing_profile_object_id\": \"<indexing_profile_object_id>\", \"text_embedding_profile_object_id\": \"<text_embedding_profile_object_id>\", \"text_partitioning_profile_object_id\": \"<text_partitioning_profile_object_id>\", \"vectorization_data_pipeline_object_id\": \"\", \"trigger_type\": \"\", \"trigger_cron_schedule\": \"\" }, \"prompt_object_id\": \"<prompt_resource_objectid>\", \"language_model\": { \"type\": \"openai\", \"provider\": \"microsoft\", \"temperature\": 0.0, \"use_chat\": true, \"api_endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"version\": \"FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion\", \"deployment\": \"FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName\" }, \"sessions_enabled\": true, \"conversation_history\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper\": { \"use_system_setting\": false, \"options\": [ \"ContentSafety\", \"Presidio\" ] }, \"orchestration_settings\": { \"orchestrator\": \"LangChain\", \"endpoint_configuration\": { \"endpoint\": \"\", \"api_version\": \"\", \"api_key\": \"\", \"auth_type\": \"\", \"api_key\": \"\", \"provider\": \"\", \"operation_type\": \"chat\" }, \"model_parameters\": { \"deployment_name\": \"\" } } } where: <name> is the name of the agent. <instance_id> is the instance ID of the deployment. <description> is the description of the agent. Ensure that this description details the purpose of the agent. <display_name> controls the title of the agent in the Chat UI dropdown menu. <data_source_object_id> is the object ID of the Data Source resource. <indexing_profile_object_id> is the object ID of the indexing profile resource. <text_embedding_profile_object_id> is the object ID of the text embedding profile resource. <text_partitioning_profile_object_id> is the object ID of the text partitioning profile resource. <prompt_resource_objectid> is the object ID of the prompt resource. Parameter Description type The type of the agent - will always be knowledge-management. type must be the first key in the request body. name The name of the agent. object_id The object ID of the agent. Remove this element when creating an agent as this is generated by the Management API. description The description of the agent, ensure this description details the purpose of the agent. display_name The title of the agent in the Chat UI dropdown menu. This field is optional. inline_context Whether or not the agent has an Inline Context. vectorization The vectorization object is only required for Knowledge Management agents without an Inline Context (inline_context is false). If the vectorization object is included, the indexing_profile_object_id and text_embedding_profile_object_id keys are required. vectorization.dedicated_pipeline A boolean indicating whether or not the agent has a dedicated Vectorization pipeline (implemented in an upcoming release). vectorization.data_source_object_id The object ID of the Data Source resource. vectorization.indexing_profile_object_id The object ID of the indexing profile resource. vectorization.text_embedding_profile_object_id The object ID of the text embedding profile resource. vectorization.text_partitioning_profile_object_id The object ID of the text partitioning profile resource. vectorization.vectorization_data_pipeline_object_id The resource ID of the agent's Vectorization pipeline (implemented in an upcoming release). vectorization.trigger_type The trigger type of the agent's Vectorization pipeline (implemented in an upcoming release). Permissible values are Manual, Schedule, and Event. vectorization.trigger_cron_schedule The schedule of the trigger in Cron format (implemented in an upcoming release). This property is valid only when trigger_type is Schedule. prompt_object_id The object ID of the prompt resource. language_model The language model configuration. The language_model object has been deprecated as of release 0.6.0. language_model.type The type of the language model. Currently supporting OpenAI based langauge models. language_model.provider The provider of the language model. Currently supporting microsoft or openai. language_model.temperature The temperature value for the language model. A value between 0 and 1. Values closer to 0 return more factual information whereas values closer to 1 yield more creative responses. language_model.use_chat Determines the type of language model to use, as an example, when using Microsoft's Azure OpenAI, specifying use_chat equal to true will use the AzureChatOpenAI model vs. the AzureOpenAI model in LangChain. language_model.api_endpoint The configuration setting key that houses the API endpoint of the language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.api_key The configuration setting key that houses a reference to a key vault value containing the API key for the language model service. Ensure these values are populated in key vault and app configuration. language_model.api_version The configuration setting key that houses the API version of the language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.version The configuration setting key that houses the version of the language model deployment. The example above uses default FLLM values. Ensure this value is populated in application configuration. language_model.deployment The configuration setting key that houses the name given to the deployed language model. The example above uses default FLLM values. Ensure this value is populated in application configuration. sessions_enabled A boolean value that indicates whether the agent is session-less (false) or supports sessions(true). conversation_history The conversation history configuration. conversation_history.enabled Indicates if conversation history is retained for subsequent agent interactions(true). conversation_history.max_history indicates the number of messages to be retained. gatekeeper The gatekeeper configuration. gatekeeper.use_system_setting Indicates if the system settings are used for the gatekeeper. gatekeeper.options Contains the list of gatekeeper options. The sample provided overrides the system setting for gatekeeper and enables Azure Content Safety and MS Presidio in the messaging pipeline. orchestration_settings The settings for the agent orchestrator. orchestration_settings.orchestrator FoundationaLLM currently supports LangChain and SemanticKernel for both types of Knowledge Management agents; however, Knowledge Management agents with an Inline Context can also use the AzureOpenAIDirect and AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration The endpoint configuration of the hosted LLM. FoundationaLLM currently supports Azure OpenAI and OpenAI. orchestration_settings.endpoint_configuration.endpoint The endpoint URL of the hosted LLM. The URL should be provided directly for the LangChain or SemanticKernel orchestrators; it should be provided as an Azure App Configuration key reference for the AzureOpenAIDirect or AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration.api_version The API version of the hosted LLM. For Azure OpenAI, this value should be set to the latest GA version. The API version should be provided directly for the LangChain or SemanticKernel orchestrators; it should be provided as an Azure App Configuration key reference for the AzureOpenAIDirect or AzureAIDirect orchestrators. orchestration_settings.endpoint_configuration.auth_type The authentication method of the hosted LLM. This value can either be token or key. For Azure OpenAI deployments, this value should be token, which configures the orchestrator to use Managed Identities for authentication. key-based authentication uses API keys. orchestration_settings.endpoint_configuration.api_key The name of the Azure App Configuration key storing the LLM endpoint API key. This parameter is required if auth_type is set to key. orchestration_settings.endpoint_configuration.provider The provider of the hosted LLM. FoundationaLLM currently supports microsoft (Azure OpenAI) or openai. orchestration_settings.endpoint_configuration.operation_type This field is set to chat by default and can be omitted. orchestration_settings.model_parameters Endpoint-specific model parameters. This field must be non-null if the provider is microsoft. orchestration_settings.model_parameters.deployment_name This field should be set to the name of the Azure OpenAI model deployment if the provider is microsoft. AzureOpenAIDirect Orchestrator The AzureOpenAIDirect orchestrator passes the user's prompt to an LLM deployed in an instance of Azure OpenAI Service, bypassing LangChain or Semantic Kernel. Example Configuration: { \"orchestration_settings\": { \"orchestrator\": \"AzureOpenAIDirect\", \"endpoint_configuration\": { \"endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"auth_type\": \"key\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"operation_type\": \"chat\" }, \"model_parameters\": { \"deployment_name\": \"completions\" } } } Note: AzureOpenAIDirect is only compatible with Knowledge Management agents with an Inline Context. AzureAIDirect Orchestrator The AzureAIDirect orchestrator passes the user's prompt to an LLM deployed as an Azure AI Studio real-time endpoint. This orchestrator allows customers to use a wider range of LLMs with FLLM agents. Example Configuration: { \"orchestration_settings\": { \"orchestrator\": \"AzureAIDirect\", \"endpoint_configuration\": { \"endpoint\": \"<AZURE APP CONFIGURATION KEY>\", \"api_key\": \"<AZURE APP CONFIGURATION KEY>\" }, \"model_parameters\": { \"temperature\": 0.8, \"max_new_tokens\": 1000, \"deployment_name\": \"<AZURE AI STUDIO DEPLOYMENT NAME>\" } } } Note: AzureAIDirect is only compatible with Knowledge Management agents with an Inline Context. Managing Knowledge Management Agents This section describes how to manage knowledge management agents using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name> Content-Type: application/json BODY <agent_configuration> where <agent_configuration> is the JSON agent configuration structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Agent/agents/<name> Note FLLM currently implements logical deletes for Knowledge Management agents. This means that users cannot create a Knowledge Management agent with the same name as a deleted Knowledge Management agent. Support for purging Knowledge Management agents will be added in a future release. Validating a Knowledge Management Agent Once configured, the knowledge management agent can be validated using an API call to the Core API or via the User Portal. Note It can take up to 5 minutes for a new Knowledge Management agent to appear in the User Portal or be accessible for requests from the Core API. Overriding agent parameters The agent parameters can be overridden at the time of the API call. Refer to the Core API documentation for more information."
  },
  "docs/archive/setup-guides/agents/prompt-resource.html": {
    "href": "docs/archive/setup-guides/agents/prompt-resource.html",
    "title": "Prompt | FoundationaLLM",
    "summary": "Prompt The FoundationaLLM (FLLM) prompt resource encapsulates the system prompt of an agent. The system prompt describes the persona of the agent and any instructional guardrails used to generate the desired responses to user prompts. The prompt resource is used in the Knowledge Management agent configuration. Prompt configuration The structure of a prompt is the following: { \"type\": \"multipart\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Prompt/prompts/<name>\", \"description\": \"<description>\", \"prefix\": \"<prompt_prefix>\", \"suffix\": \"<prompt_suffix>\" } where: <name> is the name of the agent. <instance_id> is the instance ID of the deployment. <description> is the description of the prompt, describing the persona of the agent. <prompt_prefix> is the beginning of the prompt. <prompt_suffix> (optional) appended to the end of the prompt (after any prefix and context). Parameter Description type The type - will be multipart. multipart prompts have a prefix and suffix. Support for basic prompts, which have no suffix, will be added in a future release. type must be the first parameter in the request body. name The name of the prompt. object_id The object ID of the prompt. Remove this key when creating a prompt, as it is automatically populated by the Management API. description The description of the prompt, ensure this description details the purpose or role of the prompt. prompt_prefix The beginning of the prompt. prompt_suffix Text appended to the ending of the prompt. Managing prompts This section describes how to manage knowledge management prompts using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name> Content-Type: application/json BODY <prompt_configuration> where <prompt_configuration> is the prompt configuration structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name> Note The delete operation is a logical delete. To purge a Prompt, call the /purge endpoint after deleting the Prompt. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name>/purge Content-Type: application/json BODY {}"
  },
  "docs/archive/setup-guides/branding/branding-app-configuration.html": {
    "href": "docs/archive/setup-guides/branding/branding-app-configuration.html",
    "title": "Branding Customization using the App Configurator | FoundationaLLM",
    "summary": "Branding Customization using the App Configurator The FoundationaLLM application provides a way to customize the branding of the applications. The branding customization can be done by setting any of the 21 configuration values available in the App Configuration resource in your Azure Resource Group. It can also be changed using the REST API calls pertaining to Branding. Accessing the App Configuration for branding customization In your Azure Resource Group, navigate to the App Configuration resource and select the Configuration Explorer option under Operations. You will see a list of all configuration settings for FoundationaLLM , filter the list on the word Branding and you will be able to see the 21 relevant configuration settings pertaining to Branding. The default User Interface of the FoundationaLLM is shown below The Login screen can also be customized as shown below The chat window can be customized as shown below FoundationaLLM:Branding:AccentColor Takes a hexadecimal or RGB color value to set the accent color of the chat application which is used for top header and the the token counter in the chat. FoundationaLLM:Branding:AccentTextColor Takes a hexadecimal or RGB color value to set the accent text color of the chat application which is used for top header and the the token counter in the chat. FoundationaLLM:Branding:AgentIconUrl A string value to set the Agent Icon Url which is displayed in the chat window. Important The AgentIconUrl can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the LogoUrl is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the configuration value directly. FoundationaLLM:Branding:BackgroundColor Takes a hexadecimal or RGB color value to set the background color of the chat application. FoundationaLLM:Branding:CompanyName Not is use. Reserved for future enhancements. FoundationaLLM:Branding:FavIconUrl A string value to set the FavIcon Url which is displayed in the browser tab. Important The FavIconUrl can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the FavIconUrl is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the configuration value directly. Important Any changes to the FoundationaLLM:Branding:FavIconUrl will require a restart of the CoreAPI image to take effect. FoundationaLLM:Branding:FooterText A string value to set the Footer Text value which is displayed at the bottom right of the screen. FoundationaLLM:Branding:KioskMode This is a boolean flag to indicate if the application is running in kiosk mode, if true will remove the sessions panel on the left navigation panel. Kiosk mode does not store user conversations and is meant to be used within a public kiosk setting FoundationaLLM:Branding:LogoText A string value to set the Logo Text value which is displayed only if the FoundationaLLM:Branding:LogoUrl is not set. FoundationaLLM:Branding:LogoUrl A string value to set the Logo Url which is displayed in top left header and also in the login screen. Important The LogoUrl can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the LogoUrl is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the configuration value directly. FoundationaLLM:Branding:PageTitle A string value to set the Page Title value which is displayed in the browser tab. Important Any changes to the FoundationaLLM:Branding:PageTitle will require a restart of the CoreAPI image to take effect. FoundationaLLM:Branding:PrimaryButtonBackgroundColor Takes a hexadecimal or RGB color value to set the Primary Button Background color of the chat application. It affects the Send button in the chat at the bottom right, the Close button the View Prompt screen and the login button in the login screen. FoundationaLLM:Branding:PrimaryButtonTextColor Takes a hexadecimal or RGB color value to set the Primary Button Text color of the chat application. It affects the Send button in the chat at the bottom right, the Close button the View Prompt screen and the login button in the login screen. FoundationaLLM:Branding:PrimaryColor Takes a hexadecimal or RGB color value to set the Primary color of the chat application. It affects the left navigation panel and the User Message panel in the chat. FoundationaLLM:Branding:PrimaryTextColor Takes a hexadecimal or RGB color value to set the Primary Text color of the chat application. It affects the left navigation panel and the User Message panel in the chat. FoundationaLLM:Branding:SecondaryButtonBackgroundColor Takes a hexadecimal or RGB color value to set the Secondary Button Background color of the chat application. It affects the collapsable arrow in the left navigation panel, the Sign Out bottom at the bottom and the attachment of files button in the chat window. FoundationaLLM:Branding:SecondaryButtonTextColor Takes a hexadecimal or RGB color value to set the Secondary Button Text color of the chat application. It affects the collapsable arrow in the left navigation panel, the Sign Out bottom at the bottom and the attachment of files button in the chat window. FoundationaLLM:Branding:SecondaryColor Takes a hexadecimal or RGB color value to set the Secondary color of the chat application. It affects the background color of the session identifier in the left navigations panel and the background of the entire login screen. FoundationaLLM:Branding:SecondaryTextColor Takes a hexadecimal or RGB color value to set the Secondary Text color of the chat application. It affects the background color of the session identifier in the left navigations panel. FoundationaLLM:Branding:NoAgentsMessage A string value to set the No Agents Message value which is displayed in the chat window when no agents are available. HTML allowed in this field. FoundationaLLM:Branding:DefaultAgentWelcomeMessage A string value to set the Default Agent Welcome Message value which is displayed in the chat window when the agent is connected. HTML allowed in this field."
  },
  "docs/archive/setup-guides/branding/branding-management-portal.html": {
    "href": "docs/archive/setup-guides/branding/branding-management-portal.html",
    "title": "| FoundationaLLM",
    "summary": "Company Name Not in use. Reserved for future enhancements. Fav Icon Url A string value to set the FavIcon Url which is displayed in the browser tab. Important The FavIconUrl can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the FavIconUrl is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the textbox directly.Any changes to the Fav Icon Url will require a restart of the CoreAPI image to take effect. Footer Text A string value to set the Footer Text value which is displayed at the bottom right of the screen. You can edit the text in the HTML editor as well to include special characters like ©. Kiosk Mode This is a boolean flag to indicate if the application is running in kiosk mode, if true will remove the sessions panel on the left navigation panel. Kiosk mode does not store user conversations and is meant to be used within a public kiosk setting Logo Text A string value to set the Logo Text value which is displayed only if the Logo Url is not set. Logo Url A string value to set the Logo Url which is displayed in top left header and also in the login screen. Important The Logo Url can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the Logo Url is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the textbox directly. A preview of the image will show up underneath the textbox. Page Title A string value to set the Page Title value which is displayed in the browser tab. Important Any changes to the Page Title will require a restart of the CoreAPI image to take effect. Agent Icon Url A string value to set the Agent Icon Url which is displayed in the chat window. Important The Agent Icon Url can be an SVG or PNG image placed in the Public folder of the application's source code but will require rebuilding of the docker image to reflect the changes. The preferred ways of setting the LogoUrl is to set it to a relative accessible public URL to an SVG or PNG image or include the full Base64 encoded image in the textbox directly. No Agents Message A string value to set the No Agents Message value which is displayed in the chat window when no agents are available. HTML allowed in this field. Default Agent Welcome Message A string value to set the Default Agent Welcome Message value which is displayed in the chat window when the agent is connected. HTML allowed in this field. Agents can have their own welcome message. If the selected agent does not have a configured welcome message, the Default Agent Welcome Message is displayed. It's also displayed when no agent is selected. Accent Color Takes a hexadecimal or RGB color value to set the accent color of the chat application which is used for top header and the the token counter in the chat. You can use the color picker to choose a color Accent Text Color Takes a hexadecimal or RGB color value to set the accent text color of the chat application which is used for top header and the the token counter in the chat. You can use the color picker to choose a color Background Color Takes a hexadecimal or RGB color value to set the background color of the chat application. You can use the color picker to choose a color Primary Button Background Color Takes a hexadecimal or RGB color value to set the Primary Button Background color of the chat application. It affects the Send button in the chat at the bottom right, the Close button the View Prompt screen and the login button in the login screen. You can use the color picker to choose a color Primary Button Text Color Takes a hexadecimal or RGB color value to set the Primary Button Text color of the chat application. It affects the Send button in the chat at the bottom right, the Close button the View Prompt screen and the login button in the login screen. You can use the color picker to choose a color Primary Color Takes a hexadecimal or RGB color value to set the Primary color of the chat application. It affects the left navigation panel and the User Message panel in the chat. You can use the color picker to choose a color Primary Text Color Takes a hexadecimal or RGB color value to set the Primary Text color of the chat application. It affects the left navigation panel and the User Message panel in the chat. You can use the color picker to choose a color Secondary Button Background Color Takes a hexadecimal or RGB color value to set the Secondary Button Background color of the chat application. It affects the collapsable arrow in the left navigation panel, the Sign Out bottom at the bottom and the attachment of files button in the chat window. You can use the color picker to choose a color Secondary Button Text Color Takes a hexadecimal or RGB color value to set the Secondary Button Text color of the chat application. It affects the collapsable arrow in the left navigation panel, the Sign Out bottom at the bottom and the attachment of files button in the chat window. You can use the color picker to choose a color Secondary Color Takes a hexadecimal or RGB color value to set the Secondary color of the chat application. It affects the background color of the session identifier in the left navigations panel and the background of the entire login screen. You can use the color picker to choose a color Secondary Text Color Takes a hexadecimal or RGB color value to set the Secondary Text color of the chat application. It affects the background color of the session identifier in the left navigations panel. You can use the color picker to choose a color Saving, resetting and Setting Default At the bottom of the branding page, you will find the Reset, Set Default and Save buttons. Reset: Returns any changed values to what they were before you changed them while editing the form. Set Default: This sets the branding values to the default out-of-the-box FoundationaLLM branding settings. Save: This will save the current values and apply them to the chat application. Show Contrast Information At the top of the branding page, you will find the Show Contrast Information toggle button. This will show the contrast information of the current branding settings. AA and AAA are levels of color contrast for web pages that indicate how readable the text is: AA: The minimum contrast ratio for normal text is 4.5:1, and 3:1 for large text. This level is considered a passing grade, but it might still be difficult for some users to read. AAA: The contrast ratio for normal text is 7:1, and 4.5:1 for large text. This level is considered to create the most accessible content and best user experience. Color contrast is important for legibility, especially for users with color blindness or other conditions that make it difficult to differentiate between colors. The Web Content Accessibility Guidelines (WCAG) recommend these contrast ratios to help designers create readable interfaces. While Level AAA is recommended, it's not required for all sites. The best approach is to consider your organization, the purpose of your content, and your typical users."
  },
  "docs/archive/setup-guides/branding/index.html": {
    "href": "docs/archive/setup-guides/branding/index.html",
    "title": "Branding Customization | FoundationaLLM",
    "summary": "Branding Customization The FoundationaLLM application provides three ways to customize the branding of the applications. Feel free to use whichever meets your needs. Using the App Configuration resource in your Azure Resource Group. Using the REST API calls pertaining to Branding. Using the Branding section in the FoundationaLLM Managment Portal application."
  },
  "docs/archive/setup-guides/exposed-apis/core-api.html": {
    "href": "docs/archive/setup-guides/exposed-apis/core-api.html",
    "title": "Core API | FoundationaLLM",
    "summary": "Core API The Core API serves as the entry point for user requests to FoundationaLLM's underlying engine. While clients primarily interact with the Core API through the Chat UI, the Core API exposes some convenient interfaces for developers. Sessionless Completion The sessionless completion endpoint enables users to query agents without first creating a chat session. Endpoint: [DEPLOYMENT URL]/core/orchestration/completion?api-version=1.0 Note: For AKS deployments, [DEPLOYMENT URL] is the same as the cluster FQDN, while for ACA deployments, the Core API endpoint can be found by navigating to the [DEPLOYMENT PREFIX]coreca Container App in the Azure Portal. Sample Request: { \"user_prompt\": \"What are your capabilities?\", \"settings\": { \"agent_name\": \"internal-context\", \"model_parameters\": { \"temperature\": 0.4, \"deployment_name\": \"completions\", \"top_k\": 5, \"top_p\": 0.9, \"do_sample\": true, \"max_new_tokens\": 100, \"return_full_text\": true, \"ignore_eos\": true }, \"agent_parameters\": { \"index_filter_expression\": \"search.ismatch('FoundationaLLM', 'Text')\", \"index_top_n\": 5 } } } Note The settings object provides to override various parameters at runtime, and is optional. Within settings both model_parameters and settings.agent_parameters (along with their members) are optional. If not provided, the Core API will use the default model and agent settings. model_parameters: Name Type Description temperature float Controls randomness. Lowering the temperature means that the model will produce more repetitive and deterministic responses. Increasing the temperature will result in more unexpected or creative responses. Try adjusting temperature or Top P but not both. This value should be a float between 0.0 and 1.0. deployment_name string The deployment name for the language model. top_k int The number of highest probability vocabulary tokens to keep for top-k-filtering. Default value is null, which disables top-k-filtering. top_p float The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Top P (or Top Probabilities) is imilar to temperature, this controls randomness but uses a different method. Lowering Top P will narrow the model’s token selection to likelier tokens. Increasing Top P will let the model choose from tokens with both high and low likelihood. Try adjusting temperature or Top P but not both. do_sample bool Whether or not to use sampling; use greedy decoding otherwise. max_new_tokens int Sets a limit on the number of tokens per model response. The API supports a maximum of number of tokens (depending on the deployment) shared between the prompt (including system message, examples, message history, and user query) and the model's response. One token is roughly 4 characters for typical English text. return_full_text bool Whether or not to return the full text (prompt + response) or only the generated part (response). Default value is false. ignore_eos bool Whether to ignore the End of Sequence(EOS) token and continue generating tokens after the EOS token is generated. Defaults to False. agent_parameters: Name Type Description index_filter_expression string This value should be a string representing the search filter expression to limit documents to be searched by the index retriever index_top_n int Controls the number of search results to return from an index for prompt augmentation. Payload Headers: Header Value Details Authorization Bearer [ENTRA ID BEARER TOKEN] Valid token from Entra ID Content-Type application/json Sample Response: { \"text\": \"FoundationaLLM is a copilot platform that simplifies and streamlines building knowledge management and analytic agents over the data sources present across your enterprise. It provides integration with enterprise data sources used by agents for in-context learning, fine-grain security controls over data used by agents, and pre/post completion filters that guard against attack. The solution is scalable and load balances across multiple endpoints. It is also extensible to new data sources, new LLM orchestrators, and LLMs. You can learn more about FoundationaLLM at https://foundationallm.ai.\" } Sample Postman Request: /orchestration/completion/Requests a completion from the downstream APIs."
  },
  "docs/archive/setup-guides/exposed-apis/index.html": {
    "href": "docs/archive/setup-guides/exposed-apis/index.html",
    "title": "Exposed API Documentation | FoundationaLLM",
    "summary": "Exposed API Documentation FoundationaLLM (FLLM) exposes two convenient API interfaces that enable developers and architects to directly interact with the FLLM platform for automation and integration tasks. This approach provides an alternative to using the provided user interfaces. Core API: The Core API supports direct completion requests against FLLM's agents. Management API: The Management API abstracts FLLM's configuration, enabling programmatic setup of agents, datasources, and prompts. The Management UI implements a frontend interface on top of the Management API, simplifying common agent configuration tasks"
  },
  "docs/archive/setup-guides/exposed-apis/management-api.html": {
    "href": "docs/archive/setup-guides/exposed-apis/management-api.html",
    "title": "| FoundationaLLM",
    "summary": "Management API Think of the FoundationaLLM Core API as a read-only API. It provides a way to query the system for information about agents, users, and other resources. The Management API, on the other hand, is a read-write API. It allows you to create, update, and delete resources in the system. Management API Endpoints The Management API provides configuration and resource management endpoints that can be consumed in the following three ways: Direct REST-based calls programmatically or through some tool like Postman. Through the Management Portal user interface. Through the FoundationaLLM CLI (Coming Soon). These options eliminate the need to directly manipulate files to define agents and their related resources, enable certain users to update categories of app configurations without needing to have access to the Azure App Configuration service, and perform additional platform-related steps as needed when changing configurations without the user needing to know about them (restart services, refresh caches, etc.). Management API Features In addition to modifying/creating content and platform components, the Management API centralizes health and service monitoring. Using the Management API from Postman, you can replace the {{baseUrl}} variable with the actual URL of the Management API resource, and the {{InstanceId}} variable with the actual instance ID from your app configuration resource that was generated for you. You can \"GET\" the configuration of an agent, and \"POST\" to create a new agent or \"PUT\" to update an existing agent."
  },
  "docs/archive/setup-guides/exposed-apis/resource-management/resource-management.html": {
    "href": "docs/archive/setup-guides/exposed-apis/resource-management/resource-management.html",
    "title": "Resource Management in FoundationaLLM | FoundationaLLM",
    "summary": "Resource Management in FoundationaLLM With the introduction of the Management API, you can now manage resources in FoundationaLLM programmatically or through the Management API User Interface Portal. This includes creating, updating, and deleting resources in the system. Resource Providers The main concept of the Management API is the resource provider. A resource provider is a service that provides resources to the FoundationaLLM system. For example, the agents, prompts and datasources are provided by a resource provider. The Management API provides a way to manage these resources without the need to manually work with JSON files in storage containers and mimics the same concept and functionality of resources in the Azure Portal. Resource Provider Structure The resource-provider container in the main storage account that was deployed on your behalf in your subscription contains the following structure: Agent References This first folder FoundationaLLM.Agent contains the Agent References. The content of the _agent-references references all the locations of the JSON files that contain the agent information. The _agent-references folder contains the following structure: { \"AgentReferences\": [ { \"Name\": \"sotu-2023\", \"Filename\": \"/FoundationaLLM.Agent/sotu-2023.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu2\", \"Filename\": \"/FoundationaLLM.Agent/sotu2.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu3\", \"Filename\": \"/FoundationaLLM.Agent/sotu3.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu\", \"Filename\": \"/FoundationaLLM.Agent/sotu.json\", \"Type\": \"knowledge-management\" } ] } From that starting point for the agent references, we get to point to JSON file that describes each agent available to the system. Let's start by taking a look at one odf the agents from above called sotu-2023.json { \"name\": \"sotu-2023\", \"type\": \"knowledge-management\", \"object_id\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Agent/agents/sotu-2023\", \"description\": \"Knowledge Management Agent that queries the State of the Union speech transcript\", \"indexing_profile\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Vectorization/indexingprofiles/sotu-index\", \"embedding_profile\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding\", \"language_model\": { \"type\": \"openai\", \"provider\": \"microsoft\", \"temperature\": 0.0, \"use_chat\": true, \"api_endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"version\": \"FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion\", \"deployment\": \"FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName\" }, \"sessions_enabled\": true, \"conversation_history\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper\": { \"use_system_setting\": false, \"options\": [ \"ContentSafety\", \"Presidio\" ] }, \"orchestrator\": \"LangChain\", \"prompt\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Prompt/prompts/sotu\" } Notice all the different keys and values that are present to identify the agent. This JSON file is usually created or modifed through the Management API UI Portal or via POST or PUT requests to the Management API using a product like POSTMAN. The type could be \"knowledge-management\" or \"analytical\" The language-model section is to identify the provider, its accuracy and endpoints to retrieve from the app configuration resource. sessions_enabled is a boolean to enable or disable the ability to start a session Vs just a one time query using an API tool like Postman. conversation_history is to enable or disable the ability to store the conversation history and the maximum number of conversations to store in case the previous session_enabled is set to true. The gatekeeper section is to enable or disable the use of the system settings for content safety and presidio. If set to false, then the options array will be used to identify the specific gatekeepers to use. The orchestrator is the name of the orchestrator to use for the agent. The orchestrator is the component that is responsible for managing the flow of the conversation and the execution of the agent's logic. It could be LangChain or Semantic Kernel and more options could be used in the future with the growth of the platform and the industry for orchestrators. The prompt is the reference to the prompt that the agent will use to start the conversation. The prompt is a resource that is used to start the conversation with the agent. It is a JSON file that contains the prompt text and the prompt settings. Prompt References The second folder FoundationaLLM.Prompt contains the Prompt References. Within that folder, we have the _prompt-references JSON file that contains the following structure: { \"PromptReferences\": [ { \"Name\": \"sotu5\", \"Filename\": \"/FoundationaLLM.Prompt/sotu5.json\" }, { \"Name\": \"sotu-test\", \"Filename\": \"/FoundationaLLM.Prompt/sotu-test.json\" }, { \"Name\": \"sotu\", \"Filename\": \"/FoundationaLLM.Prompt/sotu.json\" } ] } These references point to the JSON files that contain the prompt information. Let's take a look at one of the prompts from above called sotu5.json for an example: { \"name\": \"sotu5\", \"type\": \"prompt\", \"object_id\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Prompt/prompts/sotu5\", \"description\": \"Prompt for the Knowledge Management Agent that queries the State of the Union speech transcript\", \"prefix\": \"You are a political science professional named Baldwin. You are responsible for answering questions regarding the February 2023 State of the Union Address.\\nAnswer only questions about the February 2023 State of the Union address. Do not make anything up. Check your answers before replying.\\nProvide concise answers that are polite and professional.\", \"suffix\": \"\" } It contains the name, type of prompt, the object_id reference, description and of course most importantly the prefix and suffix of the prompt. The prefix and suffix are the text that will be used to start and end the conversation with the agent. Data Source References A Data Source refers to the location of data that is to be leveraged by an agent. The data source could be a storage account, database, website, etc. The data source references are stored in the FoundationaLLM.DataSource folder. The references are stored in the _datasource-references JSON file that contains the following structure: { \"DataSourceReferences\": [ { \"Name\": \"datalake01\", \"Filename\": \"/FoundationaLLM.DataSource/datalake01.json\", \"Type\": \"azure-data-lake\", \"Deleted\": false }, { \"Name\": \"sharepointsite01\", \"Filename\": \"/FoundationaLLM.DataSource/sharepointsite01.json\", \"Type\": \"sharepoint-online-site\", \"Deleted\": false } ] } These references point to the JSON files that contain the prompt information. Let's take a look at one of the prompts from above called datalake01.json for an example: { \"name\": \"datalake01\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"display_name\": null, \"description\": \"Azure Data Lake data source.\", \"folders\": [ \"/vectorization-input/journals/2024\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\", \"APIKey\": \"FoundationaLLM:DataSources:datalake01:APIKey\", \"Endpoint\": \"FoundationaLLM:DataSources:datalake01:Endpoint\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } In this example, the data source is an Azure Data Lake data source. The folders array contains the paths to the folders in the data lake that contain the data to be used by the agent. The configuration_references section contains the references to the configuration settings that are used to connect to the data source. The created_on, updated_on, created_by, updated_by are the timestamps and the user that created and updated the data source. The deleted flag is used to mark the data source as deleted. Vectorization Profile References Finally the third folder FoundationaLLM.Vectorization contains the Vectorization References. Where you will find important JSON files: vectorization-indexing-profiles.json { \"DefaultResourceName\": \"AzureAISearch_Default_002\", \"Resources\": [ { \"type\": \"indexing-profile\", \"name\": \"AzureAISearch_Default_002\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/indexingprofiles/AzureAISearch_Default_002\", \"description\": null, \"deleted\": false, \"indexer\": \"AzureAISearchIndexer\", \"settings\": { \"IndexName\": \"fllm-default-002\", \"TopN\": \"3\", \"Filters\": \"\", \"EmbeddingFieldName\": \"Embedding\", \"TextFieldName\": \"Text\" }, \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType\", \"Endpoint\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint\" } } ] } This is where we identify the name and the Indexer to use for the indexing of the content. And within the configuration_references section, we identify the AuthenticationType and Endpoint to use for the indexing. It could be indexing against the Azure AI Search or any other indexer that is available in the system and more will be supported in the future. The DefaultResourceName is the name of the default indexing profile to use in the system if none is specified. vectorization-text-embedding-profiles.json { \"DefaultResourceName\": \"AzureOpenAI_Embedding\", \"Resources\": [ { \"type\": \"text-embedding-profile\", \"name\": \"AzureOpenAI_Embedding\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding\", \"description\": null, \"deleted\": false, \"text_embedding\": \"SemanticKernelTextEmbedding\", \"settings\": {}, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey\", \"APIVersion\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIVersion\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType\", \"DeploymentName\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName\", \"Endpoint\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint\" } } ] } This is where we identify the name and the Text Embedding to use for the vectorization of the text. And within the configuration_references section, we identify the APIKey, APIVersion, AuthenticationType, DeploymentName and Endpoint to use for the text embedding. vectorization-text-partitioning-profiles.json { \"DefaultResourceName\": \"DefaultTokenTextPartition_Small\", \"Resources\": [ { \"type\": \"text-partitioning-profile\", \"name\": \"DefaultTokenTextPartition_Small\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textpartitioningprofiles/DefaultTokenTextPartition_Small\", \"display_name\": null, \"description\": null, \"text_splitter\": \"TokenTextSplitter\", \"settings\": { \"Tokenizer\": \"MicrosoftBPETokenizer\", \"TokenizerEncoder\": \"cl100k_base\", \"ChunkSizeTokens\": \"500\", \"OverlapSizeTokens\": \"50\" }, \"configuration_references\": {}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } ] } This is where we identify the name and the Text Splitter to use for the chunking and overlapping of the text. In the settings section, we identify the tokenizer and the encoder to use for the text partitioning and the chunk size and overlap size in tokens. vectorization-pipelines.json A vectorization pipeline provides a definition for a reusable and triggerable profile that includes identifying a data source that is the source for vectorization, vectorization profiles, as well as the trigger type. { \"DefaultResourceName\": \"sdzwa\", \"Resources\": [ { \"type\": \"vectorization-pipeline\", \"name\": \"sdzwa\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationPipelines/sdzwa\", \"display_name\": null, \"description\": \"Vectorization data pipeline dedicated to the sdzwa january 2024 pdf.\", \"active\": false, \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"text_partitioning_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/Streamline\", \"text_embedding_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding_v2\", \"indexing_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/indexingprofiles/AzureAISearch_Default_002\", \"trigger_type\": \"Event\", \"trigger_cron_schedule\": null, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } ] } Vectorization Request Resources The storage of vectorization request resources are located in the vectorization-state container following the standard organization of /requests/yyyyMMdd/yyyyMMdd-vectorizationrequestid.json where yyyyMMdd (UTC) is the date of the request and vectorizationrequestid is the unique identifier of the request. When a vectorization request is received, the request gets created and is updated once the request has been processed. An example of a completed vectorization request is: { \"id\": \"f8d940a2-77c0-4b3e-8709-e26445f9743e\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationRequests/f8d940a2-77c0-4b3e-8709-e26445f9743e\", \"Expired\": false, \"resource_filepath\": \"requests/20240419/20240419-f8d940a2-77c0-4b3e-8709-e26445f9743e.json\", \"content_identifier\": { \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"multipart_id\": [ \"fllmaks14sa.dfs.core.windows.net\", \"vectorization-input\", \"sdzwa/journals/2024/SDZWA-Journal-January-2024.pdf\" ], \"canonical_id\": \"sdzwa/journals/2024/SDZWA-Journal-January-2024\", \"metadata\": null }, \"processing_type\": \"Asynchronous\", \"pipeline_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationPipelines/sdzwa\", \"pipeline_execution_id\": \"541ae81c-08ea-4ba0-b58c-9d904260a5a2\", \"processing_state\": \"Completed\", \"execution_start\": \"2024-04-19T03:57:33.4571856Z\", \"execution_end\": \"2024-04-19T04:00:41.8572236Z\", \"error_messages\": [], \"steps\": [ { \"id\": \"extract\", \"parameters\": {} }, { \"id\": \"partition\", \"parameters\": { \"text_partitioning_profile_name\": \"Streamline\" } }, { \"id\": \"embed\", \"parameters\": { \"text_embedding_profile_name\": \"AzureOpenAI_Embedding_v2\" } }, { \"id\": \"index\", \"parameters\": { \"indexing_profile_name\": \"AzureAISearch_Default_002\" } } ], \"completed_steps\": [ \"extract\", \"partition\", \"embed\", \"index\" ], \"remaining_steps\": [], \"current_step\": null, \"error_count\": 0, \"running_operations\": {}, \"last_successful_step_time\": \"2024-04-19T04:00:41.8554435Z\" } Valid states for the processing_state property are New, InProgress, Completed, and Failed. Any errors encountered during the processing of a request are stored in the error_messages array. Note: Triggering the vectorization process is done through the Management API by issuing a process action on the resource. See the Triggering Vectorization section for more information. Synchronous Versus Asynchronous Vectorization The vectorization process can be done in a synchronized or asynchronized manner. The synchronized manner is when the vectorization process is done in real time in memory and the results are returned immediately. The asynchronized manner is when the vectorization process is done in the background and the results are returned at a later time. The asynchronized manner is useful when the vectorization process is expected to take a long time to complete and the user does not want to wait for the results. The asynchronized manner is also useful when the vectorization process is expected to be done in batches and the user does not want to wait for the results of each batch. For example, you would use syncronized vactorization when you have one or few files that you want to vectorize and you want the results immediately. You would use asynchronized vectorization when you have hundred or thousands of files that you want to vectorize and you want the results at a later time."
  },
  "docs/archive/setup-guides/index.html": {
    "href": "docs/archive/setup-guides/index.html",
    "title": "Setup guides | FoundationaLLM",
    "summary": "Setup guides After deploying FoundationaLLM, you can customize the solution components to adapt it for your needs. Follow the Quickstart guide to get started with FoundationaLLM. Set up FLLM agents to define a persona and connect to your data sources to generate responses. Configure vectorization to enable vector search."
  },
  "docs/archive/setup-guides/management-ui/management-ui.html": {
    "href": "docs/archive/setup-guides/management-ui/management-ui.html",
    "title": "Management UI | FoundationaLLM",
    "summary": "Management UI The Management UI enables FLLM administrators to configure agents without directly calling the Management API. Creating New Agent Navigate to the Create New Agent page using the side navigation bar. Set the agent type: Knowledge Management or Analytics. FoundationaLLM currently only supports Knowledge Management agents. Set the agent Knowledge Source: Expand the dropdown arrow next to the upper left box. Select the correct Content Source Profile. Expand the dropdown arrow next to the upper right box to open the Indexing Profile dropdown. Select the correct Indexing Profile. Expand the dropdown arrow next to the lower left box. Set the Chunk size and Overlap size settings for text partitioning. Select Done. Expand the dropdown arrow next to the lower right box. Set the trigger Frequency; FoundationaLLM currently only supports Manual triggers. Configure user-agent interactions. Enable conversation history using the Yes/No Radio Button. Select Done. Configure the Gatekeeper. Then, select Done. Enable/Disable the Gatekeeper using the Radio Button Set the Content Safety platform to either None or Azure Content Safety using the dropdown menu Set the Data Protection platform to either None or Microsoft Presidio using the dropdown menu Lastly, set the System Prompt. The prompt prefixes users' requests to the agent, influencing the tone and functionality of the agent. After setting the desired agent configuration, select Create Agent at the bottom right-hand corner of the page. You will be able to edit the agent configuration after creation from the Public Agents page."
  },
  "docs/archive/setup-guides/quickstart.html": {
    "href": "docs/archive/setup-guides/quickstart.html",
    "title": "Quickstart guide | FoundationaLLM",
    "summary": "Quickstart guide After deploying FoundationaLLM, complete the following steps to get started: Configure your deployment's authentication settings. Setup your agents to define a persona and connect to your data sources to generate responses. Find your User Portal URL and log in to start using FoundationaLLM. Find your User Portal (chat UI) URL If you performed an Azure Container Apps (ACA) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Container App resource whose name ends with chatuica. Within the Overview pane, copy the Application Url value. This is the URL for the chat application. If you performed an Azure Kubernetes Service (AKS) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Kubernetes Service resource. Select Properties in the left-hand menu and copy the HTTP application routing domain value. This is the URL for the chat application. When you navigate to the chat UI application, you will be prompted to log in. Find your Core API URL If you performed an Azure Container Apps (ACA) deployment, follow these steps to obtain the URL for the Core API: Within the Resource Group that was created as part of the deployment, select the Container App resource whose name ends with coreca. Within the Overview pane, copy the Application Url value. This is the URL for the Core API. If you performed an Azure Kubernetes Service (AKS) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Kubernetes Service resource. Select Properties in the left-hand menu and copy the HTTP application routing domain value. This is the URL for the chat application. Your Core API URL (for your AKS deployment) is the URL you just copied with /core appended to the end of it. For example, if your Core API URL is https://1cf699fd0d89446eabf2.eastus.aksapp.io/, then your Core API URL is https://1cf699fd0d89446eabf2.eastus.aksapp.io/core."
  },
  "docs/archive/setup-guides/vectorization/index.html": {
    "href": "docs/archive/setup-guides/vectorization/index.html",
    "title": "Vectorization | FoundationaLLM",
    "summary": "Vectorization This section describes the content vectorization concepts and how to configure vectorization in FoundationaLLM. Use the links below to learn more about configuring vectorization in FoundationaLLM: Vectorization concepts Configuring vectorization Managing vectorization profiles Triggering vectorization Monitoring and troubleshooting vectorization"
  },
  "docs/archive/setup-guides/vectorization/vectorization-concepts.html": {
    "href": "docs/archive/setup-guides/vectorization/vectorization-concepts.html",
    "title": "Vectorization Concepts | FoundationaLLM",
    "summary": "Vectorization Concepts FoundationaLLM (FLLM) provides utilities and services to support vectorizing your data in batch and on-demand modalities. Vectorization is a multi-step process, starting with loading your data, splitting (or chunking) the data as required, performing vector embeddings, and storing the vectors into a vector index so an agent can later retrieve relevant information through a vector search. In FLLM, vectorization is an idempotent operation, meaning that vectorizing the same document multiple times will result in the same vector being stored in the vector index. This is useful for re-vectorizing documents that have been updated, or for cases where only parts of the vectorization process need to be run (e.g., only the vector embeddings need to be updated). For each individual document, vectorization is performed by executing a vectorization request process action via the Management API upon receiving a valid vectorization request, either through a direct call or via a triggered vectorization pipeline. Based on the details of the vectorization request, a processing a vectorization request can be executed in one of the following modes: Synchonously - the vectorization steps start executing immediately and execute sequentially until the processing is completed or an error occurs. This type of execution is used for on-demand vectorization and is well suited for small to medium sized documents and relatively small numbers of documents at a time. Asynchronously - the vectorization steps are submitted to queues and executed by workers. This type of execution is used for batch vectorization and is well suited for large numbers of documents at a time. It is also well suited for vectorizing documents of any size. The FLLM platform components involved in vectorization are: Management API (creates vectorization requests and exposes the process action on vectorization request resources). Vectorization API (processes vectorization requests and executes synchonous vectorization pipelines). Note: The Vectorization API is used internally by the FoundationaLLM platform and is not intended to be used directly by users. Vectorization Worker(s) (execute asynchronous vectorization pipelines). A FLLM instance deploys one instance of the Vectorization API and one or more instances of the Vectorization Worker. See Configuring vectorization for more details on configuring these components. Note The initialization of both the Vectorization API and the Vectorization Worker is a time consuming process, as it involves dowloading and initializing various elements (e.g., Byte-Pair encoding dictionaries). As a result, after restarting the API, it might take up to a minute until it becomes ready to accept vectorization requests. It is recommended to either use the status endpoint of the Vectorization API to determine when it is ready to accept requests, or to wait for a minute after restarting the API before sending vectorization requests. Vectorization Pipelines Vectorization pipelines are aggregations of multiple vectorization requests, for example, a vectorization pipeline may be defined to vectorize all documents in a specific data source, such as an ADLS Gen2 container. Vectorization pipelines can be triggered in one of the following ways: None (no triggering of vectorization pipelines). Manual (vectorization pipelines are triggered manually by calling the Vectorization API). The typical use cases for on-demand vectorization (either synchronous or asynchronous) are testing, manual vectorization (or re-vectorization), and application integration (where another platform component triggers vectorization). Content-based (vectorization pipelines are triggered automatically when either new content is added to a content source or existing content is updated). Schedule-based (vectorization pipelines are triggered automatically based on a schedule). Note Content-based and schedule-based triggering are currently in pre-release and are not yet available in public releases of FLLM. When working with vectorization in FLLM, the typical steps you have to perform are: Ensure that the Management API, Vectorization API and Vectorization Worker are configured and running. This is a one-time operation. For more details, see Configuring vectorization. Create vectorization profiles. You can either reuse existing profiles or create new ones. For more details, see Managing vectorization profiles. Submit vectorization requests to the Management API. For more details, see Triggering vectorization."
  },
  "docs/archive/setup-guides/vectorization/vectorization-configuration.html": {
    "href": "docs/archive/setup-guides/vectorization/vectorization-configuration.html",
    "title": "Configuring vectorization | FoundationaLLM",
    "summary": "Configuring vectorization This section provides details on how to configure the vectorization API and workers in FoundationaLLM. Note These configurations should already be in place if you deployed FoundationaLLM (FLLM) using the recommended deployment scripts. The details presented here are provided for cases in which you need to troubleshoot or customize the configuration. Configuration for Vectorization API The following table describes the Azure artifacts required for the vectorization pipelines. Artifact name Description vectorization-input Azure storage container used by default to store documents to be picked up by the vectorization pipeline. Must be created on a Data Lake storage account (with the hierarchical namespace enabled). The following table describes the environment variables required for the vectorization pipelines. Environment variable Description FoundationaLLM_AppConfig_ConnectionString Connection string to the Azure App Configuration instance. The following table describes the required configuration parameters for the vectorization pipelines. App Configuration Key Default Value Description FoundationaLLM:APIs:VectorizationAPI:APIUrl The URL of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:APIKey Key Vault secret name: foundationallm-apis-vectorizationapi-apikey The API key of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization API. Note Refer to the App Configuration values page for more information on how to set these and other configuration values. Configuration for Vectorization workers The following table describes the Azure artifacts required for the vectorization pipelines. Artifact Name Description embed Azure storage queue used for the embed vectorization pipeline. Can be created on the storage account used for the other queues. extract Azure storage queue used for the extract vectorization pipeline. Can be created on the storage account used for the other queues. index Azure storage queue used for the index vectorization pipeline. Can be created on the storage account used for the other queues. partition Azure storage queue used for the partition vectorization pipeline. Can be created on the storage account used for the other queues. vectorization-state Azure storage container used for the vectorization state service. Can be created on the storage account used for the other queues. resource-provider Azure storage container used for the internal states of the FoundationaLLM resource providers. resource-provider/FoundationaLLM.Vectorization/vectorization-pipelines.json Azure storage blob used for the vectorization pipeline resources managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization pipelines. resource-provider/FoundationaLLM.Vectorization/vectorization-text-partitioning-profiles.json Azure storage blob used for the text partitioning profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization text partitioning profiles. resource-provider/FoundationaLLM.Vectorization/vectorization-text-embedding-profiles.json Azure storage blob used for the text embedding profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization text embedding profiles. resource-provider/FoundationaLLM.Vectorization/vectorization-indexing-profiles.json Azure storage blob used for the indexing profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization indexing profiles. resource-provider/FoundationaLLM.DataSources Azure storage directory where the data sources managed by the FoundationaLLM.DataSources resource provider are stored. vectorization-state/requests/{requestid-yyyyMMdd}.json Azure storage directory where vectorization requests managed by the vectorization state service are stored. vectorization-state/execution-state/{canonical_id} Azure storage directory where the execution state of the vectorization requests and their resulting artifacts are stored. The canonical_id is defined in the vectorization request. vectorization-state/pipeline-state/{pipeline_name}/{pipeline_name}-{execution_id}.json Azure storage directory where the state of the vectorization pipeline execution is stored. The pipeline_name is the name of the vectorization pipeline and the execution_id is the unique identifier of the execution. The following table describes the environment variables required for the vectorization pipelines. Environment variable Description FoundationaLLM_AppConfig_ConnectionString Connection string to the Azure App Configuration instance. The following table describes the required App Configuration parameters for the vectorization pipelines. App Configuration Key Default Value Description FoundationaLLM:APIs:VectorizationWorker:APIUrl The URL of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:APIKey Key Vault secret name: foundationallm-apis-vectorizationworker-apikey The API key of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization worker API. FoundationaLLM:Vectorization:VectorizationWorker The settings used by each instance of the vectorization worker service. For more details, see default vectorization worker settings. FoundationaLLM:Vectorization:Queues:Embed:AccountName The account name of the Azure Storage account used for the embed vectorization queue. FoundationaLLM:Vectorization:Queues:Extract:AccountName The account name of the Azure Storage account used for the extract vectorization queue. FoundationaLLM:Vectorization:Queues:Index:AccountName The account name of the Azure Storage account used for the index vectorization queue. FoundationaLLM:Vectorization:Queues:Partition:AccountName The account name of the Azure Storage account used for the partition vectorization queue. FoundationaLLM:Vectorization:StateService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey Key Vault secret name: foundationallm-vectorization-semantickerneltextembedding-openai-apikey The API key used to connect to the Azure OpenAI service. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName The name of the Azure OpenAI model deployment. The default value is embeddings. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint The endpoint of the Azure OpenAI service. FoundationaLLM:Vectorization:AzureAISearchIndexingService:APIKey Key Vault secret name: foundationallm-vectorization-azureaisearch-apikey The API key used to connect to the Azure OpenAI service. FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint The endpoint of the Azure OpenAI service. Note Refer to the App Configuration values page for more information on how to set these and other configuration values. The following table describes the external content used by the vectorization worker to initialize: Uri Description https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken The public Azure Blob Storage account used to download the OpenAI BPE ranking files. Note The vectorization worker must be able to open HTTPS connections to the external content listed above. Default vectorization worker settings The default settings for the vectorization worker are stored in the FoundationaLLM:Vectorization:VectorizationWorker App Configuration key. The default structure for this key is: { \"RequestManagers\": [ { \"RequestSourceName\": \"extract\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"partition\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"embed\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"index\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 } ], \"RequestSources\": [ { \"Name\": \"extract\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"partition\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"embed\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"index\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 } ], \"QueuingEngine\": \"AzureStorageQueue\" } The following table provides details about the configuration parameters: Parameter Description RequestManagers The list of request managers used by the vectorization worker. Each request manager is responsible for managing the execution of vectorization pipelines for a specific vectorization step. The configuration must include all request managers. RequestManagers.MaxHandlerInstances The maximum number of request handlers that process requests for the specified request source. By default, the value is 1. You can change the value to increase the processing capacity of each vectorization worker instance. The value applies to all istances of the vectorization worker. NOTE: It is important to align the value of this setting with the level of compute and memory resources allocated to the individual vectorization worker instances. RequestManagers.QueueProcessingPace Optional The delay in seconds to wait between requests after a request has been processed. The default value is 5. RequestManagers.QueuePollingInterval Optional The polling interval in seconds, this is the amount of time to wait if the previous check on the queue had no items. The default value is 60. RequestManagers.QueueMaxNumberOfRetries Optional The maximum number of retries to attempt to process a request before being removed from the queue. The default value is 5. RequestSources The list of request sources used by the vectorization worker. Each request source is responsible for managing the requests for a specific vectorization step. The configuration must include all request sources. RequestSources.Name The name of the request source. The name must match the name of the request manager. RequestSources.AccountName The name of the configuration key for the Azure Storage account used for the queue (include the tokens after FoundationaLLM:Vectorization:Queues:). RequestSources.VisibilityTimeoutSeconds In the case of queue-based request sources (the default for the vectorization worker), specifies the time in seconds until a dequeued vectorization step request must be executed. During this timeout, the message will not be visible to other handler instances within the same worker or from other worker instances. If the handler fails to process the vectorization step request successfully and remove it from the queue within the specified timeout, the message will become visibile again. The default value is 600 seconds and should not be changed."
  },
  "docs/archive/setup-guides/vectorization/vectorization-monitoring-troubleshooting.html": {
    "href": "docs/archive/setup-guides/vectorization/vectorization-monitoring-troubleshooting.html",
    "title": "Monitoring and troubleshooting vectorization | FoundationaLLM",
    "summary": "Monitoring and troubleshooting vectorization The typical steps you have to perform when monitoring and troubleshooting vectorization in FoundationaLLM (FLLM) are: Check the configuration of the Vectorization API and Vectorization Worker. For more details, see Configuring vectorization. Check the working condition of the Management API, Vectorization API and Vectorization Worker(s). Ensure the services have started and initialized successfully. Check the status endpoints for the Core API, Vectorization API and the Management API. You can do this by submitting a HTTP GET request to the /status endpoint of these APIs and validate that you get a HTTP 200 OK response with body like <api_name> - ready. Check the logs of the Management Vectorization API and Vectorization Worker(s) for errors. By default, the logs are written to the Azure App Insights Log Analytics Workspace deployed by FLLM. Check the definitions of the vectorization profiles used in the vectorization requests. For more details, see Managing vectorization profiles. Ensure all the required app configuration elements are present and have the correct values. Check the state of the vectorization requests. By default, the vectorization requests are stored in the vectorization-state container of the Azure Storage account deployed by FLLM. State and logging of vectorization requests All state and logging of vectorization requests are stored in the vectorization-state container of the Azure Storage account deployed by FLLM. Vectorization request resource files Each vectorization request resource is stored in the vectorization-state/requests folder. The request resources are created and managed through the Management API. The naming convention is: vectorization-state/requests/<request_id>-<yyyMMdd>.json. The resource file is updated as the vectorization request progresses through the processing. The resource file contains the following fields that can assist in troubleshooting: Field Description id The unique identifier of the vectorization request. When looking up the subsequent execution state, this is the identifier that is used in the file name. content-identifier.canonical_id The canonical id of the vectorization request. This is the path within the execution-state folder where additional logs and associated vectorization artifacts are stored. processing-state The current state of the vectorization request, values can be New, InProgress, Completed, Failed error_messages A high level list of error messages encountered during processing. current_step The step currently being executed, or the step in which a failure occurred. pipeline_object_id When created through a vectorization pipeline, this field contains the object id of the pipeline. pipeline_execution_id When process is initiated through a vectorization pipeline, this field contains the unique identifier of the pipeline execution. Vectorization execution state files The execution state of a vectorization request is stored in the vectorization-state/execution-state folder. The naming convention is: vectorization-state/execution-state/<canonical_id>/<file_name>_state_<request_id>.json. The execution state file provide verbose details about the request that is updated as the vectorization request is processed. This file records generated assets and logs. Error messages can be found in the log field. Vectorization pipeline state files The state of the vectorization pipeline is stored in the vectorization-state/pipeline-state folder. The naming convention is: vectorization-state/pipeline-state/<pipeline_name>/<pipeline_name>-<pipeline_execution_id>.json. The pipeline state records associated vectorization requests that are processed together in a single pipeline in the vectorization_requests field. The overall pipeline state is calculated based on the states of the collection of vectorization requests, this state is calculated by the following table in order: Condition Pipeline state At least one request is InProgress InProgress All requests are Completed Completed At least one request is Failed Failed All requests are New or there are no requests being tracked. New You can use the Management API with the object id of the request to retrieve the vectorization request resource that contains a high level overview of any errors that have occurred. If more detailed information is required, then reviewing the execution state file is recommended."
  },
  "docs/archive/setup-guides/vectorization/vectorization-profiles.html": {
    "href": "docs/archive/setup-guides/vectorization/vectorization-profiles.html",
    "title": "Managing vectorization profiles | FoundationaLLM",
    "summary": "Managing vectorization profiles The FoundationaLLM (FLLM) vectorization pipelines require the following types of profiles: Data Sources Text partitioning profiles Text embedding profiles Indexing profiles Data Sources Data sources are managed with the FoundationaLLM.DataSource resource provider through the Management API. The structure of a data source profile is the following: { \"type\": \"<data_source_type>\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.DataSource/dataSources/<name>\", \"display_name\": null, \"description\": \"<description>\", \"<settings>\": [ \"<value>\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <data_source_type> is the type of the data source. The supported types are AzureDataLake, SharePointOnline, WebSite and AzureSQLDatabase. <name> is the name of the data source. <instance_id> is the unique identifier of the FLLM instance. <description> is the description of the Data Source. <settings> is a JSON object containing the data source settings, the name of the property varies by data source type. <configuration_references> is a JSON object containing the profile configuration references. The content of this object also varies by data source type. The reminder of this section describes the configuration parameters for each of the supported Data Source types. AzureDataLake \"folders\": [ \"/vectorization-input/sdzwa/journals/2024\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\", \"APIKey\": \"FoundationaLLM:DataSources:datalake01:APIKey\", \"Endpoint\": \"FoundationaLLM:DataSources:datalake01:Endpoint\" }, The configuration parameters for AzureDataLake are the following: Parameter Description folders The list of folders in the Azure Data Lake storage account that contain the data to be vectorized. configuration_references.AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. configuration_references.ConnectionString The connection string to the Azure Storage account used for the the Azure Data Lake vectorization Data Source. SharePointOnline \"site_url\": \"https://solliance.sharepoint.com/sites/foundationallm01\", \"document_libraries\": [ \"/documents01\" ], \"configuration_references\": { \"ClientId\": \"FoundationaLLM:DataSources:sharepointsite01:ClientId\", \"TenantId\": \"FoundationaLLM:DataSources:sharepointsite01:TenantId\", \"CertificateName\": \"FoundationaLLM:DataSources:sharepointsite01:CertificateName\", \"KeyVaultURL\": \"FoundationaLLM:DataSources:sharepointsite01:KeyVaultURL\" }, The configuration parameters for SharePointOnline are the following: Parameter Description site_url The URL of the SharePoint Online site collection. document_libraries The list of document libraries in the SharePoint Online site collection that contain the data to be vectorized. configuration_references.CertificateName The name of the X.509 Certificate. The certificate must be valid and be uploaded into an Azure Key Vault certificate store. configuration_references.KeyVaultURL The URL of the KeyVault where the X.509 Certificate is stored. configuration_references.ClientId The Application (client) Id of the Microsoft Entra ID App Registration. See Entra ID app registration for SharePoint Online Data Source. configuration_references.TenantId The unique identifier of the SharePoint Online tenant. AzureSQLDatabase \"tables\": [ \"Table1\" ], \"configuration_references\": { \"ConnectionString\": \"FoundationaLLM:DataSources:sqldatabase01:ConnectionString\", \"AuthenticationType\": \"FoundationaLLM:DataSources:sqldatabase01:AuthenticationType\" }, The configuration parameters for AzureSQLDatabase are the following: Parameter Description tables The list of tables in the Azure SQL database that contain the data to be vectorized. configuration_references.ConnectionString The connection string to the Azure SQL database used for the Azure SQL Database vectorization Data Source. configuration_references.AuthenticationType The authentication type used to connect to the Azure SQL database. Can be one of AzureIdentity or ConnectionString. Managing Data Sources This section describes how to manage Data Sources using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name> Content-Type: application/json BODY <data source> where <data source> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name> Note The delete operation is a logical delete. To purge a Data Source, call the /purge endpoint after deleting the Data Source. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name>/purge Content-Type: application/json BODY {} Check Name HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/checkname Content-Type: application/json BODY { \"name\": \"<name>\" } Text partitioning profiles The structure of a text partitioning profile is the following: { \"type\": \"text-partitioning-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name>\", \"description\": \"<description>\", \"deleted\": false, \"text_splitter\": \"<text_splitter>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>} } where: <name> is the name of the text partitioning profile. <description> is the description of the text partitioning profile. <text_splitter> is the type of the text splitter. The supported types are TextTokenSplitter. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported text splitters. TextTokenSplitter \"settings\" : { \"Tokenizer\": \"MicrosoftBPETokenizer\", \"TokenizerEncoder\": \"cl100k_base\", \"ChunkSizeTokens\": \"2000\", \"OverlapSizeTokens\": \"200\" }, \"configuration_references\": {} The configuration parameters for TokenTextSplitter are the following: Parameter Description settings.Tokenizer The tokenizer used to split the text into tokens. Currently, the only supported tokenizer is MicrosoftBPETokenizer. Under the hood, it uses the .NET equivalent of OpenAI's tiktoken. settings.TokenizerEncoder The encoder used by the tokenizer. Currently, the only supported encoder is cl100k_base. This encoder is the one currently used by Azure OpenAI (and OpenAI) in gpt-3.5-turbo and gpt-4. settings.ChunkSizeTokens The maximum number of tokens in each text chunk. settings.OverlapSizeTokens The maximum number of tokens that overlap between two consecutive chunks. Managing text partitioning profiles This section describes how to manage text partitioning profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name> Content-Type: application/json BODY <text partitioning profile> where <text partitioning profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name> Note The delete operation is a logical delete. To purge a Text Partitioning Profile, call the /purge endpoint after deleting the Text Partitioning Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name>/purge Content-Type: application/json BODY {} Text embedding profiles The structure of a text embedding profile is the following: { \"type\": \"text-embedding-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name>\", \"display_name\": null, \"description\": \"<description>\", \"text_embedding\": \"<text_embedding>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <name> is the name of the text embedding profile. <description> is the description of the text embedding profile. <text_embedding> is the type of the text embedder. The supported types are SemanticKernelTextEmbedding and GatewayTextEmbedding. SemanticKernelTextEmbedding supports synchronous and asynchronous vectorization, while GatewayTextEmbedding only supports asynchronous vectorization. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported text embedders. SemanticKernelTextEmbedding \"settings\": {}, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey\", \"APIVersion\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIVersion\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType\", \"DeploymentName\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName\", \"Endpoint\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint\" } The configuration parameters for SemanticKernelTextEmbedding are the following: Parameter Description configuration_references.APIKey The API key used to connect to the Azure OpenAI service. By default, this maps to the Azure OpenAI service deployed by FLLM. configuration_references.APIVersion The API version used to connect to the Azure OpenAI service. By default, this value is 2023-05-15. configuration_references.AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. By default, it is set to APIKey. configuration_references.DeploymentName The name of the Azure OpenAI model deployment. The default value is embeddings. configuration_references.Endpoint The endpoint of the Azure OpenAI service. By default, this maps to the Azure OpenAI service deployed by FLLM. GatewayTextEmbedding \"settings\": { \"model_name\": \"embeddings\" }, \"configuration_references\": {} The settings for GatewayTextEmbedding are the following: Parameter Description settings.model_name The name of the embeddings model deployment in Azure OpenAI Service. Managing text embedding profiles This section describes how to manage text embedding profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name> Content-Type: application/json BODY <text embedding profile> where <text embedding profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name> Note The delete operation is a logical delete. To purge a Text Embedding Profile, call the /purge endpoint after deleting the Text Embedding Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name>/purge Content-Type: application/json BODY {} Indexing profiles The structure of an indexing profile is the following: { \"type\": \"indexing-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/indexingProfiles/<name>\", \"display_name\": null, \"description\": \"<description>\", \"deleted\": false, \"indexer\": \"<indexer>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <name> is the name of the indexing profile. <description> is the description of the indexing profile. <indexer> is the type of the indexer. The supported types are AzureAISearchIndexer. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported indexers. AzureAISearchIndexer \"settings\": { \"IndexName\": \"fllm-default-001\", \"TopN\": \"3\", \"Filters\": \"\", \"EmbeddingFieldName\": \"Embedding\", \"TextFieldName\": \"Text\" }, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:APIKey\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType\", \"Endpoint\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint\" } The configuration parameters for AzureAISearchIndexer are the following: Parameter Description settings.IndexName The name of the Azure AI Search index. If the index does not exist, it will be created. settings.TopN The number of embeddings closest to the index query to return. settings.Filters Optional filters to further refine the index search. settings.EmbeddingFieldName Field name of the embedding vector in the JSON documents returned by Azure AI Search. settings.TextFieldName Field name of the text equivalent of the embedding vector in the JSON documents returned by Azure AI Search. configuration_references.APIKey The API key used to connect to the Azure AI Search service. By default, this maps to the Azure AI Search service deployed by FLLM. ConfigurationReference.AuthenticationType The authentication type used to connect to the Azure AI Search service. Can be one of AzureIdentity or APIKey. By default, it is set to APIKey. configuration_references.Endpoint The endpoint of the Azure AI Search service. By default, this maps to the Azure AI Search service deployed by FLLM. Managing indexing profiles This section describes how to manage indexing profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name> Content-Type: application/json BODY <indexing profile> where <indexing profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name> Note The delete operation is a logical delete. To purge an Indexing Profile, call the /purge endpoint after deleting the Indexing Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name>/purge Content-Type: application/json BODY {} Check Name HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/checkname Content-Type: application/json BODY { \"name\": \"<name>\" } Additional configuration steps Entra ID app registration for SharePoint Online Data Source Apps typically access SharePoint Online through certificates: Anyone having the certificate and its private key can use the app with the permissions granted to it. Create a new App registration in your Microsoft Entra ID tenant. Next, provide a Name for your application and click on Register at the bottom of the blade. Navigate to the API Permissions blade and click on Add a permission button Here you choose the permissions that you will grant to this application. Select SharePoint from the Microsoft APIs tab, then select Application permissions as the type of permissions required, choose the desired permissions (i.e. Sites.Read.All) and click on Add permissions. Here are the required scopes: Group.ReadWrite.All User.ReadWrite.All Sites.Read.All OR Sites.Selected Sites.Read.All will allow the application to read documents and list items in all site collections. Sites.Selected will allow the application to access only a subset of site collections. The specific site collections and the permissions granted will be configured separately, in SharePoint Online. The application permission requires admin consent in a tenant before it can be used. In order to do this, click on API permissions in the left menu again. At the bottom you will see a section Grant consent. Click on the Grant admin consent for {{organization}} button and confirm the action by clicking on the Yes button that appears at the top. To invoke SharePoint Online with an app-only access token, you have to create and configure a self-signed X.509 certificate, which will be used to authenticate your application against Microsoft Entra ID. You can find additional details on how to do this in this document. Next step is to register the certificate you created to this application. Click on Certificates & secrets blade. Next, click on the Upload certificate button, select the .CER file you generated earlier and click on Add to upload it. To confirm that the certificate was successfully registered, click on Manifest blade and search for the keyCredentials property, which contains your certificate details. It should look like this: \"keyCredentials\": [ { \"customKeyIdentifier\": \"<$base64CertHash>\", \"endDate\": \"yyyy-MM-ddThh:mm:ssZ\", \"keyId\": \"<$guid>\", \"startDate\": \"yyyy-MM-ddThh:mm:ssZ\", \"type\": \"AsymmetricX509Cert\", \"usage\": \"Verify\", \"value\": \"<$base64Cert>\", \"displayName\": \"CN=<$name of your cert>\" } ] Upload and store the certificate in the KeyVault where the FoundationaLLM Vectorization API has permissions to read Secrets. You will need the Certificate Name for the App Configuration settings listed in the table above. NOTE Can I use other means besides certificates for realizing app-only access for my Azure AD app? NO, all other options are blocked by SharePoint Online and will result in an Access Denied message. Create a new Data Source using the Management API. Ensure that you set the necessary App Configuration settings appropriately."
  },
  "docs/archive/setup-guides/vectorization/vectorization-triggering.html": {
    "href": "docs/archive/setup-guides/vectorization/vectorization-triggering.html",
    "title": "Triggering vectorization | FoundationaLLM",
    "summary": "Triggering vectorization Vectorization pipelines are started when the Management API receives a processvectorization request. The following types of triggers are supported: None (no triggering of vectorization pipelines). Manual (vectorization pipelines are triggered manually by calling the Vectorization API). The typical use cases for on-demand vectorization (either synchronous or asynchronous) are testing, manual vectorization (or re-vectorization), and application integration (where another platform component triggers vectorization). Content-based (vectorization pipelines are triggered automatically when either new content is added to a content source or existing content is updated). Schedule-based (vectorization pipelines are triggered automatically based on a schedule). Note Content-based and schedule-based triggering are currently in pre-release and are not yet available in public releases of FLLM. Vectorization requests The typical structure of a vectorization request is the following { \"id\": \"fc4a2499-4771-4a2d-9ba0-3874c3d4586c\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationRequests/fc4a2499-4771-4a2d-9ba0-3874c3d4586c\", \"content_identifier\": { \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"multipart_id\": [ \"abcdefg.dfs.core.windows.net\", \"vectorization-input\", \"/journals/2024/Journal-January-2024.pdf\" ], \"canonical_id\": \"/journals/Journal-January-2024\", \"metadata\": null }, \"processing_type\": \"Synchronous\", \"steps\": [ { \"id\": \"extract\", \"parameters\": {} }, { \"id\": \"partition\", \"parameters\": { \"text_partitioning_profile_name\": \"DefaultTokenTextPartition_Small\" } }, { \"id\": \"embed\", \"parameters\": { \"text_embedding_profile_name\": \"AzureOpenAI_Embedding\" } }, { \"id\": \"index\", \"parameters\": { \"indexing_profile_name\": \"AzureAISearch_CPTEST\" } } ], \"completed_steps\": [ ], \"remaining_steps\": [ \"extract\", \"partition\", \"embed\", \"index\" ] } The following table describes the properties of a vectorization request. Property Description id The unique identifier of the vectorization request. The system issuing the request is responsible for generating this value. object_id The object ID of the vectorization request. This can be empty when creating the request. content_identifier The content identifier of the content to be vectorized. content_identifier.data_source_object_id The object id of the data source resource indicating the location of the data to be vectorized. content_identifier.multipart_id The multipart ID of the content to be vectorized. The multipart ID is a list of strings that uniquely identifies the content. The multipart ID is specific to the content source profile. content_identifier.canonical_id The canonical ID of the content to be vectorized. The canonical ID is a string that uniquely identifies the content in a logical namespace. The caller is responsible for the generation of this identifier. The identifier should have a path form (using the / separator). The last part of the path should always be equal to the file name (without its extension). processing_type The type of processing to be performed. The following values are supported: Synchronous and Asynchronous. See Vectorization concepts for more details. steps The vectorization steps to be executed. Most vectorization requests will contain the full set of standard steps: extract, partition, embed, and index. Each step (except for the extract one) will contain one parameter specifying the name of the associated vectorization profile name. completed_steps The list of steps that have been completed. This array needs to be empty when creating a vectorization request. remaining_steps The list of steps that are yet to be executed. This array can't be empty when creating a vectorization request, at least one step is needed. The meaning of the multipart strings depends on the specific type of the data source. The following table describes the meaning of the multipart strings for the AzureDataLake content source. Position Description 1 The URL of the Azure Data Lake storage account. When providing this value, you can use the known neutral URLs naming conventions. 2 The name of the container. 3 The path of the file relative to the container. The following table describes the meaning of the multipart strings for the SharePointOnline content source. Position Description 1 The URL of the SharePoint Online tenant. When providing this value, you can use the known neutral URLs naming conventions. 2 The path of the site/subsite relative to the tenant URL. 3 The folder path, starting with the document library name. 4 The name of the file. The following table describes the meaning of the multipart strings for the AzureSQLDatabase content source. Position Description 1 The name of the database schema. 2 The name of the table. 3 The name of the column that stores file content. 4 The name of the column that stores file row identifier. 5 The value of the row identifier. 5 The file name. The following table describes the meaning of the multipart strings for the WebSite content source. Position Description 1 The protocol, either http or https. 2 The web URL without the protocol. 3 CSS classes to filter by, space delimited. Known neutral URLs Depending on the specific configuration of various layers of security, vectorization request might end up being filtered out by infrastructure components like firewalls or proxies. To avoid this, the Vectorization API supports the use of known neutral URLs. Known neutral URLs are URLs that have a neutral form that is not subject to filtering. The platform currently supports two conventions for specifing known neutral URLs: Simple: xyz.blob.core.windows.net - this will be translated into https://xyz.blob.core.windows.net by the platform. Complex: FLLM:xyz#blob#core#windows#net - this will be translated into https://xyz.blob.core.windows.net by the platform. Depending on the specific configuration of the infrastructure, one of the two conventions might be more suitable than the other. Note To avoid bypassing security measures, the use of known neutral URLs is currently restricted to the following domains: onelake.dfs.fabric.microsoft.com, blob.core.windows.net, dfs.core.windows.net, and sharepoint.com. When parsing multipart components that are subject to known neutral URL naming conventions, the platform will apply the following logic: If the component starts with https:// or http:// (case-insensitive), the platform will not apply any transformation since the explicit intent is to use a fully qualified URL. If the component starts with FLLM:, the platform will replace the FLLM: prefix with https:// and replace all # characters with .. Then, it will check if the tail of the resulting URL is in the list of allowed domains. If it is, the platform will use the resulting URL. If it is not, the platform will use the original form of the component. At this point, the platform will assume that the component is a simple known neutral URL and will prepend https:// to it. Then, if the tail of the resulting URL is in the list of allowed domains, the platform will use the resulting URL. If it is not, the platform will use the original form of the component. Creating a vectorization request See Vectorization Request Resources for more information on creating a vectorization request. Upon completion, the API will return a response with the following structure: { \"object_id\":\"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationrequests/6041849a-d4d8-428d-97ff-c6a3443ecdae\", \"is_success\":true, \"error_message\":null } Processing a vectorization request To initiate the processing of a vectorization request, the caller must issue a process request to the Management API through a POST on the resource object id (the body is {}). The following is an example of a process, example: {baseUrl}/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationrequests/6041849a-d4d8-428d-97ff-c6a3443ecdae/process."
  },
  "docs/archive/vectorization/index.html": {
    "href": "docs/archive/vectorization/index.html",
    "title": "Vectorization | FoundationaLLM",
    "summary": "Vectorization This section describes the content vectorization concepts and how to configure vectorization in FoundationaLLM. Use the links below to learn more about configuring vectorization in FoundationaLLM: Vectorization concepts Configuring vectorization Managing vectorization profiles Triggering vectorization Monitoring and troubleshooting vectorization"
  },
  "docs/archive/vectorization/vectorization-concepts.html": {
    "href": "docs/archive/vectorization/vectorization-concepts.html",
    "title": "Vectorization Concepts | FoundationaLLM",
    "summary": "Vectorization Concepts FoundationaLLM (FLLM) provides utilities and services to support vectorizing your data in batch and on-demand modalities. Vectorization is a multi-step process, starting with loading your data, splitting (or chunking) the data as required, performing vector embeddings, and storing the vectors into a vector index so an agent can later retrieve relevant information through a vector search. In FLLM, vectorization is an idempotent operation, meaning that vectorizing the same document multiple times will result in the same vector being stored in the vector index. This is useful for re-vectorizing documents that have been updated, or for cases where only parts of the vectorization process need to be run (e.g., only the vector embeddings need to be updated). For each individual document, vectorization is performed by executing a vectorization request process action via the Management API upon receiving a valid vectorization request, either through a direct call or via a triggered vectorization pipeline. Based on the details of the vectorization request, a processing a vectorization request can be executed in one of the following modes: Synchonously - the vectorization steps start executing immediately and execute sequentially until the processing is completed or an error occurs. This type of execution is used for on-demand vectorization and is well suited for small to medium sized documents and relatively small numbers of documents at a time. Asynchronously - the vectorization steps are submitted to queues and executed by workers. This type of execution is used for batch vectorization and is well suited for large numbers of documents at a time. It is also well suited for vectorizing documents of any size. The FLLM platform components involved in vectorization are: Management API (creates vectorization requests and exposes the process action on vectorization request resources). Vectorization API (processes vectorization requests and executes synchonous vectorization pipelines). Note: The Vectorization API is used internally by the FoundationaLLM platform and is not intended to be used directly by users. Vectorization Worker(s) (execute asynchronous vectorization pipelines). A FLLM instance deploys one instance of the Vectorization API and one or more instances of the Vectorization Worker. See Configuring vectorization for more details on configuring these components. Note The initialization of both the Vectorization API and the Vectorization Worker is a time consuming process, as it involves dowloading and initializing various elements (e.g., Byte-Pair encoding dictionaries). As a result, after restarting the API, it might take up to a minute until it becomes ready to accept vectorization requests. It is recommended to either use the status endpoint of the Vectorization API to determine when it is ready to accept requests, or to wait for a minute after restarting the API before sending vectorization requests. Vectorization Pipelines Vectorization pipelines are aggregations of multiple vectorization requests, for example, a vectorization pipeline may be defined to vectorize all documents in a specific data source, such as an ADLS Gen2 container. Vectorization pipelines can be triggered in one of the following ways: None (no triggering of vectorization pipelines). Manual (vectorization pipelines are triggered manually by calling the Vectorization API). The typical use cases for on-demand vectorization (either synchronous or asynchronous) are testing, manual vectorization (or re-vectorization), and application integration (where another platform component triggers vectorization). Content-based (vectorization pipelines are triggered automatically when either new content is added to a content source or existing content is updated). Schedule-based (vectorization pipelines are triggered automatically based on a schedule). Note Content-based and schedule-based triggering are currently in pre-release and are not yet available in public releases of FLLM. When working with vectorization in FLLM, the typical steps you have to perform are: Ensure that the Management API, Vectorization API and Vectorization Worker are configured and running. This is a one-time operation. For more details, see Configuring vectorization. Create vectorization profiles. You can either reuse existing profiles or create new ones. For more details, see Managing vectorization profiles. Submit vectorization requests to the Management API. For more details, see Triggering vectorization."
  },
  "docs/archive/vectorization/vectorization-configuration.html": {
    "href": "docs/archive/vectorization/vectorization-configuration.html",
    "title": "Configuring vectorization | FoundationaLLM",
    "summary": "Configuring vectorization This section provides details on how to configure the vectorization API and workers in FoundationaLLM. Note These configurations should already be in place if you deployed FoundationaLLM (FLLM) using the recommended deployment scripts. The details presented here are provided for cases in which you need to troubleshoot or customize the configuration. Configuration for Vectorization API The following table describes the Azure artifacts required for the vectorization pipelines. Artifact name Description vectorization-input Azure storage container used by default to store documents to be picked up by the vectorization pipeline. Must be created on a Data Lake storage account (with the hierarchical namespace enabled). The following table describes the environment variables required for the vectorization pipelines. Environment variable Description FoundationaLLM_AppConfig_ConnectionString Connection string to the Azure App Configuration instance. The following table describes the required configuration parameters for the vectorization pipelines. App Configuration Key Default Value Description FoundationaLLM:APIs:VectorizationAPI:APIUrl The URL of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:APIKey Key Vault secret name: foundationallm-apis-vectorizationapi-apikey The API key of the vectorization API. FoundationaLLM:APIs:VectorizationAPI:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization API. Note Refer to the App Configuration values page for more information on how to set these and other configuration values. Configuration for Vectorization workers The following table describes the Azure artifacts required for the vectorization pipelines. Artifact Name Description embed Azure storage queue used for the embed vectorization pipeline. Can be created on the storage account used for the other queues. extract Azure storage queue used for the extract vectorization pipeline. Can be created on the storage account used for the other queues. index Azure storage queue used for the index vectorization pipeline. Can be created on the storage account used for the other queues. partition Azure storage queue used for the partition vectorization pipeline. Can be created on the storage account used for the other queues. vectorization-state Azure storage container used for the vectorization state service. Can be created on the storage account used for the other queues. resource-provider Azure storage container used for the internal states of the FoundationaLLM resource providers. resource-provider/FoundationaLLM.Vectorization/vectorization-pipelines.json Azure storage blob used for the vectorization pipeline resources managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization pipelines. resource-provider/FoundationaLLM.Vectorization/vectorization-text-partitioning-profiles.json Azure storage blob used for the text partitioning profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization text partitioning profiles. resource-provider/FoundationaLLM.Vectorization/vectorization-text-embedding-profiles.json Azure storage blob used for the text embedding profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization text embedding profiles. resource-provider/FoundationaLLM.Vectorization/vectorization-indexing-profiles.json Azure storage blob used for the indexing profiles managed by the FoundationaLLM.Vectorization resource provider. For more details, see vectorization indexing profiles. resource-provider/FoundationaLLM.DataSources Azure storage directory where the data sources managed by the FoundationaLLM.DataSources resource provider are stored. vectorization-state/requests/{requestid-yyyyMMdd}.json Azure storage directory where vectorization requests managed by the vectorization state service are stored. vectorization-state/execution-state/{canonical_id} Azure storage directory where the execution state of the vectorization requests and their resulting artifacts are stored. The canonical_id is defined in the vectorization request. vectorization-state/pipeline-state/{pipeline_name}/{pipeline_name}-{execution_id}.json Azure storage directory where the state of the vectorization pipeline execution is stored. The pipeline_name is the name of the vectorization pipeline and the execution_id is the unique identifier of the execution. The following table describes the environment variables required for the vectorization pipelines. Environment variable Description FoundationaLLM_AppConfig_ConnectionString Connection string to the Azure App Configuration instance. The following table describes the required App Configuration parameters for the vectorization pipelines. App Configuration Key Default Value Description FoundationaLLM:APIs:VectorizationWorker:APIUrl The URL of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:APIKey Key Vault secret name: foundationallm-apis-vectorizationworker-apikey The API key of the vectorization worker API. FoundationaLLM:APIs:VectorizationWorker:AppInsightsConnectionString Key Vault secret name: foundationallm-app-insights-connection-string The connection string to the Application Insights instance used by the vectorization worker API. FoundationaLLM:Vectorization:VectorizationWorker The settings used by each instance of the vectorization worker service. For more details, see default vectorization worker settings. FoundationaLLM:Vectorization:Queues:Embed:AccountName The account name of the Azure Storage account used for the embed vectorization queue. FoundationaLLM:Vectorization:Queues:Extract:AccountName The account name of the Azure Storage account used for the extract vectorization queue. FoundationaLLM:Vectorization:Queues:Index:AccountName The account name of the Azure Storage account used for the index vectorization queue. FoundationaLLM:Vectorization:Queues:Partition:AccountName The account name of the Azure Storage account used for the partition vectorization queue. FoundationaLLM:Vectorization:StateService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:ResourceProviderService:Storage:AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey Key Vault secret name: foundationallm-vectorization-semantickerneltextembedding-openai-apikey The API key used to connect to the Azure OpenAI service. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName The name of the Azure OpenAI model deployment. The default value is embeddings. FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint The endpoint of the Azure OpenAI service. FoundationaLLM:Vectorization:AzureAISearchIndexingService:APIKey Key Vault secret name: foundationallm-vectorization-azureaisearch-apikey The API key used to connect to the Azure OpenAI service. FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint The endpoint of the Azure OpenAI service. Note Refer to the App Configuration values page for more information on how to set these and other configuration values. The following table describes the external content used by the vectorization worker to initialize: Uri Description https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken The public Azure Blob Storage account used to download the OpenAI BPE ranking files. Note The vectorization worker must be able to open HTTPS connections to the external content listed above. Default vectorization worker settings The default settings for the vectorization worker are stored in the FoundationaLLM:Vectorization:VectorizationWorker App Configuration key. The default structure for this key is: { \"RequestManagers\": [ { \"RequestSourceName\": \"extract\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"partition\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"embed\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 }, { \"RequestSourceName\": \"index\", \"MaxHandlerInstances\": 1, \"QueueProcessingPace\": 5, \"QueuePollingInterval\": 60, \"QueueMaxNumberOfRetries\": 5 } ], \"RequestSources\": [ { \"Name\": \"extract\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"partition\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"embed\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 }, { \"Name\": \"index\", \"AccountName\": \"{{accountName}}\", \"VisibilityTimeoutSeconds\": 600 } ], \"QueuingEngine\": \"AzureStorageQueue\" } The following table provides details about the configuration parameters: Parameter Description RequestManagers The list of request managers used by the vectorization worker. Each request manager is responsible for managing the execution of vectorization pipelines for a specific vectorization step. The configuration must include all request managers. RequestManagers.MaxHandlerInstances The maximum number of request handlers that process requests for the specified request source. By default, the value is 1. You can change the value to increase the processing capacity of each vectorization worker instance. The value applies to all istances of the vectorization worker. NOTE: It is important to align the value of this setting with the level of compute and memory resources allocated to the individual vectorization worker instances. RequestManagers.QueueProcessingPace Optional The delay in seconds to wait between requests after a request has been processed. The default value is 5. RequestManagers.QueuePollingInterval Optional The polling interval in seconds, this is the amount of time to wait if the previous check on the queue had no items. The default value is 60. RequestManagers.QueueMaxNumberOfRetries Optional The maximum number of retries to attempt to process a request before being removed from the queue. The default value is 5. RequestSources The list of request sources used by the vectorization worker. Each request source is responsible for managing the requests for a specific vectorization step. The configuration must include all request sources. RequestSources.Name The name of the request source. The name must match the name of the request manager. RequestSources.AccountName The name of the configuration key for the Azure Storage account used for the queue (include the tokens after FoundationaLLM:Vectorization:Queues:). RequestSources.VisibilityTimeoutSeconds In the case of queue-based request sources (the default for the vectorization worker), specifies the time in seconds until a dequeued vectorization step request must be executed. During this timeout, the message will not be visible to other handler instances within the same worker or from other worker instances. If the handler fails to process the vectorization step request successfully and remove it from the queue within the specified timeout, the message will become visibile again. The default value is 600 seconds and should not be changed."
  },
  "docs/archive/vectorization/vectorization-monitoring-troubleshooting.html": {
    "href": "docs/archive/vectorization/vectorization-monitoring-troubleshooting.html",
    "title": "Monitoring and troubleshooting vectorization | FoundationaLLM",
    "summary": "Monitoring and troubleshooting vectorization The typical steps you have to perform when monitoring and troubleshooting vectorization in FoundationaLLM (FLLM) are: Check the configuration of the Vectorization API and Vectorization Worker. For more details, see Configuring vectorization. Check the working condition of the Management API, Vectorization API and Vectorization Worker(s). Ensure the services have started and initialized successfully. Check the status endpoints for the Core API, Vectorization API and the Management API. You can do this by submitting a HTTP GET request to the /status endpoint of these APIs and validate that you get a HTTP 200 OK response with body like <api_name> - ready. Check the logs of the Management Vectorization API and Vectorization Worker(s) for errors. By default, the logs are written to the Azure App Insights Log Analytics Workspace deployed by FLLM. Check the definitions of the vectorization profiles used in the vectorization requests. For more details, see Managing vectorization profiles. Ensure all the required app configuration elements are present and have the correct values. Check the state of the vectorization requests. By default, the vectorization requests are stored in the vectorization-state container of the Azure Storage account deployed by FLLM. State and logging of vectorization requests All state and logging of vectorization requests are stored in the vectorization-state container of the Azure Storage account deployed by FLLM. Vectorization request resource files Each vectorization request resource is stored in the vectorization-state/requests folder. The request resources are created and managed through the Management API. The naming convention is: vectorization-state/requests/<request_id>-<yyyMMdd>.json. The resource file is updated as the vectorization request progresses through the processing. The resource file contains the following fields that can assist in troubleshooting: Field Description id The unique identifier of the vectorization request. When looking up the subsequent execution state, this is the identifier that is used in the file name. content-identifier.canonical_id The canonical id of the vectorization request. This is the path within the execution-state folder where additional logs and associated vectorization artifacts are stored. processing-state The current state of the vectorization request, values can be New, InProgress, Completed, Failed error_messages A high level list of error messages encountered during processing. current_step The step currently being executed, or the step in which a failure occurred. pipeline_object_id When created through a vectorization pipeline, this field contains the object id of the pipeline. pipeline_execution_id When process is initiated through a vectorization pipeline, this field contains the unique identifier of the pipeline execution. Vectorization execution state files The execution state of a vectorization request is stored in the vectorization-state/execution-state folder. The naming convention is: vectorization-state/execution-state/<canonical_id>/<file_name>_state_<request_id>.json. The execution state file provide verbose details about the request that is updated as the vectorization request is processed. This file records generated assets and logs. Error messages can be found in the log field. Vectorization pipeline state files The state of the vectorization pipeline is stored in the vectorization-state/pipeline-state folder. The naming convention is: vectorization-state/pipeline-state/<pipeline_name>/<pipeline_name>-<pipeline_execution_id>.json. The pipeline state records associated vectorization requests that are processed together in a single pipeline in the vectorization_requests field. The overall pipeline state is calculated based on the states of the collection of vectorization requests, this state is calculated by the following table in order: Condition Pipeline state At least one request is InProgress InProgress All requests are Completed Completed At least one request is Failed Failed All requests are New or there are no requests being tracked. New You can use the Management API with the object id of the request to retrieve the vectorization request resource that contains a high level overview of any errors that have occurred. If more detailed information is required, then reviewing the execution state file is recommended."
  },
  "docs/archive/vectorization/vectorization-profiles.html": {
    "href": "docs/archive/vectorization/vectorization-profiles.html",
    "title": "Managing vectorization profiles | FoundationaLLM",
    "summary": "Managing vectorization profiles The FoundationaLLM (FLLM) vectorization pipelines require the following types of profiles: Data Sources Text partitioning profiles Text embedding profiles Indexing profiles Data Sources Data sources are managed with the FoundationaLLM.DataSource resource provider through the Management API. The structure of a data source profile is the following: { \"type\": \"<data_source_type>\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.DataSource/dataSources/<name>\", \"display_name\": null, \"description\": \"<description>\", \"<settings>\": [ \"<value>\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <data_source_type> is the type of the data source. The supported types are AzureDataLake, SharePointOnline, WebSite and AzureSQLDatabase. <name> is the name of the data source. <instance_id> is the unique identifier of the FLLM instance. <description> is the description of the Data Source. <settings> is a JSON object containing the data source settings, the name of the property varies by data source type. <configuration_references> is a JSON object containing the profile configuration references. The content of this object also varies by data source type. The reminder of this section describes the configuration parameters for each of the supported Data Source types. AzureDataLake \"folders\": [ \"/vectorization-input/sdzwa/journals/2024\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\", \"APIKey\": \"FoundationaLLM:DataSources:datalake01:APIKey\", \"Endpoint\": \"FoundationaLLM:DataSources:datalake01:Endpoint\" }, The configuration parameters for AzureDataLake are the following: Parameter Description folders The list of folders in the Azure Data Lake storage account that contain the data to be vectorized. configuration_references.AuthenticationType The authentication type used to connect to the underlying storage. Can be one of AzureIdentity, AccountKey, or ConnectionString. configuration_references.ConnectionString The connection string to the Azure Storage account used for the the Azure Data Lake vectorization Data Source. SharePointOnline \"site_url\": \"https://solliance.sharepoint.com/sites/foundationallm01\", \"document_libraries\": [ \"/documents01\" ], \"configuration_references\": { \"ClientId\": \"FoundationaLLM:DataSources:sharepointsite01:ClientId\", \"TenantId\": \"FoundationaLLM:DataSources:sharepointsite01:TenantId\", \"CertificateName\": \"FoundationaLLM:DataSources:sharepointsite01:CertificateName\", \"KeyVaultURL\": \"FoundationaLLM:DataSources:sharepointsite01:KeyVaultURL\" }, The configuration parameters for SharePointOnline are the following: Parameter Description site_url The URL of the SharePoint Online site collection. document_libraries The list of document libraries in the SharePoint Online site collection that contain the data to be vectorized. configuration_references.CertificateName The name of the X.509 Certificate. The certificate must be valid and be uploaded into an Azure Key Vault certificate store. configuration_references.KeyVaultURL The URL of the KeyVault where the X.509 Certificate is stored. configuration_references.ClientId The Application (client) Id of the Microsoft Entra ID App Registration. See Entra ID app registration for SharePoint Online Data Source. configuration_references.TenantId The unique identifier of the SharePoint Online tenant. AzureSQLDatabase \"tables\": [ \"Table1\" ], \"configuration_references\": { \"ConnectionString\": \"FoundationaLLM:DataSources:sqldatabase01:ConnectionString\", \"AuthenticationType\": \"FoundationaLLM:DataSources:sqldatabase01:AuthenticationType\" }, The configuration parameters for AzureSQLDatabase are the following: Parameter Description tables The list of tables in the Azure SQL database that contain the data to be vectorized. configuration_references.ConnectionString The connection string to the Azure SQL database used for the Azure SQL Database vectorization Data Source. configuration_references.AuthenticationType The authentication type used to connect to the Azure SQL database. Can be one of AzureIdentity or ConnectionString. Managing Data Sources This section describes how to manage Data Sources using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name> Content-Type: application/json BODY <data source> where <data source> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name> Note The delete operation is a logical delete. To purge a Data Source, call the /purge endpoint after deleting the Data Source. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/<name>/purge Content-Type: application/json BODY {} Check Name HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/checkname Content-Type: application/json BODY { \"name\": \"<name>\" } Text partitioning profiles The structure of a text partitioning profile is the following: { \"type\": \"text-partitioning-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name>\", \"description\": \"<description>\", \"deleted\": false, \"text_splitter\": \"<text_splitter>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>} } where: <name> is the name of the text partitioning profile. <description> is the description of the text partitioning profile. <text_splitter> is the type of the text splitter. The supported types are TextTokenSplitter. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported text splitters. TextTokenSplitter \"settings\" : { \"Tokenizer\": \"MicrosoftBPETokenizer\", \"TokenizerEncoder\": \"cl100k_base\", \"ChunkSizeTokens\": \"2000\", \"OverlapSizeTokens\": \"200\" }, \"configuration_references\": {} The configuration parameters for TokenTextSplitter are the following: Parameter Description settings.Tokenizer The tokenizer used to split the text into tokens. Currently, the only supported tokenizer is MicrosoftBPETokenizer. Under the hood, it uses the .NET equivalent of OpenAI's tiktoken. settings.TokenizerEncoder The encoder used by the tokenizer. Currently, the only supported encoder is cl100k_base. This encoder is the one currently used by Azure OpenAI (and OpenAI) in gpt-3.5-turbo and gpt-4. settings.ChunkSizeTokens The maximum number of tokens in each text chunk. settings.OverlapSizeTokens The maximum number of tokens that overlap between two consecutive chunks. Managing text partitioning profiles This section describes how to manage text partitioning profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name> Content-Type: application/json BODY <text partitioning profile> where <text partitioning profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name> Note The delete operation is a logical delete. To purge a Text Partitioning Profile, call the /purge endpoint after deleting the Text Partitioning Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/<name>/purge Content-Type: application/json BODY {} Text embedding profiles The structure of a text embedding profile is the following: { \"type\": \"text-embedding-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name>\", \"display_name\": null, \"description\": \"<description>\", \"text_embedding\": \"<text_embedding>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <name> is the name of the text embedding profile. <description> is the description of the text embedding profile. <text_embedding> is the type of the text embedder. The supported types are SemanticKernelTextEmbedding and GatewayTextEmbedding. SemanticKernelTextEmbedding supports synchronous and asynchronous vectorization, while GatewayTextEmbedding only supports asynchronous vectorization. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported text embedders. SemanticKernelTextEmbedding \"settings\": {}, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey\", \"APIVersion\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIVersion\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType\", \"DeploymentName\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName\", \"Endpoint\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint\" } The configuration parameters for SemanticKernelTextEmbedding are the following: Parameter Description configuration_references.APIKey The API key used to connect to the Azure OpenAI service. By default, this maps to the Azure OpenAI service deployed by FLLM. configuration_references.APIVersion The API version used to connect to the Azure OpenAI service. By default, this value is 2023-05-15. configuration_references.AuthenticationType The authentication type used to connect to the Azure OpenAI service. Can be one of AzureIdentity or APIKey. By default, it is set to APIKey. configuration_references.DeploymentName The name of the Azure OpenAI model deployment. The default value is embeddings. configuration_references.Endpoint The endpoint of the Azure OpenAI service. By default, this maps to the Azure OpenAI service deployed by FLLM. GatewayTextEmbedding \"settings\": { \"model_name\": \"embeddings\" }, \"configuration_references\": {} The settings for GatewayTextEmbedding are the following: Parameter Description settings.model_name The name of the embeddings model deployment in Azure OpenAI Service. Managing text embedding profiles This section describes how to manage text embedding profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name> Content-Type: application/json BODY <text embedding profile> where <text embedding profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name> Note The delete operation is a logical delete. To purge a Text Embedding Profile, call the /purge endpoint after deleting the Text Embedding Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/<name>/purge Content-Type: application/json BODY {} Indexing profiles The structure of an indexing profile is the following: { \"type\": \"indexing-profile\", \"name\": \"<name>\", \"object_id\": \"/instances/[INSTANCE ID]/providers/FoundationaLLM.Vectorization/indexingProfiles/<name>\", \"display_name\": null, \"description\": \"<description>\", \"deleted\": false, \"indexer\": \"<indexer>\", \"settings\": {<profile_settings>}, \"configuration_references\": {<profile_configuration_references>}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } where: <name> is the name of the indexing profile. <description> is the description of the indexing profile. <indexer> is the type of the indexer. The supported types are AzureAISearchIndexer. <profile_settings> is a JSON object containing the profile settings. <profile_configuration_references> is a JSON object containing the profile configuration references. The reminder of this section describes the configuration parameters for each of the supported indexers. AzureAISearchIndexer \"settings\": { \"IndexName\": \"fllm-default-001\", \"TopN\": \"3\", \"Filters\": \"\", \"EmbeddingFieldName\": \"Embedding\", \"TextFieldName\": \"Text\" }, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:APIKey\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType\", \"Endpoint\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint\" } The configuration parameters for AzureAISearchIndexer are the following: Parameter Description settings.IndexName The name of the Azure AI Search index. If the index does not exist, it will be created. settings.TopN The number of embeddings closest to the index query to return. settings.Filters Optional filters to further refine the index search. settings.EmbeddingFieldName Field name of the embedding vector in the JSON documents returned by Azure AI Search. settings.TextFieldName Field name of the text equivalent of the embedding vector in the JSON documents returned by Azure AI Search. configuration_references.APIKey The API key used to connect to the Azure AI Search service. By default, this maps to the Azure AI Search service deployed by FLLM. ConfigurationReference.AuthenticationType The authentication type used to connect to the Azure AI Search service. Can be one of AzureIdentity or APIKey. By default, it is set to APIKey. configuration_references.Endpoint The endpoint of the Azure AI Search service. By default, this maps to the Azure AI Search service deployed by FLLM. Managing indexing profiles This section describes how to manage indexing profiles using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name> Content-Type: application/json BODY <indexing profile> where <indexing profile> is a JSON object with the structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name> Note The delete operation is a logical delete. To purge an Indexing Profile, call the /purge endpoint after deleting the Indexing Profile. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/<name>/purge Content-Type: application/json BODY {} Check Name HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Vectorization/indexingProfiles/checkname Content-Type: application/json BODY { \"name\": \"<name>\" } Additional configuration steps Entra ID app registration for SharePoint Online Data Source Apps typically access SharePoint Online through certificates: Anyone having the certificate and its private key can use the app with the permissions granted to it. Create a new App registration in your Microsoft Entra ID tenant. Next, provide a Name for your application and click on Register at the bottom of the blade. Navigate to the API Permissions blade and click on Add a permission button Here you choose the permissions that you will grant to this application. Select SharePoint from the Microsoft APIs tab, then select Application permissions as the type of permissions required, choose the desired permissions (i.e. Sites.Read.All) and click on Add permissions. Here are the required scopes: Group.ReadWrite.All User.ReadWrite.All Sites.Read.All OR Sites.Selected Sites.Read.All will allow the application to read documents and list items in all site collections. Sites.Selected will allow the application to access only a subset of site collections. The specific site collections and the permissions granted will be configured separately, in SharePoint Online. The application permission requires admin consent in a tenant before it can be used. In order to do this, click on API permissions in the left menu again. At the bottom you will see a section Grant consent. Click on the Grant admin consent for {{organization}} button and confirm the action by clicking on the Yes button that appears at the top. To invoke SharePoint Online with an app-only access token, you have to create and configure a self-signed X.509 certificate, which will be used to authenticate your application against Microsoft Entra ID. You can find additional details on how to do this in this document. Next step is to register the certificate you created to this application. Click on Certificates & secrets blade. Next, click on the Upload certificate button, select the .CER file you generated earlier and click on Add to upload it. To confirm that the certificate was successfully registered, click on Manifest blade and search for the keyCredentials property, which contains your certificate details. It should look like this: \"keyCredentials\": [ { \"customKeyIdentifier\": \"<$base64CertHash>\", \"endDate\": \"yyyy-MM-ddThh:mm:ssZ\", \"keyId\": \"<$guid>\", \"startDate\": \"yyyy-MM-ddThh:mm:ssZ\", \"type\": \"AsymmetricX509Cert\", \"usage\": \"Verify\", \"value\": \"<$base64Cert>\", \"displayName\": \"CN=<$name of your cert>\" } ] Upload and store the certificate in the KeyVault where the FoundationaLLM Vectorization API has permissions to read Secrets. You will need the Certificate Name for the App Configuration settings listed in the table above. NOTE Can I use other means besides certificates for realizing app-only access for my Azure AD app? NO, all other options are blocked by SharePoint Online and will result in an Access Denied message. Create a new Data Source using the Management API. Ensure that you set the necessary App Configuration settings appropriately."
  },
  "docs/archive/vectorization/vectorization-triggering.html": {
    "href": "docs/archive/vectorization/vectorization-triggering.html",
    "title": "Triggering vectorization | FoundationaLLM",
    "summary": "Triggering vectorization Vectorization pipelines are started when the Management API receives a processvectorization request. The following types of triggers are supported: None (no triggering of vectorization pipelines). Manual (vectorization pipelines are triggered manually by calling the Vectorization API). The typical use cases for on-demand vectorization (either synchronous or asynchronous) are testing, manual vectorization (or re-vectorization), and application integration (where another platform component triggers vectorization). Content-based (vectorization pipelines are triggered automatically when either new content is added to a content source or existing content is updated). Schedule-based (vectorization pipelines are triggered automatically based on a schedule). Note Content-based and schedule-based triggering are currently in pre-release and are not yet available in public releases of FLLM. Vectorization requests The typical structure of a vectorization request is the following { \"id\": \"fc4a2499-4771-4a2d-9ba0-3874c3d4586c\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationRequests/fc4a2499-4771-4a2d-9ba0-3874c3d4586c\", \"content_identifier\": { \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"multipart_id\": [ \"abcdefg.dfs.core.windows.net\", \"vectorization-input\", \"/journals/2024/Journal-January-2024.pdf\" ], \"canonical_id\": \"/journals/Journal-January-2024\", \"metadata\": null }, \"processing_type\": \"Synchronous\", \"steps\": [ { \"id\": \"extract\", \"parameters\": {} }, { \"id\": \"partition\", \"parameters\": { \"text_partitioning_profile_name\": \"DefaultTokenTextPartition_Small\" } }, { \"id\": \"embed\", \"parameters\": { \"text_embedding_profile_name\": \"AzureOpenAI_Embedding\" } }, { \"id\": \"index\", \"parameters\": { \"indexing_profile_name\": \"AzureAISearch_CPTEST\" } } ], \"completed_steps\": [ ], \"remaining_steps\": [ \"extract\", \"partition\", \"embed\", \"index\" ] } The following table describes the properties of a vectorization request. Property Description id The unique identifier of the vectorization request. The system issuing the request is responsible for generating this value. object_id The object ID of the vectorization request. This can be empty when creating the request. content_identifier The content identifier of the content to be vectorized. content_identifier.data_source_object_id The object id of the data source resource indicating the location of the data to be vectorized. content_identifier.multipart_id The multipart ID of the content to be vectorized. The multipart ID is a list of strings that uniquely identifies the content. The multipart ID is specific to the content source profile. content_identifier.canonical_id The canonical ID of the content to be vectorized. The canonical ID is a string that uniquely identifies the content in a logical namespace. The caller is responsible for the generation of this identifier. The identifier should have a path form (using the / separator). The last part of the path should always be equal to the file name (without its extension). processing_type The type of processing to be performed. The following values are supported: Synchronous and Asynchronous. See Vectorization concepts for more details. steps The vectorization steps to be executed. Most vectorization requests will contain the full set of standard steps: extract, partition, embed, and index. Each step (except for the extract one) will contain one parameter specifying the name of the associated vectorization profile name. completed_steps The list of steps that have been completed. This array needs to be empty when creating a vectorization request. remaining_steps The list of steps that are yet to be executed. This array can't be empty when creating a vectorization request, at least one step is needed. The meaning of the multipart strings depends on the specific type of the data source. The following table describes the meaning of the multipart strings for the AzureDataLake content source. Position Description 1 The URL of the Azure Data Lake storage account. When providing this value, you can use the known neutral URLs naming conventions. 2 The name of the container. 3 The path of the file relative to the container. The following table describes the meaning of the multipart strings for the SharePointOnline content source. Position Description 1 The URL of the SharePoint Online tenant. When providing this value, you can use the known neutral URLs naming conventions. 2 The path of the site/subsite relative to the tenant URL. 3 The folder path, starting with the document library name. 4 The name of the file. The following table describes the meaning of the multipart strings for the AzureSQLDatabase content source. Position Description 1 The name of the database schema. 2 The name of the table. 3 The name of the column that stores file content. 4 The name of the column that stores file row identifier. 5 The value of the row identifier. 5 The file name. The following table describes the meaning of the multipart strings for the WebSite content source. Position Description 1 The protocol, either http or https. 2 The web URL without the protocol. 3 CSS classes to filter by, space delimited. Known neutral URLs Depending on the specific configuration of various layers of security, vectorization request might end up being filtered out by infrastructure components like firewalls or proxies. To avoid this, the Vectorization API supports the use of known neutral URLs. Known neutral URLs are URLs that have a neutral form that is not subject to filtering. The platform currently supports two conventions for specifing known neutral URLs: Simple: xyz.blob.core.windows.net - this will be translated into https://xyz.blob.core.windows.net by the platform. Complex: FLLM:xyz#blob#core#windows#net - this will be translated into https://xyz.blob.core.windows.net by the platform. Depending on the specific configuration of the infrastructure, one of the two conventions might be more suitable than the other. Note To avoid bypassing security measures, the use of known neutral URLs is currently restricted to the following domains: onelake.dfs.fabric.microsoft.com, blob.core.windows.net, dfs.core.windows.net, and sharepoint.com. When parsing multipart components that are subject to known neutral URL naming conventions, the platform will apply the following logic: If the component starts with https:// or http:// (case-insensitive), the platform will not apply any transformation since the explicit intent is to use a fully qualified URL. If the component starts with FLLM:, the platform will replace the FLLM: prefix with https:// and replace all # characters with .. Then, it will check if the tail of the resulting URL is in the list of allowed domains. If it is, the platform will use the resulting URL. If it is not, the platform will use the original form of the component. At this point, the platform will assume that the component is a simple known neutral URL and will prepend https:// to it. Then, if the tail of the resulting URL is in the list of allowed domains, the platform will use the resulting URL. If it is not, the platform will use the original form of the component. Creating a vectorization request See Vectorization Request Resources for more information on creating a vectorization request. Upon completion, the API will return a response with the following structure: { \"object_id\":\"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationrequests/6041849a-d4d8-428d-97ff-c6a3443ecdae\", \"is_success\":true, \"error_message\":null } Processing a vectorization request To initiate the processing of a vectorization request, the caller must issue a process request to the Management API through a POST on the resource object id (the body is {}). The following is an example of a process, example: {baseUrl}/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationrequests/6041849a-d4d8-428d-97ff-c6a3443ecdae/process."
  },
  "docs/chat-user-portal/how-to-guides/using-agents/configuring-accessibility.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/configuring-accessibility.html",
    "title": "Configuring Accessibility Settings | FoundationaLLM",
    "summary": "Configuring Accessibility Settings Customize the Chat User Portal to meet your visual and notification preferences. FoundationaLLM is designed with accessibility in mind, supporting WCAG (Web Content Accessibility Guidelines) compliance. WCAG Compliance Overview The Chat User Portal is built following WCAG 2.1 Level AA guidelines to ensure accessibility for users with diverse abilities: WCAG Principle Implementation Perceivable Text alternatives, adaptable content, distinguishable elements Operable Keyboard accessible, sufficient time, navigable Understandable Readable, predictable, input assistance Robust Compatible with assistive technologies Key Accessibility Features Feature Description Keyboard Navigation Full functionality without a mouse Screen Reader Support ARIA labels and semantic HTML Text Scaling Adjustable text size up to 200% Color Independence Information not conveyed by color alone Focus Indicators Visible focus states on interactive elements Error Identification Clear error messages and guidance Accessing Accessibility Settings Click the Settings button (gear icon ⚙️) in the bottom-left corner of the sidebar, next to your name In the Settings dialog, click the Accessibility tab Adjust settings as needed Click Close when finished — changes are applied immediately Available Settings Auto-Hide Popup Notifications Control whether notification messages (toasts) disappear automatically or stay visible until you dismiss them. Setting Behavior On (default) Notifications fade away after a few seconds Off Notifications stay visible until you click to dismiss them When to disable auto-hide: If you need more time to read notifications If you use a screen reader and want notifications to remain accessible If you frequently miss important status messages How to change: Find the Auto hide popup notifications toggle Click the toggle to switch between On and Off Text Size Adjust the overall text size throughout the portal to improve readability. Size Best For 80% Seeing more content on screen 100% (default) Standard viewing 150% Easier reading, larger text How to change: Find the Text size slider Drag the slider left to decrease or right to increase The percentage displays next to the slider (e.g., \"120%\") Text updates immediately as you adjust Tip: You can also use your browser's built-in zoom (Ctrl + / Ctrl - on Windows, Cmd + / Cmd - on Mac) for additional size adjustments. Keyboard Navigation The Chat User Portal supports full keyboard navigation: General Navigation Action Keys Move between elements Tab / Shift + Tab Activate buttons Enter or Space Close dialogs/popups Escape Submit messages Enter Add new line in message Shift + Enter Sidebar Shortcuts Action Keys Navigate to chat Tab to chat, then Enter Delete a chat Select chat, then Delete or Backspace Close sidebar Tab to toggle button, then Enter Message Area Action Keys Navigate between messages Tab through message elements Copy message Tab to Copy button, then Enter Rate a response Tab to Rate button, then Enter Screen Reader Support The portal includes accessibility features for screen readers: ARIA labels: Interactive elements have descriptive labels Role attributes: Elements are properly identified (buttons, dialogs, lists) Live regions: Status changes and notifications are announced Focus management: Focus moves logically through the interface Recommended Screen Readers Platform Recommended Windows NVDA, JAWS Mac VoiceOver Linux Orca Visual Accessibility Tips Using Browser Features Your browser offers additional accessibility options: Feature How to Access Zoom Ctrl + / Ctrl - (Windows) or Cmd + / Cmd - (Mac) High contrast mode Enable in Windows Settings > Accessibility Dark mode Some browsers apply system dark mode automatically Custom fonts Configure in browser accessibility settings Display Recommendations For the best experience: Use a screen resolution of at least 1280×720 Ensure sufficient contrast between text and background Position your monitor to reduce glare Take regular breaks when reading long conversations Mobile Accessibility On mobile devices: Use your device's built-in accessibility features (VoiceOver on iOS, TalkBack on Android) Enable larger text in your device's Display settings The interface automatically adjusts for smaller screens Touch targets are sized for easy tapping Troubleshooting Text Size Not Changing Try refreshing the page (F5 or Ctrl + R) Check that JavaScript is enabled in your browser Clear your browser cache and try again Screen Reader Not Reading Elements Ensure your screen reader is in \"forms mode\" or \"focus mode\" when interacting with inputs Try a different browser — Chrome and Firefox have the best screen reader support Update your screen reader to the latest version Keyboard Focus Not Visible Check your browser's accessibility settings for \"Show focus indicators\" Some browser extensions can interfere with focus styles Try disabling browser extensions temporarily Settings Not Saving Accessibility settings are saved in your browser's session storage. They persist as long as: You stay logged in You don't clear browser data You use the same browser If settings reset unexpectedly: Check that cookies and site data aren't being blocked Avoid using \"private\" or \"incognito\" mode for persistent settings WCAG 2.1 Feature Details Perceivable Content Text Alternatives (WCAG 1.1) All images have alt text descriptions Icons are accompanied by text labels or ARIA labels Non-text content is described for screen readers Adaptable Content (WCAG 1.3) Content is structured with proper headings Form elements have associated labels Reading order is logical and consistent Distinguishable Content (WCAG 1.4) Text can be resized up to 200% without loss of functionality Color contrast meets 4.5:1 ratio for normal text Color is not the only means of conveying information Content reflows for different screen sizes Operable Interface Keyboard Accessible (WCAG 2.1) All functionality available via keyboard No keyboard traps Skip navigation option for repetitive content Enough Time (WCAG 2.2) Session timeouts provide warning and extension options Auto-hide notifications can be disabled No time-limited interactions required Seizure Prevention (WCAG 2.3) No flashing content above 3 flashes per second Animations respect reduced-motion preferences Navigable (WCAG 2.4) Page titles are descriptive Focus order is logical Link purpose is clear from context Multiple ways to find content Understandable Content Readable (WCAG 3.1) Page language is identified Technical terms are explained Plain language is used where possible Predictable (WCAG 3.2) Navigation is consistent across pages Components behave predictably Changes don't occur unexpectedly Input Assistance (WCAG 3.3) Errors are clearly identified Labels and instructions are provided Error suggestions help users correct mistakes Robust Technology Compatible (WCAG 4.1) Valid HTML markup ARIA properly implemented Status messages announced to screen readers Compatible with current assistive technologies Color Contrast The User Portal maintains accessible color contrast ratios: Element Contrast Ratio WCAG Requirement Body text 4.5:1 or higher Level AA Large text 3:1 or higher Level AA UI components 3:1 or higher Level AA Focus indicators 3:1 or higher Level AA Checking Contrast If you have difficulty distinguishing colors: Enable your operating system's high contrast mode Use browser extensions like \"High Contrast\" for Chrome Adjust display settings in your system preferences Reduced Motion For users sensitive to motion: System Preferences The portal respects your operating system's reduced motion settings: Windows: Settings > Accessibility > Visual effects > Show animations macOS: System Preferences > Accessibility > Display > Reduce motion iOS: Settings > Accessibility > Motion > Reduce Motion Android: Settings > Accessibility > Remove animations What Changes with Reduced Motion Normal Behavior With Reduced Motion Animated transitions Instant transitions Sliding panels Instant show/hide Loading spinners Static indicators Smooth scrolling Instant scrolling Assistive Technology Compatibility Tested Screen Readers Screen Reader Browser Support Level NVDA Firefox, Chrome Full JAWS Chrome, Edge Full VoiceOver Safari, Chrome Full Narrator Edge Full Orca Firefox Basic Tips for Screen Reader Users Use Browse Mode to read conversation content Use Forms Mode when typing in the message input Listen for ARIA live regions announcing new messages Use heading navigation (H key) to jump between sections Voice Control For voice control users (Dragon NaturallySpeaking, Voice Control): Buttons have visible text labels Form fields have accessible names Links are descriptively named \"Click [element text]\" commands work reliably Cognitive Accessibility Simple Language Error messages explain what went wrong and how to fix it Instructions use clear, concise language Complex terms include explanations Consistent Design Navigation stays in the same location Similar functions use similar designs Buttons and controls are predictably placed Memory Support Conversation history is preserved Settings persist across sessions Important information isn't hidden Reporting Accessibility Issues If you encounter accessibility barriers: Note the specific issue and how it affects you Include the browser and assistive technology you're using Contact your organization's IT support Request that they report the issue to FoundationaLLM Related Topics Managing Conversations — Navigate and organize your chats Printing Conversations — Create accessible printed copies Viewing Status Messages — Accessible status notifications"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/copying-prompts-results.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/copying-prompts-results.html",
    "title": "Copying Messages from Conversations | FoundationaLLM",
    "summary": "Copying Messages from Conversations Copy agent responses and your own messages to use in documents, emails, or other applications. Copying a Message Copying Your Own Messages Your messages appear on the right side of the conversation with a colored background (typically matching your organization's theme color). To copy a message you sent: Find the message you want to copy Look in the upper-right corner of your message, next to the timestamp Click the copy icon (it looks like two overlapping rectangles \uD83D\uDCCB) A green confirmation message appears briefly at the top of the screen: \"Message copied to clipboard with formatting!\" Tip: The copy icon may only appear when you hover over the message area with your mouse. Copying Agent Responses Agent responses appear on the left side of the conversation with a light gray background. They typically have the agent's name and icon at the top. To copy an agent's response: Find the agent response you want to copy Look at the bottom of the message where you'll see action buttons Click the Copy button (it shows a copy icon and the word \"Copy\") A green confirmation message appears briefly: \"Message copied to clipboard with formatting!\" Pasting Copied Content The copied content automatically adapts to where you paste it: In Word Processors and Email (Rich Text) When you paste into applications that support formatting (Microsoft Word, Google Docs, Outlook, Gmail): Bold text and italics are preserved Headings maintain their size and weight Bullet points and numbered lists appear formatted Tables are rendered as actual tables Code appears in a styled box with a monospace font In Plain Text Editors When you paste into plain text applications (Notepad, code editors, some chat apps): Content appears as raw Markdown text Formatting markers like **bold** and - bullet are visible Code blocks show with their original ``` markers Copying Code Blocks Agent responses often include code examples in special formatted blocks. Each code block has its own dedicated Copy button for convenience. To copy just the code from a code block: Find the code block in the agent's response—it appears with a dark background and colored syntax highlighting Look at the top of the code block where you'll see: The programming language name on the left (e.g., \"python\", \"javascript\", \"sql\") A Copy button on the right Click the Copy button A confirmation message appears: \"Code copied to clipboard with formatting!\" Where Code Gets Pasted Destination What Happens Code editor (VS Code, etc.) Clean code ready to run—no extra formatting Microsoft Word or Google Docs Code in a styled gray box with monospace font Email Code in a formatted box, easy to read Notepad Plain code text Tips for Best Results Choose the Right Copy Button Use the message Copy button when you want the full response including explanations, lists, and context Use the code block Copy button when you only need the code itself Pasting Options Destination What You Get Microsoft Word Fully formatted text with styles Google Docs Fully formatted text with styles Email (Outlook, Gmail) Formatted text preserving headers and lists Notepad / Plain text Raw Markdown source VS Code / Code editors Raw text or Markdown Slack / Teams May vary—try both regular paste and paste as plain text Keyboard Shortcuts After copying, use these shortcuts to paste: Action Windows Mac Standard paste Ctrl + V Cmd + V Paste as plain text Ctrl + Shift + V Cmd + Shift + V Note: \"Paste as plain text\" is useful when the formatted version doesn't look right in your destination app. Troubleshooting Nothing Happens When I Click Copy If clicking Copy doesn't show a confirmation message: Check browser permissions: Your browser may have blocked clipboard access Look for a clipboard or permission icon in your browser's address bar Click it and select \"Allow\" for clipboard access Refresh the page: Press F5 or click the refresh button, then try again Try a different browser: Chrome and Edge have the best clipboard support. Firefox works well too. Safari has limited support for advanced copy features. No Confirmation Message Appears The confirmation message appears briefly at the top of the screen. If you miss it: The copy likely still worked—try pasting (Ctrl+V / Cmd+V) to verify Make sure pop-ups or notifications aren't blocked for this site Formatting Looks Wrong After Pasting In email: Check that you're composing in \"Rich Text\" or \"HTML\" mode, not \"Plain Text\" mode In Outlook: Look for the \"Format Text\" tab and select \"HTML\" In Word or Docs: Use \"Paste Special\" (Ctrl+Shift+V in Word) and choose \"Keep Source Formatting\" Or paste normally and use \"Paste Options\" button that appears to adjust For code: If code styling doesn't appear correctly, use \"Paste as plain text\" instead In Word, insert a text box or use a code-formatting add-in for best results Large Responses Are Slow to Copy Very long agent responses may take a moment to process before the confirmation appears. Wait a second or two for the confirmation message before pasting. What Gets Copied When you copy a message, the system copies: ✅ Included: All text content Formatting (bold, italic, headers, lists) Tables Code blocks with syntax highlighting Links ❌ Not included: Attached files (you'll need to download these separately) Images generated by the agent (right-click to save these) Interactive elements Related Topics Printing Conversations — Create printable versions of your chats Managing Conversations — Organize and find your past conversations Downloading Files — Save files generated by agents"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/creating-editing-agents.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/creating-editing-agents.html",
    "title": "Creating and Editing Custom Agents | FoundationaLLM",
    "summary": "This article is still being authored. Some sections contain placeholder content that requires additional information. Creating and Editing Custom Agents Learn how to create and customize your own agents in the Chat User Portal when self-service agent creation is enabled. Overview When your organization enables the Agent Self-Service feature, you can create custom agents tailored to your specific needs directly from the Chat User Portal. Custom agents allow you to: Define specialized behaviors with custom prompts Select appropriate AI models for your use case Configure tools like image generation and file upload Share agents with colleagues Note: This feature must be enabled by your administrator. If you don't see agent creation options, contact your IT department. Prerequisites Before creating custom agents: Your organization must have Agent Self-Service enabled You need appropriate permissions (typically the Agent Contributor role) At least one AI model must be available for selection Accessing Agent Creation From Settings Click the Settings button (gear icon ⚙️) in the bottom-left corner of the sidebar Click the Manage Agents link at the bottom of the dialog Click Create New Agent From Agent Management Open the Agent Management page (if available) Click Create New Agent button The agent creation form opens Agent Configuration Options Basic Information Field Description Requirements Agent Display Name The name users see when selecting the agent Required. Any text. Description Brief description of what the agent does Recommended for discoverability Welcome Message Greeting shown when starting a conversation Optional. Supports rich text. Prompt Definition The prompt defines your agent's personality, knowledge focus, and behavior: Setting Description System Prompt Instructions that guide the agent's responses Prompt Style Formal, conversational, technical, etc. Focus Area What topics the agent specializes in Best practices for prompts: Be specific about the agent's role and expertise Include any constraints or boundaries Specify the tone and style of responses Define how to handle off-topic questions Model Selection Choose the AI model that powers your agent: Consideration Guidance Capability More capable models handle complex tasks better Speed Faster models provide quicker responses Cost Different models have different usage costs Context Length Larger context windows support longer conversations Note: Available models depend on what your administrator has configured. Temperature Setting Temperature controls the randomness/creativity of responses: Temperature Behavior Best For Low (0.0-0.3) Consistent, focused responses Factual Q&A, data analysis Medium (0.4-0.7) Balanced creativity General assistance High (0.8-1.0) More creative, varied responses Creative writing, brainstorming Agent Status Status Description Active Agent is available for use Inactive Agent exists but cannot be selected Expired Agent has passed its expiration date (if set) You can set an Expiration Date to automatically disable the agent after a certain date. Tool Selection Enable tools to extend your agent's capabilities: Image Generation Enable agents to create images using AI: Powered by DALL-E or similar models Users can request image creation in conversations Generated images can be downloaded Upload from Computer Allow users to upload files for analysis: Supports common document formats (PDF, DOCX, etc.) Files are processed for the current conversation Enable for document analysis, data processing tasks Private Storage Knowledge Source Connect to agent-specific private storage: Access documents stored in the agent's dedicated storage Provides persistent knowledge across conversations Configured separately in Private Storage settings Website Crawler TODO: This feature is planned for future release. Website crawler tool will allow agents to access and search web content. Accessibility Best Practices When creating agents, follow accessibility guidelines: Explainer Text Provide clear, accessible descriptions: Use plain language in agent descriptions Explain what the agent does in the welcome message Include guidance on how to interact effectively Accessibility Considerations Element Guidance Display Name Use clear, descriptive names Welcome Message Provide context for screen reader users Tool Descriptions Explain what each enabled tool does Error Messages Ensure errors are clearly communicated Saving Your Agent Review all configuration settings Click Create Agent or Save Changes Wait for the creation process to complete Your agent appears in your agent list Editing Existing Agents Accessing Edit Mode Open Settings > Agents tab Find your agent in the list Click the Edit button (pencil icon) Editable Properties You can modify: Display name (not the internal agent name) Welcome message Prompt definition Model selection Temperature Tool configuration Agent status and expiration Note: The internal agent name cannot be changed after creation. Saving Changes Make your modifications Click Save Changes Changes take effect immediately for new conversations Agent Visibility Who Can See Your Agent Custom agents you create are: Visible to you by default Shareable with others through the sharing model Subject to your organization's visibility policies Making Agents Available See Sharing Agents for information on sharing your custom agents with others. Limitations Limitation Description Model Access Only models configured by admins are available Tool Access Only enabled tools can be selected Storage Private storage requires separate configuration Permissions Some settings may require elevated permissions Troubleshooting Can't Create Agents Verify Agent Self-Service is enabled for your organization Check you have the Agent Contributor role Contact your administrator for permission Model Not Available The model may not be configured in your deployment Your permissions may not include that model Contact your administrator Tools Not Working Verify the tool is properly enabled Check your agent configuration Some tools require additional setup Agent Not Appearing Ensure the agent status is Active Check the agent hasn't expired Verify sharing settings if others can't see it Related Topics Sharing Agents — Share your agents with others Managing Available Agents — Control which agents you see Selecting an Agent — Choose an agent for conversations"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/downloading-files.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/downloading-files.html",
    "title": "Downloading Files from a Conversation | FoundationaLLM",
    "summary": "Downloading Files from a Conversation Learn how to download files that agents generate during your conversation. When Agents Generate Files Some agents can create files as part of their responses. These files might include: Charts and graphs — Visualizations created from data analysis Generated images — Images created by image generation tools like DALL-E Data exports — CSV, JSON, or other data files Code files — Scripts or programs written by the agent Reports — Formatted documents or summaries Processed data — Results from calculations or transformations Note: Not all agents can generate files. This capability depends on the tools and configurations enabled for each agent. Recognizing Downloadable Files When an agent generates a file, it appears in the conversation in one of these ways: File Links in the Message Files appear as clickable links within the agent's response. You'll see: A file icon next to the link indicating the file type The filename as clickable text Links are styled differently from regular web links Images in the Response Generated images display directly in the conversation: Click the image to view it in full size The full-size preview includes a close button Right-click the image preview to save it Additional Content Section Some files appear in an Additional Content section at the bottom of the agent's message, separate from the main response text. How to Download Files Downloading Linked Files Find the file link in the agent's response (look for the file icon \uD83D\uDCC4) Click the link — the file will begin downloading Check your browser's download location for the file The file is saved with its original filename Saving Generated Images Click the image in the conversation to open the full-size preview Right-click on the preview image Select Save image as... from the context menu Choose a location and filename Click Save Alternative method: Click the image to open the preview Use Ctrl + S (Windows) or Cmd + S (Mac) to save File Types You May Encounter Category Common Types Description Images PNG, JPEG, SVG Charts, graphs, generated images Data CSV, JSON, XLSX Exported data, analysis results Documents PDF, TXT, MD Reports, summaries, documentation Code PY, JS, SQL Scripts, queries, programs Where Downloads Go Downloaded files go to your browser's default download location: Operating System Default Location Windows C:\\Users\\[YourName]\\Downloads Mac /Users/[YourName]/Downloads Linux /home/[YourName]/Downloads Tip: You can change your download location in your browser's settings under Downloads or Privacy & Security. Troubleshooting Download Doesn't Start Pop-up blocker: Your browser may be blocking the download. Look for a blocked pop-up notification in the address bar Check downloads: Some browsers download files silently — check your Downloads folder Try again: Click the link again; temporary network issues may have interrupted the download File Won't Open Check the file extension: Make sure you have software installed that can open the file type File appears corrupted: Try downloading again Large files: Large files may take longer to download completely Image Won't Save Wait for loading: Make sure the image has fully loaded before trying to save Try the preview: Click to open the full-size preview, then save from there Right-click options: If \"Save image as\" isn't available, try \"Copy image\" and paste into an image editor Can't Find the Download Check your browser's download history (Ctrl + J on Windows, Cmd + Shift + J on Mac in Chrome) Look in your Downloads folder Search your computer for the filename Check if downloads are going to a different folder (browser settings) File Link Shows an Error The file may have expired or been removed Try asking the agent to regenerate the file Check your internet connection Tips for Managing Downloaded Files Organize immediately: Move downloaded files to appropriate folders rather than leaving them in Downloads Rename if needed: Generated filenames may be technical — rename to something meaningful Check file integrity: Open files after downloading to verify they're complete and correct Note the source: If saving for reference, note which conversation generated the file Security Considerations Downloaded files come from the agent's processing environment Treat downloaded files like any other download — scan with antivirus if concerned Be cautious with executable files (.exe, .bat, .sh) — review code before running Data exports may contain sensitive information — store securely Related Topics Uploading Files to a Conversation — Send files to agents for analysis Using the Code Interpreter Tool — How agents generate files Copying Messages — Copy text content from responses"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/managing-available-agents.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/managing-available-agents.html",
    "title": "Managing Available Agents | FoundationaLLM",
    "summary": "Managing Available Agents Control which agents appear in your agent selector and learn how to access agent settings. Understanding the Agent Catalog The agent catalog is the collection of all agents available to you. Think of it as your personal library of AI assistants. Each user's catalog may be different based on their permissions and organization settings. What Determines Your Agent Catalog The agents you see in the Chat User Portal depend on several factors: Organization publishing: Administrators publish agents for general use Access permissions: Your role determines which agents you can access Shared agents: Colleagues may share custom agents with you Your own agents: Agents you create (if self-service is enabled) Your preferences: You can enable or disable agents you have access to Featured agents: Some agents are highlighted as featured and cannot be disabled How Agents Appear in the Dropdown When you click the agent selector in the navigation bar: Featured agents appear first, grouped at the top Other enabled agents appear below in alphabetical order Only agents you've enabled in Settings appear in the dropdown Your most recently used agents may be prioritized User Portal vs. Management Portal Agent management happens in different places depending on your role: Task User Portal Management Portal Select an agent to use ✅ ❌ Enable/disable agents for yourself ✅ ❌ Create self-service agents ✅ (if enabled) ✅ Edit your custom agents ✅ (if enabled) ✅ Configure enterprise agents ❌ ✅ Manage all organization agents ❌ ✅ Set up data sources and pipelines ❌ ✅ Configure agent access control ❌ ✅ User Portal is designed for end users who want to: Use agents in conversations Manage which agents appear in their personal agent selector Create and edit their own custom agents (if self-service is enabled) Management Portal is designed for administrators who need to: Create and configure enterprise-wide agents Manage agent permissions and access control Set up knowledge sources and data pipelines Configure platform-wide settings Accessing Agent Settings Click the Settings button (gear icon ⚙️) in the bottom-left corner of the sidebar The Agents tab opens by default View your available agents in the table The Agents Table The agent settings table shows: Column Description Name The display name of the agent Enabled Checkbox showing if the agent appears in your selector Edit Edit button (if you have permission to modify the agent) Enabling and Disabling Agents To Enable an Agent Find the agent in the table Click the checkbox in the Enabled column A checkmark (✓) appears and the agent is now available in your agent selector A confirmation toast appears: \"Agent is now enabled\" To Disable an Agent Find the agent in the table Click the checked checkbox in the Enabled column The checkmark disappears and the agent is removed from your selector A confirmation toast appears: \"Agent is now disabled\" Agents You Cannot Disable Some agents cannot be disabled: Currently active agent: You cannot disable the agent you're currently using in a conversation Pinned featured agents: Administrators may require certain agents to always be available These agents show a grayed-out checkbox that cannot be changed. Finding Agents Using Search Find the Search agents by name field above the table Type part of the agent's name The list filters to show matching agents Clear the search to show all agents again Filtering by Status Check the Show enabled agents only checkbox The table shows only agents you've enabled Uncheck to show all available agents Understanding Agent Status Visual Indicator Meaning ✓ Checkmark (blue) Agent is enabled and available Empty checkbox Agent is disabled and hidden from selector Grayed checkbox Agent cannot be disabled (featured or in use) Edit icon (blue) You can edit this agent Edit icon (gray) You can view but not edit this agent Requesting Access to More Agents If you need access to agents not in your list: Look for the Request permission to manage agents link at the bottom of the Settings dialog Click the link to open your organization's access request process Follow your organization's procedure to request additional agent access Note: This link only appears if your administrator has configured an access request URL. Editing Agents (If Permitted) If you have agent contributor permissions: Find the agent in the table Click the Edit button (pencil icon) You'll be taken to the agent editing page Make your changes Save when finished Note: The Edit button appears only if the Agent Self-Service feature is enabled for your organization. Managing Agents Page For more advanced agent management: Open the Settings dialog Look for the Manage Agents link at the bottom Click to open the full agent management page This page provides: Detailed agent information Agent creation (if permitted) Full editing capabilities File management for agent knowledge bases Tips for Organizing Your Agents Keep Your List Focused Enable only the agents you use regularly Disable agents for projects you've completed This makes finding the right agent faster Use Descriptive Names When Searching Search by keywords that describe what the agent does Agent names often include their purpose (e.g., \"Sales Report Generator\") Check for New Agents Regularly Administrators may add new agents over time Periodically review disabled agents to see if new ones are available Troubleshooting Agent List is Empty Wait a moment — agents are still loading Refresh the page if the loading spinner persists Check with your administrator if no agents appear Cannot Enable/Disable an Agent The agent may be pinned by administrators You may be currently using the agent Switch to a different agent, then try again Changes Not Saving Ensure you have a stable internet connection Try refreshing the page and making the change again Check if you're seeing any error messages Edit Button is Grayed Out You may not have permission to edit this agent The agent may be read-only Contact your administrator for editing access Search Returns No Results Check your spelling Try a partial name or different keywords Clear the search to see all agents Related Topics Selecting an Agent — Choose an agent for your conversation Managing Conversations — Work with your chat sessions"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/managing-conversations.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/managing-conversations.html",
    "title": "Managing Conversations | FoundationaLLM",
    "summary": "Managing Conversations Create, organize, rename, and delete your conversations in the Chat User Portal. Understanding Conversations A conversation (also called a chat or session) is a continuous dialog with an AI agent. Each conversation: Has its own message history Maintains context from previous messages in that conversation Can use a different agent than other conversations Is saved automatically so you can return to it later The Sidebar The sidebar on the left side of the screen shows all your saved conversations: Chats header with a + button to create new conversations List of your conversations, newest first Each conversation shows its name (which you can customize) The currently selected conversation is highlighted Creating a New Conversation Using the + Button Click the + button next to the \"Chats\" header in the sidebar A new conversation is created immediately The conversation becomes active and ready for your first message Automatic Creation A new conversation is also created when: You switch agents during an active conversation Your current conversation is the first message with a new agent Tip: If you accidentally create a new conversation when you meant to continue an existing one, click the existing conversation in the sidebar to return to it. Selecting a Conversation Look at the conversation list in the sidebar Click on any conversation to open it The selected conversation is highlighted with a colored background and left border Your messages and the agent's responses appear in the main area Renaming a Conversation By default, conversations are named with a timestamp (e.g., \"Chat 12/14/2024, 10:30 AM\"). You can give them more descriptive names. To Rename Find the conversation in the sidebar Hover over the conversation to reveal the action icons Click the Settings button (gear icon ⚙️) In the dialog that appears, edit the Name field Click Update to save Naming Tips Use descriptive names like \"Q4 Sales Analysis\" or \"Code Review - Login Feature\" Include the project or topic for easy identification Add dates if you have ongoing conversations about the same topic Adding Metadata to Conversations You can attach metadata (additional information) to conversations for organizational purposes. Click the Settings button (gear icon) on a conversation Find the Metadata field Enter valid JSON data, such as: { \"project\": \"Marketing Campaign\", \"priority\": \"high\", \"deadline\": \"2024-01-15\" } Click Update to save Note: Metadata must be valid JSON format. If you enter invalid JSON, you'll see an error message. Deleting a Conversation Using the Delete Button Find the conversation in the sidebar Hover over the conversation to reveal the action icons Click the Delete button (trash icon \uD83D\uDDD1️) A confirmation dialog appears: \"Do you want to delete the chat '[conversation name]'?\" Click Yes to confirm deletion Using Keyboard Navigate to the conversation using Tab key Press Delete or Backspace Confirm the deletion in the dialog ⚠️ Warning: Deleted conversations cannot be recovered. Make sure you no longer need the information before deleting. Switching Agents Within Conversations When you change agents: Click the agent selector dropdown in the top navigation Choose a different agent A new conversation starts automatically Your previous conversation with the other agent remains in the sidebar This behavior ensures each conversation maintains consistent context with a single agent. Conversation Features Context Retention Agents remember previous messages in your conversation: Recent messages are included when generating responses This allows for follow-up questions and clarifications The number of messages retained depends on agent configuration Automatic Saving Conversations save automatically as you chat You can close the browser and return later No \"save\" button is needed Session Persistence Conversations are tied to your user account Access your conversations from any device by logging in Conversations persist until you delete them Managing Many Conversations If you have many conversations: Quick Navigation Scroll through the sidebar to find conversations Click any conversation to open it immediately The current conversation remains highlighted Cleanup Strategy Regularly delete conversations you no longer need Rename important conversations for easy identification Consider the conversation's purpose before deleting Troubleshooting Conversation Won't Load Wait for the loading spinner to complete Refresh the page (F5 or Ctrl + R) Check your internet connection Messages Not Appearing Scroll down to see newer messages Wait for the conversation to fully load The conversation may be empty if you haven't sent any messages yet Cannot Delete Conversation Make sure you've confirmed the deletion dialog Check if you have a stable internet connection Try refreshing the page and attempting again Lost a Conversation Conversations cannot be recovered once deleted Check if you're logged into the correct account The conversation may have been deleted from another device New Conversation Created Unexpectedly This happens when: You switched agents during an active conversation The current agent was changed You clicked the + button Your previous conversation is still available in the sidebar. Rename Not Saving Make sure metadata (if provided) is valid JSON Check your internet connection Click the Update button (not just close the dialog) Keyboard Shortcuts Action Keys Navigate sidebar Tab through items Select conversation Enter when focused Delete conversation Delete or Backspace when focused Cancel dialog Escape Confirm action Enter Related Topics Selecting an Agent — Choose the right agent for your task Printing Conversations — Save conversations as PDF or print Copying Messages — Copy content from conversations"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/monitoring-tokens.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/monitoring-tokens.html",
    "title": "Monitoring Token Consumption | FoundationaLLM",
    "summary": "Monitoring Token Consumption Understand and track token usage during your conversations with AI agents. What Are Tokens? Tokens are the units of text that AI models process. When you send a message or receive a response, the text is broken into tokens for processing. Token Basics 1 token ≈ 4 characters in English (roughly 3/4 of a word) Shorter words may be one token, longer words may be multiple tokens Punctuation and spaces also consume tokens Non-English text may use more tokens per character Examples Text Approximate Tokens \"Hello\" 1 token \"Hello, how are you today?\" 7 tokens \"The quick brown fox jumps over the lazy dog\" 10 tokens A 500-word document ~650-700 tokens Why Token Tracking Matters Understanding token usage helps you: Manage costs: Higher token usage may affect your organization's AI usage costs Optimize prompts: Write more efficient prompts to reduce token consumption Understand limits: Know when you're approaching conversation or request limits Troubleshoot: Large token counts may explain slower responses Viewing Token Information Token display is a feature that must be enabled by your administrator. When enabled: Token Chip Each message displays a token chip showing the token count: Located in the top-right corner of each message, next to the timestamp Shows \"Tokens: [number]\" User messages: Show prompt tokens (what you sent) Agent responses: Show completion tokens (what was generated) Token Chip Colors The token chip appearance differs based on the message type: Your messages: Chip matches your message's accent color Agent responses: Chip matches the agent message's theme color When Token Display Is Hidden You won't see token information if: Your administrator hasn't enabled token display The current agent doesn't have token display enabled The message is still being generated (tokens show after completion) Note: Even when not displayed, tokens are still being consumed. Contact your administrator if you need token visibility enabled. Understanding Token Counts Prompt Tokens (Your Messages) Prompt tokens include: The text you typed Any system instructions the agent uses Context from previous messages in the conversation Information from uploaded files being referenced Completion Tokens (Agent Responses) Completion tokens include: The agent's response text Any formatted content (tables, lists, code) Generated data or analysis Total Tokens The total tokens for a single exchange equals: Prompt tokens + Completion tokens For a conversation, total usage is the sum of all message token counts. Tips for Optimizing Token Usage Write Concise Prompts Instead of: \"I would like you to please help me understand what the quarterly sales numbers were for the last fiscal year, if you could be so kind as to summarize them for me.\" Try: \"Summarize Q1-Q4 sales from last fiscal year.\" Be Specific Clear, specific questions reduce the need for follow-up clarifications: Specify the format you want (bullet points, table, paragraph) Mention the level of detail needed Include relevant constraints upfront Use Conversation History Wisely Reference previous responses instead of repeating information Start new conversations for unrelated topics The agent includes recent history, which adds to prompt tokens Break Down Complex Requests For complex tasks: Ask one question at a time Build on previous answers This often uses fewer total tokens than one massive request Token Limits Message Limits Individual messages may have token limits: Very long prompts may be truncated Extremely long responses may be cut short Your administrator sets these limits Conversation Context Limits The conversation history included in each request has limits: Older messages may not be included in context This is why agents may \"forget\" earlier parts of long conversations Rate Limits Your organization may have: Requests per minute limits Tokens per minute limits Daily or monthly token quotas Troubleshooting Token Count Seems Too High High token counts may be due to: Long conversation history being included Uploaded files adding context Complex formatting in the response The agent retrieving information from knowledge bases Token Count Shows Zero or Not Displaying Wait for the message to finish generating Token display may not be enabled (ask your administrator) There may have been an error calculating tokens Response Cut Off Mid-Sentence The response may have hit a token limit: Try asking for a shorter response Request the information in parts Ask the agent to continue from where it stopped Slow Responses with High Token Counts More tokens = longer processing time: Consider breaking the request into smaller parts Ask for summaries instead of full details Reduce the complexity of your request Checking Your Usage If you need to monitor overall token usage: Contact your administrator for usage reports Ask about organization-wide dashboards Review any quotas or limits that apply to your account Related Topics Managing Conversations — Control conversation length and context Copying Messages — Extract content without regenerating Rating Responses — Provide feedback on responses"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/printing-conversations.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/printing-conversations.html",
    "title": "Printing Conversations | FoundationaLLM",
    "summary": "Printing Conversations Create a printed copy or PDF of your conversation with an AI agent. How to Print a Conversation Open the conversation you want to print Look in the top navigation bar, to the right of the agent selector dropdown Click the Print button (it looks like a printer icon \uD83D\uDDA8️) Your browser's print dialog will open Choose your printer or select Save as PDF Click Print or Save Tip: The print button only appears when you have a conversation open. If you don't see it, make sure you've selected a chat session from the sidebar. What Gets Printed When you print a conversation, the output includes: ✅ Included in printout: All your messages (questions and prompts) All agent responses Message formatting (bold, italics, headers, lists) Tables Code blocks (with syntax highlighting colors preserved) Mathematical equations and formulas Timestamps showing when each message was sent ❌ Not included in printout: Navigation sidebar Agent selector dropdown Chat input box Copy, Rate, and View Prompt buttons File upload areas Any attached files (these need to be downloaded separately) Saving as PDF Instead of printing to paper, you can save your conversation as a PDF file: Click the Print button in the navigation bar In the print dialog, look for the printer/destination selection Choose Save as PDF, Microsoft Print to PDF, or similar option Click Save or Print Choose where to save the PDF file on your computer Click Save Where to Find \"Save as PDF\" The option location varies by browser: Browser Where to Find It Chrome Destination dropdown → Select \"Save as PDF\" Edge Printer dropdown → Select \"Microsoft Print to PDF\" or \"Save as PDF\" Firefox Destination dropdown → Select \"Save to PDF\" Safari PDF dropdown (bottom-left) → Select \"Save as PDF\" Print Settings Before printing, you can adjust settings in your browser's print dialog: Page Layout Portrait (default): Best for most conversations Landscape: Useful if your conversation has wide tables or code blocks Margins Default/Normal: Recommended for most printouts Minimum/None: Fits more content per page but may cut off edges Scale 100% (default): Normal size Fit to page: Shrinks content if needed to fit the page width Custom: Adjust percentage manually Background Graphics Enable this option to preserve message colors and code block backgrounds Usually labeled \"Background graphics\" or \"Print backgrounds\" Important: Enable \"Background graphics\" or \"Print background colors\" to keep the visual distinction between your messages and agent responses. Tips for Better Printouts Before Printing Scroll through the entire conversation first to ensure all messages have fully loaded Long conversations will span multiple pages automatically Code blocks print better in landscape orientation if they're wide Choosing Paper Size Letter (8.5\" × 11\") or A4: Standard choice for most printouts Larger paper sizes work well for conversations with wide tables or code For Professional Documents If you're printing for a report or documentation: Save as PDF first Open the PDF and verify it looks correct Print from the PDF for consistent results Troubleshooting Print Button Not Visible Make sure you have a conversation open (not just the welcome screen) The button appears next to the agent dropdown in the top navigation bar Try refreshing the page if buttons aren't loading Colors Not Printing In the print dialog, look for Background graphics or Print backgrounds Make sure this option is enabled/checked Some printers default to black and white—check your printer settings Content Getting Cut Off Try Landscape orientation for wide content Reduce the Scale to 90% or 80% Use Minimum margins if available For code blocks, consider copying the code separately instead Pages Are Blank or Missing Content Wait for the conversation to fully load before printing Scroll through the conversation to trigger loading of all messages Try refreshing the page and printing again Check if your browser is blocking pop-ups (the print dialog is a pop-up) Mathematical Equations Look Wrong Mathematical formulas should print correctly. If they appear broken: Try using Chrome or Edge for better equation rendering Save as PDF first, then print from the PDF Print Dialog Doesn't Open Check if your browser is blocking pop-ups for this site Try the keyboard shortcut: Ctrl + P (Windows) or Cmd + P (Mac) Make sure you clicked the print button and not another icon Keyboard Shortcut You can also use your browser's built-in print shortcut: Operating System Shortcut Windows Ctrl + P Mac Cmd + P This opens the same print dialog as clicking the print button. Alternatives to Printing If printing doesn't meet your needs, consider these options: Need Alternative Share specific responses Use the Copy button on individual messages Save for later reference Save as PDF instead of printing Include in a document Copy messages and paste into Word or Google Docs Archive conversations Your conversations are automatically saved in the sidebar Related Topics Copying Messages — Copy individual messages with formatting Managing Conversations — Find and organize your saved conversations Downloading Files — Save files generated by agents"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/rating-responses.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/rating-responses.html",
    "title": "Rating Agent Responses | FoundationaLLM",
    "summary": "Rating Agent Responses Provide feedback on agent responses to help improve quality and inform administrators. Why Rate Responses? Your ratings help: Improve agent quality: Administrators use feedback to refine agent configurations Track performance: Organizations can monitor how well agents are meeting user needs Identify issues: Low ratings highlight problems that need attention Guide development: Feedback influences future agent improvements When Rating Is Available The rating feature appears when: Your administrator has enabled message rating The agent is configured to allow ratings The agent's response has finished generating Look for the Rate Message button in the footer of agent responses. How to Rate a Response Step 1: Find the Rate Button Look at the bottom of an agent's response message Find the Rate Message button (shows a thumbs-up icon \uD83D\uDC4D) The icon changes based on any existing rating: Outline icon: Not yet rated Filled thumbs-up: You liked this response Filled thumbs-down: You disliked this response Step 2: Open the Rating Dialog Click Rate Message A dialog opens with rating options Step 3: Choose Your Rating In the rating dialog: Button Meaning Like (thumbs up) The response was helpful, accurate, and useful Dislike (thumbs down) The response had issues, was unhelpful, or incorrect Click a button to select your rating: The button fills in to show your selection Click the same button again to remove your rating You can change your mind before submitting Step 4: Add Comments (Optional) The Comments field lets you explain your rating: For positive ratings, you might note: What made the response particularly helpful How the agent exceeded expectations Specific aspects that worked well For negative ratings, consider explaining: What was incorrect or misleading What information was missing How the response could have been better What you expected instead Step 5: Submit Your Rating Click Submit to save your rating A confirmation message appears: \"Rating submitted!\" The dialog closes automatically Rating Indicators After rating, the response shows your feedback: Icon State Meaning \uD83D\uDC4D (filled) You liked this response \uD83D\uDC4E (filled) You disliked this response \uD83D\uDC4D (outline) Not rated, or rating cleared Changing Your Rating You can update a rating at any time: Click Rate Message on the response Select a different rating option Update your comments if desired Click Submit Your new rating replaces the previous one. Clearing a Rating To remove your rating entirely: Click Rate Message Click your current rating (Like or Dislike) to deselect it Clear the comments field if desired Click Submit What Makes a Good Rating? When to Give a Positive Rating (\uD83D\uDC4D) The response directly answered your question Information was accurate and complete The format was easy to read and use The agent understood your intent correctly The response saved you time or effort When to Give a Negative Rating (\uD83D\uDC4E) The response was factually incorrect The agent misunderstood your question Important information was missing The response was confusing or poorly formatted The agent gave generic content instead of specific help Writing Helpful Comments Good comments are specific: Instead of: \"Bad response\" Try: \"The sales figures for Q3 were incorrect — actual revenue was $2.1M not $1.8M\" Instead of: \"Great!\" Try: \"The step-by-step format made it easy to follow the instructions\" When Rating Isn't Available You won't see the rating option if: Feature disabled: Your administrator hasn't enabled ratings Agent configuration: The specific agent doesn't support ratings Message still loading: Wait for the response to complete User messages: Only agent responses can be rated Privacy and Visibility Your ratings: Are associated with your user account Are visible to administrators and system managers May be used to improve agent performance Help identify patterns across users Note: Comments should be professional and focused on the response quality. Avoid including sensitive personal information in rating comments. Best Practices Rate Consistently Apply the same standards across similar responses Don't let one bad experience color ratings of unrelated responses Rate based on the specific response, not the agent overall Provide Constructive Feedback Focus on how the response could improve Be specific about what was wrong or right Suggest what a better response would include Rate Important Responses You don't need to rate every message Focus on responses that were notably helpful or problematic Your feedback is most valuable for responses you have expertise to evaluate Troubleshooting Rate Button Not Visible The feature may not be enabled — contact your administrator Wait for the response to finish generating Check if you're looking at an agent response (not your own message) Rating Won't Submit Check your internet connection Try refreshing the page and rating again Ensure you've selected either Like or Dislike Comments Not Saving Comments are saved with the rating submission Make sure you clicked Submit after adding comments Very long comments may be truncated Wrong Rating Submitted Click Rate Message to open the dialog again Select the correct rating Click Submit to update Related Topics Viewing Agent Prompts — Understand how responses are generated Managing Conversations — Organize your chat history Copying Messages — Save responses you find valuable"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/selecting-agent.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/selecting-agent.html",
    "title": "Selecting an Agent | FoundationaLLM",
    "summary": "Selecting an Agent Choose the right AI agent for your task from the available options. Understanding Agents Agents are AI assistants configured for specific purposes. Different agents may: Have access to different knowledge bases or data sources Be optimized for particular types of tasks Use different AI models or capabilities Have unique tools like code execution or image generation Selecting the right agent ensures you get the best possible responses for your needs. The Agent Selector The agent selector is a dropdown menu located in the top navigation bar, to the right of the current conversation name. What You'll See Agent icon: An icon representing the current agent Dropdown menu: Shows the currently selected agent's name Agent groups: Agents are organized into categories Selecting an Agent Step 1: Open the Dropdown Look in the top-right area of the navigation bar Click the agent dropdown (shows the current agent's name) The dropdown expands to show available agents Step 2: Browse Available Agents Agents are organized into groups: Group Description Featured Highlighted agents recommended by your organization Other Additional agents you have access to Step 3: Choose an Agent Review the agent names in the dropdown Click on the agent you want to use A confirmation message appears: \"Agent changed to [agent name]\" What Happens When You Switch Agents When you select a different agent during an active conversation: A new conversation starts — This ensures the new agent isn't confused by context from the previous agent Your previous conversation is preserved — It remains in the sidebar and you can return to it The new agent's welcome message appears — Introducing what the new agent can help with Note: Switching agents always creates a new conversation to maintain clean context for each agent. Agent Information Welcome Messages When you start a conversation with an agent, you may see a welcome message explaining: What the agent specializes in Types of questions it can answer How to get the best results Identifying the Current Agent The current agent is shown: In the dropdown display in the navigation bar Next to each agent response in the conversation (with an agent icon) Featured vs. Other Agents Featured Agents Featured agents are highlighted by your organization because they: Are commonly used or recommended Are particularly well-suited for common tasks Have been recently added or updated Featured agents appear at the top of the dropdown in their own section. Other Agents The \"Other\" section contains: Agents not designated as featured Specialized agents for less common tasks Agents you've enabled but aren't featured Managing Your Agent List You can control which agents appear in your dropdown: Click the Settings button (gear icon) in the sidebar Go to the Agents tab Enable or disable agents using the checkboxes Only enabled agents appear in your agent selector See Managing Available Agents for more details. Choosing the Right Agent Match Agent to Task Task Type Look For Data analysis Agents with code interpreter tools Document questions Agents with knowledge base access Creative writing General-purpose or creative agents Technical support Domain-specific technical agents Image generation Agents with DALL-E or image tools Read Agent Descriptions If available, agent descriptions help you understand: The agent's purpose and specialization Types of tasks it handles well Any limitations or specific use cases Try Different Agents If one agent's response isn't what you need: Try the same question with a different agent Some agents may be better suited for your specific question Feedback through ratings helps administrators improve agent assignments Tips for Better Results Start Fresh When Switching Since switching agents creates a new conversation: Restate your question clearly in the new conversation Don't assume the new agent knows what you discussed before Provide necessary context in your first message Use Agent Specializations Use specialized agents for their intended purpose General agents are versatile but may not have domain expertise Specialized agents often provide more accurate, detailed responses Ask About Capabilities If you're unsure what an agent can do: Ask: \"What can you help me with?\" Ask: \"What tools or data do you have access to?\" The agent can describe its capabilities Troubleshooting No Agents in Dropdown Wait for agents to load (you may see a loading state) Refresh the page Check Settings > Agents to ensure agents are enabled Contact your administrator if no agents are available Agent Not Responding The selected agent may be experiencing issues Try selecting a different agent Refresh the page and try again Check for system status announcements Wrong Agent Selected Simply select the correct agent from the dropdown A new conversation will start with the correct agent Return to your previous conversation from the sidebar if needed Can't Find a Specific Agent Use Settings > Agents to see all available agents The agent may need to be enabled Search by name in the agent settings Contact your administrator if you need access to an agent Agent Keeps Creating New Conversations This is expected behavior: Switching agents always starts a new conversation This maintains clean context for each agent Previous conversations are saved in the sidebar Related Topics Managing Available Agents — Enable/disable agents Managing Conversations — Navigate between conversations Viewing Agent Prompts — See how agents generate responses"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/setting-default-agent.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/setting-default-agent.html",
    "title": "Setting a Default Agent | FoundationaLLM",
    "summary": "Setting a Default Agent Learn how to set a default agent that's automatically selected for new conversations. Overview The default agent is the AI assistant that's automatically selected when you start a new conversation. Setting a default agent saves time if you primarily use one agent for your work. How Default Agents Work System Default vs. Personal Default Type Set By Applies To System Default Administrators All users (unless they choose differently) Personal Default You Only your account Precedence Your personal default (if set) takes priority Otherwise, the system default is used If no defaults are set, the first available agent is selected Setting Your Default Agent From Settings Click the Settings button (gear icon ⚙️) in the bottom-left corner of the sidebar Click the Agents tab Find the agent you want as your default Click the Set as Default option (if available) A confirmation appears Visual Indicators The current default agent shows a Default badge or star icon When you change the default, the badge moves to the new selection Who Can Set Defaults Personal Default Most users can set their own default agent from the agents they have access to. System Default (Organization-Wide) Setting the system-wide default requires: Access to the Management Portal The User Access Administrator role Navigate to All Agents and use the Set Default action When the Default Agent is Used The default agent is automatically selected when: You open the User Portal for a new session You create a new conversation You don't explicitly select a different agent The default agent is not used when: You switch agents during a conversation (the new agent becomes active) You select a different agent from the dropdown You return to an existing conversation (which uses its original agent) Changing the Default Agent Switching Your Default Go to Settings > Agents Find the new agent you want as default Click Set as Default The previous default is cleared automatically Clearing the Default To stop having a personal default: Note: Some configurations may not allow clearing the default. In this case, simply choose a different agent as your default. Best Practices Choosing a Good Default Use Case Recommended Default General office work A versatile, general-purpose agent Specialized role An agent matching your primary job function Frequent task The agent you use most often When Not to Use Default Consider not relying heavily on the default if: You frequently need different agents for different tasks Your work varies significantly day-to-day You're evaluating multiple agents Troubleshooting Can't Set Default Verify you have access to the agent Check if your organization allows personal defaults The agent may need to be enabled first Default Not Being Used Clear your browser cache Check if you have a conversation already open with a different agent Verify the agent is still active and available Default Changed Unexpectedly An administrator may have changed the system default The previous default agent may have been disabled or deleted Check your Settings to confirm your current default Related Topics Selecting an Agent — Manually choose an agent Managing Available Agents — Control your agent catalog Creating and Editing Custom Agents — Create your own agents"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/sharing-agents.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/sharing-agents.html",
    "title": "Sharing Agents with Others | FoundationaLLM",
    "summary": "Sharing Agents with Others Learn how to share custom agents with colleagues and manage agent access permissions. Overview When you create a custom agent, you can share it with others in your organization. The sharing model defines who can access your agent and what they can do with it. Sharing Model Roles FoundationaLLM uses a three-tier sharing model for agents: Role Can Use Can Edit Can Share Can Delete Owner ✅ ✅ ✅ ✅ Collaborator ✅ ✅ ✅ ❌ User ✅ ❌ ❌ ❌ Owner Owners have full control over the agent: Use the agent in conversations Edit all agent settings and configuration Share the agent with others (assign any role) Delete the agent permanently Transfer ownership to another user When you create an agent, you automatically become its Owner. Collaborator Collaborators can help maintain and improve the agent: Use the agent in conversations Edit agent settings (prompts, tools, configuration) Share the agent with others (assign User or Collaborator role) Cannot delete the agent Cannot remove the Owner's access Collaborators are ideal for team members who help maintain the agent but shouldn't have full control. User Users can interact with the agent but cannot modify it: Use the agent in conversations Cannot edit any settings Cannot share with others Cannot delete the agent Users are ideal for end users who simply need to use the agent. Sharing an Agent Accessing Share Settings Open the agent for editing (if you have Owner or Collaborator access) Look for the Share or Access Control button The sharing panel opens Adding Users In the sharing panel, click Add User or Add Role Assignment Search for the user by name or email Select the user from the results Choose a role: Owner, Collaborator, or User Click Add or Save Removing Users Open the sharing panel Find the user in the list Click the Remove button next to their name Confirm the removal Note: You cannot remove the last Owner. Transfer ownership first if needed. Changing Roles Open the sharing panel Find the user whose role you want to change Click on their current role Select the new role from the dropdown Save changes Sharing Best Practices Team Collaboration Scenario Recommended Approach Small team maintaining an agent Make all team members Collaborators Department-wide agent Owners maintain, department members are Users Cross-team project Project leads as Collaborators, team members as Users Access Principles Least Privilege: Grant the minimum role needed Multiple Owners: Consider having 2-3 Owners for important agents Regular Review: Periodically audit who has access Clear Ownership: Ensure someone is responsible for each agent Who Can See Shared Agents Visibility Rules Shared agents appear in the agent selector for users with access: Users see agents where they have any role (Owner, Collaborator, or User) Agents appear in the dropdown alongside other available agents Users can enable/disable shared agents in Settings Organization Policies Your administrator may configure: Whether self-service agents are visible to others by default Required approval for sharing agents widely Limits on the number of users an agent can be shared with Managing Received Shares Viewing Your Shared Agents Open Settings > Agents tab View the list of available agents Shared agents show your role (Owner, Collaborator, or User) Removing Yourself from a Shared Agent Open Settings > Agents tab Find the shared agent Disable the agent using the checkbox Note: This hides the agent from your view but doesn't remove your access. The Owner can still see you have access. Ownership Transfer When to Transfer Ownership Original creator leaving the organization Changing team responsibilities Consolidating agent management How to Transfer Ownership Open the agent sharing settings (as current Owner) Add the new Owner with the Owner role The new user now has full control Optionally, change your role to Collaborator or remove yourself Notifications TODO: Document notification behavior for sharing events (when users are notified of being added to/removed from agents). Troubleshooting Can't Share Agent Verify you have Owner or Collaborator role Check that sharing features are enabled for your organization Ensure the user exists in your organization's directory User Can't See Shared Agent Verify the user was added correctly Check if the agent is Active (not expired) The user may need to enable the agent in their Settings Can't Remove User You may not have permission (check your role) Cannot remove the last Owner System accounts may have protected access Can't Change Roles Only Owners can assign Owner role Collaborators can only assign User or Collaborator roles Some role changes may require approval Related Topics Creating and Editing Custom Agents — Create your own agents Managing Available Agents — Control which agents you see Selecting an Agent — Choose an agent for conversations"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/uploading-files.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/uploading-files.html",
    "title": "Uploading Files to a Conversation | FoundationaLLM",
    "summary": "Uploading Files to a Conversation Share documents, images, and other files with agents for analysis and reference. When to Upload Files Upload files when you want an agent to: Analyze data in spreadsheets or CSV files Answer questions about document content Process or transform data Generate visualizations from your data Extract information from images Reference specific files in responses Note: Not all agents support file uploads. The upload feature only appears if the agent is configured to accept files. Prerequisites Before uploading files: Agent must support uploads: Check if the paperclip icon appears in the chat input Supported file format: Your file must be in a format the agent can process File size limits: Files must be within size limits set by your administrator Supported File Types Common supported formats include: Category File Types Documents PDF, DOCX, DOC, TXT, MD, RTF Spreadsheets XLSX, XLS, CSV, TSV Images PNG, JPG, JPEG, GIF, BMP, TIFF Code Files PY, JS, TS, JSON, XML, HTML, CSS Archives ZIP, TAR Presentations PPTX, PPT Note: Supported types vary by agent configuration. Some agents may support additional formats or have restrictions. How to Upload Files Step 1: Open the Upload Panel Look for the paperclip icon (\uD83D\uDCCE) to the left of the message input box Click the paperclip to open the file upload panel A badge on the icon shows how many files are currently attached Step 2: Select Files You have two options: From Your Computer Click Select file from Computer Browse to find your file Select one or more files Click Open From OneDrive (If Available) If you see Connect to OneDrive, click it first Authorize access to your OneDrive Click Select file from OneDrive Browse and select files from your OneDrive The OneDrive picker shows your available files Step 3: Review Selected Files Before uploading, selected files appear in the panel with: Information Description File name The name of your file File size Size displayed for local files Source badge \"Local Computer\" or \"OneDrive Work/School\" Status badge \"Pending\" (not yet uploaded) Remove button X button to remove the file Step 4: Upload the Files Review your selected files Click the Upload button A progress bar shows upload status Wait for \"Uploaded\" badges to appear Successfully uploaded files show a green \"Uploaded\" badge Step 5: Close the Panel Click Close to close the upload panel The paperclip badge shows your total attached files Send a message to start working with the files Managing Uploaded Files Viewing Attached Files Click the paperclip icon to see all files for the current conversation Files are listed with their status and source Removing Files To remove a file before sending a message: Open the upload panel (click paperclip) Find the file you want to remove Click the X button next to the file Confirm removal if prompted File Persistence Uploaded files are attached to the current conversation They remain available throughout the conversation Starting a new conversation clears the attachment list OneDrive Integration If your organization has enabled OneDrive integration: Connecting OneDrive Click Connect to OneDrive in the upload panel Sign in with your work or school account Grant necessary permissions Once connected, you'll see Select file from OneDrive Selecting from OneDrive Click Select file from OneDrive A file picker opens showing your OneDrive contents Navigate to find your file Select one or more files Confirm your selection Disconnecting OneDrive Open the upload panel Click Disconnect OneDrive Your OneDrive access is revoked You can reconnect anytime After Uploading Once files are uploaded: Referencing Files Simply mention the file in your message Ask questions about the content Request analysis or processing Example Prompts \"Analyze the sales data in the uploaded spreadsheet\" \"Summarize the key points from the uploaded document\" \"What trends do you see in this CSV file?\" \"Extract all email addresses from the uploaded text file\" \"Create a chart from the data I uploaded\" How Agents Use Files Agents process uploaded files by: Extracting text and data content Making the content available for reference Using the information to answer your questions Potentially generating new files based on your data File Size Limits Limits vary by configuration: Consideration Typical Behavior Per-file limit Usually 10-50 MB per file Total per message Often limited to 10 files Large files May take longer to process Check with your administrator for specific limits in your environment. Troubleshooting Paperclip Icon Not Showing The current agent doesn't support file uploads Try selecting a different agent Contact your administrator if you need upload capability Upload Fails Check that the file is within size limits Verify the file format is supported Try a different file to isolate the issue Check your internet connection File Format Not Accepted Convert the file to a supported format PDF is widely supported for documents CSV works well for data files PNG or JPEG for images OneDrive Connection Issues Ensure you have a work/school OneDrive account Check that your organization allows OneDrive integration Try disconnecting and reconnecting Clear browser cache and try again File Not Being Referenced If the agent doesn't seem to see your file: Make sure the upload completed (shows \"Uploaded\" badge) Reference the file specifically in your message Upload may need to complete before sending your question Try asking \"What files do I have uploaded?\" Progress Bar Stuck Check your internet connection Wait a moment — large files take time If stuck for a long time, refresh the page and try again Very large files may timeout Files Missing After Refresh Uploaded files are tied to the current conversation Switching agents clears attachments Re-upload files if needed after refreshing Tips for Better Results Prepare Your Files Remove unnecessary data from spreadsheets Use clear column headers Ensure text is readable (not scanned images of text) Smaller, focused files process faster Be Specific in Your Questions Reference the file by name if you have multiple Specify what analysis or output you want Ask about specific columns, sections, or data Multiple Files Upload related files together Mention how files relate to each other Consider combining data into one file if appropriate Sensitive Data Be mindful of what data you upload Uploaded files are processed by AI systems Follow your organization's data handling policies Related Topics Downloading Files from a Conversation — Save files agents generate Using the Code Interpreter Tool — Analyze uploaded data Using the Knowledge Tool — Search uploaded documents"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/using-code-interpreter.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/using-code-interpreter.html",
    "title": "Using the Code Interpreter Tool | FoundationaLLM",
    "summary": "Using the Code Interpreter Tool Leverage the Code Interpreter tool for data analysis, calculations, and dynamic code execution. What Is Code Interpreter? Code Interpreter is a tool that allows agents to write and execute Python code in real-time. When enabled, agents can: Analyze data from uploaded files Perform complex calculations Generate charts and visualizations Transform and process data Create files for you to download When Code Interpreter Is Available The Code Interpreter tool is available when: The agent you're using has Code Interpreter enabled Your administrator has configured Python execution environments You're asking questions that benefit from code execution Note: Not all agents have Code Interpreter. If you need this capability, check with your administrator or select an agent that supports it. How It Works The Process You ask a question that requires computation or data analysis The agent writes Python code to address your request Code runs in a secure environment isolated from your system Results are returned in the conversation as text, tables, or images Behind the Scenes Code executes in a containerized Python environment The agent has access to common data science libraries Uploaded files can be accessed by the code Generated files are made available for download What You Can Do Data Analysis Upload spreadsheets or data files and ask questions: \"What's the average sales by region in this file?\" \"Find correlations between columns A and B\" \"Show me the top 10 customers by revenue\" \"Calculate year-over-year growth rates\" Visualizations Request charts and graphs from your data: \"Create a bar chart of monthly sales\" \"Plot the trend line for quarterly revenue\" \"Make a pie chart showing market share\" \"Generate a scatter plot comparing price and quantity\" Calculations Perform mathematical or statistical operations: \"Calculate the compound interest on $10,000 at 5% for 10 years\" \"What's the standard deviation of these numbers?\" \"Solve this equation: 2x + 5 = 15\" \"Run a regression analysis on this dataset\" Data Transformation Process and transform data: \"Convert this CSV to JSON format\" \"Clean this data by removing duplicates\" \"Merge these two datasets by customer ID\" \"Pivot this data to show totals by category\" File Generation Create new files from your data: \"Export the results as a CSV file\" \"Save this chart as a PNG image\" \"Create a summary report of the analysis\" \"Generate a cleaned version of this dataset\" Example Prompts Basic Analysis \"I've uploaded a CSV file with sales data. Can you summarize the key metrics including total revenue, number of transactions, and average order value?\" Visualization Request \"Using the uploaded data, create a line chart showing sales trends over the past 12 months with a trend line.\" Complex Processing \"Compare the sales performance across all regions. Show the top 3 and bottom 3 performers, and calculate the percentage difference from the average.\" Data Cleaning \"The uploaded file has some issues — there are duplicate rows and some missing values in the 'price' column. Clean the data and show me what was fixed.\" Understanding the Output Text Results The agent explains findings in natural language: Summaries of calculations Interpretations of data Answers to your questions Tables Data results may appear as formatted tables: Column headers Organized rows of data Numerical results Charts and Images Visualizations appear directly in the conversation: Click to view full size Right-click to save Charts are PNG images by default Generated Files When the agent creates files: Download links appear in the response Click to download the file Files may be in various formats (CSV, XLSX, PNG, etc.) Tips for Best Results Provide Clear Data Use clean, well-formatted files Include clear column headers Remove unnecessary data before uploading Smaller, focused datasets process faster Be Specific in Requests Instead of: \"Analyze this data\" Try: \"Calculate the monthly average and identify any months with values more than 2 standard deviations from the mean\" Request Specific Formats Specify chart types you want Ask for specific file formats for exports Request particular calculations or metrics Build on Previous Results Reference earlier analysis in the conversation Ask follow-up questions Request modifications to previous outputs Check the Results Verify calculations make sense Spot-check numbers against your source data Ask for explanations if results are unexpected Limitations What Code Interpreter Cannot Do Access the internet or external systems Run indefinitely (there are time limits) Access files outside what you've uploaded Install arbitrary packages Access your local computer Size and Time Limits Very large files may take longer to process Complex operations may timeout Some requests may be too resource-intensive Library Availability Common libraries are available (pandas, numpy, matplotlib, etc.) but: Specialized libraries may not be installed Ask the agent what capabilities are available Troubleshooting Code Doesn't Run The agent may not have Code Interpreter enabled Try a different agent with code execution capability Rephrase your request to be more specific Analysis Takes Too Long Try with a smaller sample of data Simplify your request Break complex analysis into steps Results Are Incorrect Check your source data for issues Be more specific about what you want Ask the agent to explain its approach Try rephrasing your question Chart Doesn't Generate Request a specific chart type Ensure the data is suitable for visualization Ask for a different visualization approach Can't Download Generated File Wait for the response to fully complete Click the download link in the message Check your browser's downloads Try asking the agent to regenerate the file Security and Privacy Code runs in an isolated environment Your uploaded data is processed by the agent Generated code is visible in some configurations Follow your organization's data policies Related Topics Uploading Files to a Conversation — Get data into the conversation Downloading Files from a Conversation — Save generated outputs Using the Knowledge Tool — Search document content Viewing Agent Prompts — See how responses are generated"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/using-knowledge-tool.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/using-knowledge-tool.html",
    "title": "Using the Knowledge Tool | FoundationaLLM",
    "summary": "Using the Knowledge Tool Learn how the Knowledge Tool helps agents search and retrieve relevant information from documents and knowledge bases. What Is the Knowledge Tool? The Knowledge Tool enables agents to search through documents and retrieve relevant information to answer your questions. Instead of relying solely on general knowledge, agents with this tool can: Search uploaded files for specific information Query organizational knowledge bases Retrieve relevant excerpts from documents Provide answers grounded in your actual data When the Knowledge Tool Is Active The Knowledge Tool works automatically when: The agent has access to knowledge sources You've uploaded files to the conversation You ask questions about specific content You don't need to explicitly activate it — the agent uses it when relevant to your question. How It Works The Search Process You ask a question about a topic or document The agent searches through available knowledge sources Relevant passages are retrieved from documents The agent synthesizes an answer using the retrieved information Sources may be cited in the response Knowledge Sources The Knowledge Tool can search: Source Type Description Uploaded files Documents you've attached to the conversation Knowledge bases Pre-configured document collections Connected data sources Organizational repositories (SharePoint, data lakes, etc.) What You Can Do Ask Questions About Documents Upload a document and ask: \"What does this report say about Q3 performance?\" \"Summarize the key findings from the uploaded study\" \"What recommendations does the document make?\" \"Find all mentions of [specific term] in this file\" Search Across Multiple Files When you upload several files: \"Compare the conclusions across all uploaded reports\" \"Which document discusses pricing changes?\" \"What do these files say about the timeline?\" Query Knowledge Bases If the agent has access to knowledge bases: \"What's our company policy on [topic]?\" \"Find documentation about [feature]\" \"What procedures should I follow for [task]?\" Get Specific Information Ask detailed questions: \"What was the revenue in Q2 according to the financial report?\" \"Who is mentioned as the project lead in the document?\" \"What date was the policy last updated?\" Example Prompts Document Summarization \"I've uploaded our annual report. Please summarize the executive summary and highlight the three most important metrics.\" Specific Fact Finding \"According to the uploaded employee handbook, how many vacation days do employees receive after their first year?\" Cross-Document Analysis \"Compare the project timelines mentioned in the two uploaded project plans and identify any conflicts.\" Knowledge Base Search \"What does our IT documentation say about setting up VPN access?\" Understanding Responses Grounded Answers When the Knowledge Tool is used: Responses are based on actual document content The agent may quote or paraphrase source material Answers are limited to what's in the knowledge sources When Information Isn't Found If the agent can't find relevant information: It will tell you the information wasn't found It may ask for clarification It might suggest alternative sources or approaches Content Artifacts Some agents show \"Content Artifacts\" — clickable items that show: Source document titles Retrieved passages Metadata about the information source Click on content artifacts to see details about where information came from. Tips for Best Results Ask Specific Questions Instead of: \"Tell me about this document\" Try: \"What are the main conclusions in section 3 of the document?\" Provide Context Mention which document you're asking about (if multiple are uploaded) Specify the type of information you need Include relevant keywords Use Clear File Names When uploading files: Use descriptive names Reference files by name in your questions Organize related files together Start Broad, Then Narrow First: \"What topics does this document cover?\" Then: \"Tell me more about [specific topic] from the document\" Finally: \"What specific recommendations does it make about [detail]?\" Limitations What the Knowledge Tool Cannot Do Search your local computer or systems outside the platform Access documents you haven't uploaded or that aren't in configured sources Read scanned images without OCR (text-as-images) Access real-time or frequently changing data Search Scope Searches only configured knowledge sources May not find information in poorly formatted documents Very large documents may have some sections not indexed Information Accuracy Answers depend on the quality of source documents The agent interprets and summarizes — verify critical information Outdated documents may provide outdated information Troubleshooting Agent Doesn't Find Information If the information should be there: Rephrase your question using different terms Ask what topics the document covers Check if the document uploaded successfully Verify the document format is searchable (not a scanned image) If you're unsure: Ask the agent what files it has access to Try uploading the document again Use simpler, more direct questions Incorrect Information Retrieved Ask for the source of the information Request direct quotes from the document Rephrase to be more specific about what you want Check if the agent is looking at the correct document Knowledge Base Not Available The agent may not have knowledge base access configured Try a different agent with knowledge base capabilities Contact your administrator about available knowledge sources Can't Find Specific Passages The passage may not be indexed correctly Try searching for key unique terms from the passage Ask about the general topic first Consider if the document format allows text extraction Security and Privacy Uploaded files are processed to extract searchable content Knowledge base access is controlled by administrators Search results are limited to what you're authorized to access Follow your organization's document handling policies Differences from Code Interpreter Feature Knowledge Tool Code Interpreter Purpose Search and retrieve Compute and analyze Output Text from documents Calculated results, charts Best for Q&A about content Data analysis File use Search text Process data Use both tools together for powerful document analysis: Upload data files Use Knowledge Tool to understand the context Use Code Interpreter to analyze the data Related Topics Uploading Files to a Conversation — Get documents into the conversation Using the Code Interpreter Tool — Analyze data programmatically Viewing Agent Prompts — See what context is used Downloading Files from a Conversation — Save generated outputs"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/using-other-tools.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/using-other-tools.html",
    "title": "Using Other Tools | FoundationaLLM",
    "summary": "Using Other Tools Learn about additional tools and capabilities that may be available to agents beyond Code Interpreter and Knowledge tools. Understanding Agent Tools Agents in FoundationaLLM can be equipped with various tools that extend their capabilities. While Code Interpreter and Knowledge tools are common, agents may have access to: Image generation tools Custom organizational tools External API integrations Specialized processing capabilities How Tools Work Automatic Tool Selection You don't need to manually select which tool to use: Describe what you need in natural language The agent determines which tool is appropriate The tool executes and returns results Results appear in the conversation Tool Transparency Some agents show information about tool usage: Tool names may appear in responses Processing indicators show when tools are working Results clearly indicate they came from a tool Image Generation Some agents can create images using tools like DALL-E. When to Use Creating illustrations or graphics Visualizing concepts Generating placeholder images Creating custom visuals How to Request Images Be descriptive in your request: \"Create an image of a modern office building at sunset\" \"Generate a diagram showing the project workflow\" \"Design a logo concept with blue and green colors\" Image Results Generated images: Appear directly in the conversation Can be clicked for full-size preview Can be downloaded by right-clicking Tips for Better Images Be specific about style, colors, and composition Mention the intended use (icon, background, illustration) Request revisions by describing what to change Note: photorealistic images may have limitations Custom Organizational Tools Your organization may have deployed custom tools for specific purposes. Examples of Custom Tools Tool Type Purpose Database queries Access internal data systems API integrations Connect to external services Document processing Handle specific document formats Workflow automation Trigger organizational processes Specialized calculations Industry-specific computations Discovering Custom Tools To learn what custom tools an agent has: Ask the agent: \"What tools do you have access to?\" Check the agent description in Settings > Agents Contact your administrator for documentation Using Custom Tools Custom tools work like any other tool: Describe what you need The agent uses the appropriate tool Results appear in the response External API Integrations Some agents can connect to external services: Weather services Stock market data News aggregators Social media platforms Business applications Important Considerations API calls may have rate limits Real-time data depends on API availability Results are as current as the API provides Some integrations may require authentication Specialized Processing Agents may have specialized capabilities: Language Processing Translation between languages Sentiment analysis Text summarization Named entity recognition Document Processing PDF extraction Form parsing Table recognition Metadata extraction Audio/Visual Processing Image analysis and description Audio transcription Visual element detection Tips for Using Tools Effectively Be Specific About What You Need Good: \"Generate a pie chart showing Q3 sales by region\" Not as good: \"Make a chart\" Ask About Capabilities If you're unsure what an agent can do: \"What tools or capabilities do you have?\" \"Can you access [specific system/data]?\" \"What types of files can you process?\" Provide Context Explain why you need something Share relevant background Specify the output format you want Iterate and Refine Start with a basic request Ask for modifications based on results Build on successful outputs Understanding Tool Limitations What Tools Cannot Do Access systems they're not connected to Bypass security controls Perform actions outside their scope Return real-time data without real-time connections Response Time Tool execution may take time depending on: The complexity of the operation External service response times Data processing requirements A loading indicator shows when tools are working. Troubleshooting Tool Doesn't Seem to Work The agent may not have that tool available Try rephrasing your request Ask what capabilities the agent has Try a different agent Unexpected Results Be more specific in your request Ask the agent to explain what it did Try breaking the task into smaller steps Verify your input data Tool Response is Slow Complex operations take longer External APIs may have delays Large data processing requires time Wait for the response to complete Tool Returns an Error Check if your request is within the tool's capabilities Verify any input data is correct Try a simpler version of the request Ask the agent what went wrong Security and Access Tools operate within security boundaries: Access is controlled by administrators User permissions affect what tools can do Sensitive operations may require additional authentication Actions are logged for security auditing Learning More Documentation Check organizational documentation for custom tools Review agent descriptions for capability details Ask your administrator about available integrations Experimentation Try different types of requests Ask agents about their capabilities Test tools with sample data first Related Topics Using the Code Interpreter Tool — Data analysis and visualization Using the Knowledge Tool — Document search and retrieval Selecting an Agent — Choose agents with the right tools Viewing Agent Prompts — Understand how agents work"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/viewing-agent-prompts.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/viewing-agent-prompts.html",
    "title": "Viewing Agent Prompts | FoundationaLLM",
    "summary": "Viewing Agent Prompts Learn how to view the prompts used by agents when generating responses. What Are Agent Prompts? When you send a message to an agent, a \"prompt\" is created that includes: System instructions: The agent's core configuration and persona Context: Relevant information retrieved from knowledge sources Conversation history: Previous messages for continuity Your message: The question or request you submitted Viewing prompts helps you understand how the agent generates its responses. When Prompt Viewing Is Available The \"View Prompt\" feature appears when: Your administrator has enabled prompt viewing The agent is configured to allow prompt visibility The response has finished generating Not all agents expose prompts — this is controlled by configuration. How to View a Prompt Step 1: Find the View Prompt Button Look at the bottom of an agent response (not your own messages) Find the message footer area where buttons appear Look for the View Prompt button (shows a book icon \uD83D\uDCD6) Step 2: Open the Prompt Dialog Click View Prompt A dialog opens titled \"Completion Prompt\" The full prompt text is displayed Step 3: Review the Content The prompt dialog shows the text that was sent to the AI model to generate the response you received. Step 4: Close the Dialog Click Close to dismiss the prompt dialog and return to the conversation. Understanding Prompt Contents System Instructions The beginning of the prompt typically contains: The agent's role and persona Guidelines for how to respond Any restrictions or rules Formatting preferences Retrieved Context If the Knowledge Tool was used: Relevant passages from documents appear Information from knowledge bases is included This provides the \"grounding\" for the response Conversation History Recent messages may be included: Your previous questions The agent's previous answers This maintains continuity Your Current Message Your most recent question or request appears, forming the actual query the agent needs to address. Why View Prompts? Debugging Responses Understanding the prompt helps when: Responses aren't what you expected The agent seems to miss information You want to know what context was used Learning How Agents Work Prompts reveal: How system instructions shape behavior What knowledge was retrieved How conversation context is used Verifying Context Check that: The right documents were searched Relevant information was retrieved The agent had proper context Improving Your Questions See how your input fits into the larger prompt to: Phrase questions more effectively Provide better context upfront Avoid redundant information Prompt Components Explained Component Description Purpose System prompt Agent's base instructions Defines agent behavior and capabilities Knowledge context Retrieved document excerpts Grounds responses in specific information Conversation history Previous messages Maintains coherent dialogue User message Your current input The question being answered When View Prompt Is Not Available You won't see the View Prompt option if: Feature disabled: Your administrator hasn't enabled it Agent configuration: The specific agent doesn't allow prompt viewing Message type: Only agent responses have viewable prompts (not your messages) Still loading: Wait for the response to complete Tips for Using View Prompt Compare Prompts Across Responses View prompts for multiple responses Notice how context changes based on your questions See how conversation history accumulates Check Knowledge Retrieval See what documents were searched Verify relevant passages were found Understand why certain information was used Understand Response Limitations If a response was limited or incomplete: Check if the relevant information was in the prompt See if context was missing Understand what the agent had to work with Verify Agent Behavior Confirm the agent is following its instructions Check that appropriate guidelines are being applied Understand the agent's configured capabilities Troubleshooting View Prompt Button Not Showing The feature may be disabled for your environment The specific agent may not support prompt viewing Wait for the response to fully complete Contact your administrator to request access Prompt Text Is Very Long Prompts can be lengthy, especially with lots of context Scroll through the dialog to see all content The dialog is scrollable Can't Understand the Prompt Prompts may contain technical elements: System instructions are often detailed Formatting may include special tokens Focus on the parts relevant to your question Prompt Seems Incomplete Some prompt components may be summarized Very long contexts may be truncated Technical details may be omitted from the display Privacy Considerations Prompts may contain: System configurations (visible by design) Retrieved document content Previous conversation messages Your user inputs Follow organizational guidelines for handling prompt information. Using Prompts for Better Results Understand What Helps By viewing prompts, you can learn: What type of context improves responses How much conversation history matters What format of questions works best Adjust Your Approach Based on prompt insights: Provide more specific questions Upload relevant documents Reference key terms the agent needs Maintain conversation focus Work Within Limitations If prompts reveal constraints: Work around token limits Provide concise inputs Focus on essential information Related Topics Rating Responses — Provide feedback on agent outputs Using the Knowledge Tool — Understand context retrieval Monitoring Token Consumption — Track prompt sizes Managing Conversations — Control conversation history"
  },
  "docs/chat-user-portal/how-to-guides/using-agents/viewing-status-messages.html": {
    "href": "docs/chat-user-portal/how-to-guides/using-agents/viewing-status-messages.html",
    "title": "Viewing Status Messages | FoundationaLLM",
    "summary": "This article is still being authored. Some sections contain placeholder content that requires additional information. Viewing Status Messages Learn how to view and interact with system status messages in the Chat User Portal. Overview Status messages are announcements from your IT team about the FoundationaLLM system. They inform you about: Planned maintenance windows Current system issues or outages Important updates and changes Service restoration notices Where Status Messages Appear Status messages may appear in several locations within the Chat User Portal: TODO: Document exact display locations once implemented. Banner Messages Important messages may display as a banner at the top of the portal: Banners are visible on all pages They may have different colors based on severity Critical messages may require acknowledgment before proceeding Notification Area Less critical messages may appear in the notification area: Look for a notification icon in the navigation bar A badge indicates unread messages Click to expand and view all notifications Login Page Critical system messages may appear on the login page: These are visible before you sign in Used for major outages or maintenance May include estimated resolution times Message Types Status messages are categorized by severity: Type Appearance Meaning Information Blue/neutral General announcements, tips, or news Warning Yellow/orange Upcoming maintenance, potential issues Error/Outage Red Active problems, service unavailable Success Green Issue resolved, service restored Interacting with Messages Reading Messages Click on a message banner or notification to see full details Some messages expand to show more information Critical messages may link to additional resources Dismissing Messages For informational messages you've read: Click the X or Dismiss button (if available) The message hides from your view It won't reappear unless the IT team publishes it again Note: You cannot dismiss critical warnings or outage notifications. These remain visible until the situation is resolved. Message Persistence Message Type Can Dismiss? Reappears? Information Usually yes No Warning Sometimes May return on next login Error/Outage No Clears when resolved Success Usually yes No Understanding Message Content What Messages Include Good status messages typically contain: Title: Brief summary of the situation Description: Details about what's happening Timing: When it started, expected duration or resolution Impact: What services are affected Actions: What you should do (if anything) Example Messages Planned Maintenance: \"Scheduled Maintenance - Saturday 2:00 AM to 6:00 AM ET. The system will be unavailable during this time.\" Active Issue: \"Service Degradation - Some users may experience slow responses. Our team is investigating.\" Resolution: \"Resolved - The file upload issue reported earlier has been fixed. Normal service is restored.\" What to Do During Outages When the System is Unavailable Don't panic — IT is likely already aware and working on it Check the status message for expected resolution time Save any work in other applications if possible Wait until the maintenance window ends Contact support only if issues persist past the stated time When Experiencing Issues If you're having problems not mentioned in status messages: Refresh the page Clear your browser cache Try a different browser Check your internet connection Contact IT support if problems persist Checking System Status Current Status Look for status messages in the portal to understand current system health. Historical Status TODO: Document if users can access historical status information or a status page. Accessibility Status messages are designed to be accessible: Screen readers announce new messages Messages have appropriate ARIA labels Color is not the only indicator of severity (icons are also used) Keyboard navigation works for dismissing messages Notification Preferences TODO: Document if users can configure notification preferences for status messages. Troubleshooting Message Won't Dismiss The message may be a critical notification that can't be dismissed Try refreshing the page The IT team may have configured it to stay visible Not Receiving Messages Check that notifications are enabled in your browser Verify your browser isn't blocking notifications Status messages require being logged in (for most types) Message Content Unclear Read the full message by clicking to expand Check for links to additional information Contact IT support for clarification Related Topics Managing Conversations — Work with your chat sessions Configuring Accessibility — Accessibility settings including notifications"
  },
  "docs/chat-user-portal/index.html": {
    "href": "docs/chat-user-portal/index.html",
    "title": "Chat User Portal | FoundationaLLM",
    "summary": "Chat User Portal The Chat User Portal is the primary interface for end users to interact with FoundationaLLM agents. What is the Chat User Portal? The Chat User Portal provides a conversational interface where users can: Select and interact with available AI agents Manage conversations and chat history Upload and download files within conversations Use specialized tools like Code Interpreter and Knowledge Search Monitor token consumption Rate agent responses Finding Your User Portal URL See the Quickstart Guide for instructions on finding your User Portal URL based on your deployment type (ACA or AKS). Quick Start Quickstart Guide - Get started with the User Portal Creating Your First Agent - Create and configure your first agent How-To Guides Using Agents Selecting an Agent Managing Available Agents Managing Conversations Configuring Accessibility Working with Files Uploading Files to a Conversation Downloading Files from a Conversation Using Tools Using the Code Interpreter Tool Using the Knowledge Tool Using Other Tools Additional Features Monitoring Token Consumption Rating Agent Responses Copying Prompts & Formatted Results Printing Conversations Viewing Agent Prompts"
  },
  "docs/chat-user-portal/quick-start/creating-first-agent.html": {
    "href": "docs/chat-user-portal/quick-start/creating-first-agent.html",
    "title": "Creating Your First Agent | FoundationaLLM",
    "summary": "Creating Your First Agent This guide walks you through creating your first agent in FoundationaLLM. Note: This guide covers agent creation from the user's perspective. For detailed agent configuration, see the Management Portal - Create New Agent guide. Prerequisites A deployed FoundationaLLM instance Access to the Management Portal (for agent creation) Appropriate permissions Overview Creating an agent in FoundationaLLM involves: Accessing the Management Portal - Navigate to the Management Portal URL Configuring the Agent - Set up agent name, type, knowledge sources, and behavior Testing in User Portal - Use the Chat User Portal to interact with your new agent Quick Steps Log into the Management Portal Navigate to Create New Agent Configure the agent settings (see detailed guide) Save and publish the agent Access the User Portal and select your new agent from the dropdown Next Steps Selecting an Agent Managing Conversations Detailed Agent Creation Guide"
  },
  "docs/chat-user-portal/quick-start/quickstart.html": {
    "href": "docs/chat-user-portal/quick-start/quickstart.html",
    "title": "Quickstart guide | FoundationaLLM",
    "summary": "Quickstart guide After deploying FoundationaLLM, complete the following steps to get started: Configure your deployment's authentication settings. Setup your agents to define a persona and connect to your data sources to generate responses. Find your User Portal URL and log in to start using FoundationaLLM. Find your User Portal (chat UI) URL If you performed an Azure Container Apps (ACA) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Container App resource whose name ends with chatuica. Within the Overview pane, copy the Application Url value. This is the URL for the chat application. If you performed an Azure Kubernetes Service (AKS) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Kubernetes Service resource. Select Properties in the left-hand menu and copy the HTTP application routing domain value. This is the URL for the chat application. When you navigate to the chat UI application, you will be prompted to log in. Find your Core API URL If you performed an Azure Container Apps (ACA) deployment, follow these steps to obtain the URL for the Core API: Within the Resource Group that was created as part of the deployment, select the Container App resource whose name ends with coreca. Within the Overview pane, copy the Application Url value. This is the URL for the Core API. If you performed an Azure Kubernetes Service (AKS) deployment, follow these steps to obtain the URL for the chat UI application: Within the Resource Group that was created as part of the deployment, select the Kubernetes Service resource. Select Properties in the left-hand menu and copy the HTTP application routing domain value. This is the URL for the chat application. Your Core API URL (for your AKS deployment) is the URL you just copied with /core appended to the end of it. For example, if your Core API URL is https://1cf699fd0d89446eabf2.eastus.aksapp.io/, then your Core API URL is https://1cf699fd0d89446eabf2.eastus.aksapp.io/core."
  },
  "docs/development/calling-apis/directly-calling-core-api.html": {
    "href": "docs/development/calling-apis/directly-calling-core-api.html",
    "title": "Directly calling the Core API | FoundationaLLM",
    "summary": "Directly calling the Core API Typically, the only interaction with the FoundationaLLM (FLLM) APIs is indirectly through the User Portal and Management Portal. However, you can also call the APIs directly to perform certain tasks, such as using your configured FLLM agents to perform completions (via the Core API), or updating your branding configurations (via the Management API). API architecture The FLLM architecture contains layers of APIs that are used to perform different tasks along a call chain, starting with the Core API. The following diagram shows a very high-level flow of the API architecture: sequenceDiagram actor U as Caller participant C as CoreAPI participant A as OrchestrationAPI participant O as OrchestrationWrapperAPI U->>C: Calls Orchestration endpoint C->>A: Calls Orchestration endpoint Note over A,O: Agent resolution from cache or hubs A->>+O: Invokes orchestrator Note over O: Calls LangChain or Semantic Kernel O->>-A: Returns result A->>C: Returns result C->>U: Returns result Note The OrchestrationAPI contains a caching layer for the full agent metadata, including the agent, its datasource(s), and prompts. This caching layer is used to improve performance by reducing the number of calls to the underlying hubs. The OrchestrationAPI also includes endpoints to clear the cache across different categories. In the more detailed diagram below, you can see that the OrchestrationAPI calls the AgentHubAPI, PromptHubAPI, and DataSourceHubAPI to retrieve the agent metadata. When we look a level deeper, we see that there are several interactions between the APIs that occur during the call chain. The following diagram shows a more detailed flow of the API architecture: graph TD; A[CoreAPI] -->|1. User Request| B[GatekeeperAPI] -->|Gatekeeper Extensions| BB[GatekeeperIntegrationAPI] A -...->|\"1a. User Request (Bypass Gatekeeper)\"| C[OrchestrationAPI] B ---->|2. Processed Request| C[OrchestrationAPI] C -->|3. Request| E[(AgentHubAPI)] C --->|4. Instantiate Agent| D[[Agent]] D -->|Request| F[(PromptHubAPI)] D -->|Request| G[(DataSourceHubAPI)] E -->|Metadata| C F -->|Metadata| D G -->|Metadata| D D -->|Hydrated Agent| C D -->|5. Composed Information| H[OrchestrationWrapperAPI] H -->|6. Response| D C -->|7. Response| B C -...->|\"7a. Response (Bypass Gatekeeper)\"| A B -->|8. Final Response| A Note Notice that there is an alternate path that bypasses the Gatekeeper API. This path is used when the FoundationaLLM:APIs:CoreAPI:BypassGatekeeper configuration value is set to true. By default, the Core API bypasses the Gatekeeper API. To override this behavior and enable the Gatekeeper API, set this value to true. Bypassing the Gatekeeper means that you bypass content protection and filtering in favor of improved performance. Postman collection The ability to test the API endpoints of FoundationaLLM is a critical part of the development process. Postman is a tool that allows you to do just that. This document will walk you through the process of setting up Postman to work with FoundationaLLM. Tip To find the Core API URL for your deployment, you can retrieve it from your App Configuration resource in the portal by viewing the FoundationaLLM:APIs:CoreAPI:APIUrl configuration value. Alternatively, follow the instructions in the Quickstart guide to find the Core API URL. To see the API endpoints available in FoundationaLLM, you can get your Core API endpoint from your App Configuration resource in the portal and add /swagger/ to the end of it. For example, if your Core API endpoint is https://fllmaca002coreca.graybush-c554b849.eastus.azurecontainerapps.io, then you would navigate to https://fllmaca002coreca.graybush-c554b849.eastus.azurecontainerapps.io/swagger/ to see the API endpoints. Note The example link above is for a Quick Start deployment of FoundationaLLM, which deploys the APIs to Azure Container Apps (ACA). If you are using the standard deployment that deploys the APIs to Azure Kubernetes Service (AKS), then you cannot currently access the Swagger UI for the APIs. However, you will be able to obtain the OpenAPI swagger.json file from the Core API endpoint by navigating to https://{{AKS URL}}/core/swagger/v1/swagger.json. Install Postman If you don't have Postman installed on your machine, visit the Postman website and download the app. Once you have it installed, Create a Blank Workspace. Import the Postman collection First, select the button below to fork and import the Postman collection for the Core API. Select Fork Collection to create a fork and import the collection into your Postman workspace. Within the dialog that displays, enter your fork label, select the Postman workspace into which you want to create the fork, optionally check the Watch original collection checkbox to receive updates to the original collection, and then select Fork collection. You will now see the FoundationaLLM.Core.API collection in your Postman workspace. Set up the Postman environment variables The Postman collection you imported contains a number of API endpoints that you can use to test the Core API. However, before you can use these endpoints, you need to set up a Postman environment variables within your collection that contains the Core API URL, agent hint value, and other variables. We will set up your authentication token in the next section. Select the FoundationaLLM.Core.API collection in the left-hand menu. Select the Variables tab. Tip The https://localhost:63279 value is the default value for the Core API URL (baseUrl variable) when debugging it locally. You can leave this value as-is if you are testing locally, or you can replace it with the Core API URL for your deployment. Note The Initial value column is the value that will be used when you first import the collection. The Current value column is the value that will be used when you run the collection. If you change the Current value column, the Initial value column will not be updated. For the steps that follow, you will be updating the Current value column. Update the baseUrl variable Current value with the Core API URL for your deployment. Fill out the tenantId, appClientId, and appScope Current value settings for your Core Client Entra ID app registration (setup instructions). Use the list below the screenshot to find the values. tenantId: The tenant ID of your Core Client (Chat UI) Entra ID app. You can find this value in the Overview tab of your Entra ID app in the portal. appClientId: The client ID of your Core Client Entra ID app. You can find this value in the Overview tab of your Entra ID app in the portal. appScope: The scope of your Core Client Entra ID app. You can find this value in the Api Permissions section of your Entra ID app in the portal. Select the Save button in the top right corner of the Variables pane to save your changes. Set up the Postman authentication token There are two ways to obtain the authentication token that you will use to authenticate your API calls: Configure the Postman collection authorization token (recommended) Though this method takes a few more steps, it is the recommended method because it allows you to use the same token for all of the API calls in the collection. Important If you previously configured the Microsoft Entra ID app registration for the Chat UI application (Core Client), you will need to update the Redirect URI to https://oauth.pstmn.io/v1/callback in order to use the Postman mobile app to get the token. You can do this by following the steps in the Add a redirect URI to the client application section of the authentication setup guide. Select the Authorization tab within the FoundationaLLM.Core.API collection. You will see the Authorization tab at the collection level. This means that you can configure the token at the collection level and use it for all of the requests in the collection. Notice that the Token Name is FLLM CoreAPI Token. This will automatically be used by the requests in the collection. Note All of the values are pre-filled and use the variables that you set up in the previous section. You do not need to change any values at this time. Scroll down to the bottom of the page and click on Get New Access Token. This will open a new window in your browser and will ask you to login with your credentials. Once you login, you will be asked to consent to the permissions that you specified in the Scope field. Click on Accept to consent to the permissions. You will then be redirected to the callback url that you specified in the Callback Url field. This will close the browser window and will take you back to Postman. You should now see the token in the Authorization tab. Click on Use Token to use the token in the collection. Important Be sure to click the Save button in the top right corner of the Postman app to save your changes. Now you are ready to make your first CoreAPI request. Within the FoundationaLLM.Core.API collection, select the Sessions GET request under the sessions folder. When you select the Authorization tab, notice that the selected type is Inherit auth from parent. This means that the request will use the token that you configured at the collection level. Also notice that the {{baseUrl}} variable is used in the Request Url field. This means that the request will use the Core API URL that you configured at the collection level. Select the Send button to send the request. Even if you do not have any chat sessions in your system, you should receive a successful response (200) from the Core API. Now you can use the same token to test any other request in the collection with ease. Obtain the authentication token from the User Portal (not recommended) As an alternative to saving the authentication token at the collection level, you can obtain the token from the User Portal and save it at the request level. This method is not recommended because it requires you to obtain a new token for each request that you want to make, and the token expires after a certain amount of time. Navigate to the User Portal and log in. Open the browser's developer tools (F12), select the Network tab, refresh the page, and copy the value of the token under the XHR tab from any of the API calls that are made to the Core API. Within the FoundationaLLM.Core.API collection, select the Sessions GET request under the sessions folder. Select the Authorization tab, select Bearer Token as the type, and paste the token value into the Token field. Select Send to send the request. Even if you do not have any chat sessions in your system, you should receive a successful response (200) from the Core API."
  },
  "docs/development/calling-apis/directly-calling-management-api.html": {
    "href": "docs/development/calling-apis/directly-calling-management-api.html",
    "title": "Directly calling the Management API | FoundationaLLM",
    "summary": "Directly calling the Management API This guide provides steps for importing and configuring the Postman collection for the FoundationaLLM Management API. The Management API is used to manage the FoundationaLLM system, including creating and managing agents, data pipelines, knowledge sources, and more. Once you configure the Postman collection, including authentication, follow the instructions in the Management Portal documentation to perform various operations using the Management API. Postman collection The ability to test the API endpoints of FoundationaLLM is a critical part of the development process. Postman is a tool that allows you to do just that. This document will walk you through the process of setting up Postman to work with FoundationaLLM. Tip To find the Management API URL for your deployment, you can retrieve it from your App Configuration resource in the portal by viewing the FoundationaLLM:APIs:ManagementAPI:APIUrl configuration value. To see the API endpoints available in FoundationaLLM, you can get your Management API endpoint from your App Configuration resource in the portal and add /swagger/ to the end of it. For example, if your Management API endpoint is https://fllmaca729managementca.icelake-c554b849.eastus.azurecontainerapps.io, then you would navigate to https://fllmaca729managementca.icelake-c554b849.eastus.azurecontainerapps.io/swagger/ to see the API endpoints. Note The example link above is for a Quick Start deployment of FoundationaLLM, which deploys the APIs to Azure Container Apps (ACA). If you are using the standard deployment that deploys the APIs to Azure Kubernetes Service (AKS), then you cannot currently access the Swagger UI for the APIs. However, you will be able to obtain the OpenAPI swagger.json file from the Management API endpoint by navigating to https://{{AKS URL}}/management/swagger/v1/swagger.json. Install Postman If you don't have Postman installed on your machine, visit the Postman website and download the app. Once you have it installed, Create a Blank Workspace. Import the Postman collection First, select the button below to fork and import the Postman collection for the Management API. Select Fork Collection to create a fork and import the collection into your Postman workspace. Within the dialog that displays, enter your fork label, select the Postman workspace into which you want to create the fork, optionally check the Watch original collection checkbox to receive updates to the original collection, and then select Fork collection. You will now see the FoundationaLLM.Management.API collection in your Postman workspace. Set up the Postman environment variables The Postman collection you imported contains a number of API endpoints that you can use to test the Management API. However, before you can use these endpoints, you need to set up a Postman environment variables within your collection that contains the Management API URL and other variables. We will set up your authentication token in the next section. Select the FoundationaLLM.Management.API collection in the left-hand menu. Select the Variables tab. Note The Initial value column is the value that will be used when you first import the collection. The Current value column is the value that will be used when you run the collection. If you change the Current value column, the Initial value column will not be updated. For the steps that follow, you will be updating the Current value column. Update the baseUrl variable Current value with the Management API URL for your deployment. Update the instanceId variable Current value with the instance ID of your FoundationaLLM deployment. You can find the instance ID in the FoundationaLLM:Instance:Id Azure App Configuration property. Fill out the tenantId, appClientId, and appScope Current value settings for your Management Client Entra ID app registration (setup instructions). Use the list below the screenshot to find the values. tenantId: The tenant ID of your Management Client (Management Portal) Entra ID app. You can find this value in the Overview tab of your Entra ID app in the portal. appClientId: The client ID of your Management Client Entra ID app. You can find this value in the Overview tab of your Entra ID app in the portal. appScope: The scope of your Management Client Entra ID app. You can find this value in the Api Permissions section of your Entra ID app in the portal. Select the Save button in the top right corner of the Variables pane to save your changes. Set up the Postman authentication token Configure the Postman collection authorization token Complete these steps to configure Postman to use the same token for all of the API calls in the collection. Important If you previously configured the Microsoft Entra ID app registration for the Management Client (UI) application, you will need to update the Redirect URI to https://oauth.pstmn.io/v1/callback in order to use the Postman mobile app to get the token. You can do this by following the steps in the Add a redirect URI to the client application section of the authentication setup guide. Select the Authorization tab within the FoundationaLLM.Management.API collection. You will see the Authorization tab at the collection level. This means that you can configure the token at the collection level and use it for all of the requests in the collection. Notice that the Token Name is FLLM Management Token. This will automatically be used by the requests in the collection. Note All of the values are pre-filled and use the variables that you set up in the previous section. You do not need to change any values at this time. Scroll down to the bottom of the page and click on Get New Access Token. This will open a new window in your browser and will ask you to login with your credentials. Once you login, you will be asked to consent to the permissions that you specified in the Scope field. Click on Accept to consent to the permissions. You will then be redirected to the callback url that you specified in the Callback Url field. This will close the browser window and will take you back to Postman. You should now see the token in the Authorization tab. Click on Use Token to use the token in the collection. Important Be sure to click the Save button in the top right corner of the Postman app to save your changes. Now you are ready to make your first ManagementAPI request. Within the FoundationaLLM.Management.API collection, select the Get Agents GET request. When you select the Authorization tab, notice that the selected type is Inherit auth from parent. This means that the request will use the token that you configured at the collection level. Also notice that the {{baseUrl}} and {{instanceId}} variables is used in the Request Url field. This means that the request will use the Management API URL and Instance Id that you configured at the collection level. Select the Send button to send the request. Even if you do not have any agents sessions in your system, you should receive a successful response (200) from the Management API. Now you can use the same token to test any other request in the collection with ease."
  },
  "docs/development/calling-apis/index.html": {
    "href": "docs/development/calling-apis/index.html",
    "title": "Directly calling APIs | FoundationaLLM",
    "summary": "Directly calling APIs Although FoundationaLLM provides user interfaces for most of the operations, you can also call the APIs directly. This is useful when you want to automate some operations or when you want to integrate FoundationaLLM with other systems. The links below provide the documentation for calling the Core and Management APIs, respectively: Core API Management API"
  },
  "docs/development/calling-apis/standard-deployment-local-api-access.html": {
    "href": "docs/development/calling-apis/standard-deployment-local-api-access.html",
    "title": "Standard Deployment Local API Access | FoundationaLLM",
    "summary": "Standard Deployment Local API Access Standard deployments expose backend services internally, preventing API access over the public internet. Using the kubectl CLI, however, it is possible to forward FoundationaLLM APIs deployed within Kubernetes for local consumption. kubectl Forwarding Script Prerequisites kubectl with the kubelogin extension Both of these utilities can be installed by the Azure CLI: az aks install-cli. If you use this command, you will need to restart your terminal to reflect the changes to $PATH. Kubernetes credentials stored in $HOME/.kube/config Obtain these credentials using the Azure CLI. az login az aks get-credentials --name MyManagedCluster --resource-group MyResourceGroup Script Navigate to /deploy/standard/scripts/Kubectl-Proxy.ps1 or copy the following PowerShell script to your local environment. Before running it, ensure that no applications are running on ports 5000-5010. To stop the tunnels, press any key in the terminal context where you started the script. #!/bin/pwsh Set-PSDebug -Trace 0 # Echo every command (0 to disable, 1 to enable, 2 to enable verbose) Set-StrictMode -Version 3.0 $ErrorActionPreference = \"Stop\" $services = @{ \"foundationallm-orchestration-api\" = 5000 \"foundationallm-gatekeeper-api\" = 5001 \"foundationallm-agent-hub-api\" = 5002 \"foundationallm-core-api\" = 5003 \"foundationallm-data-source-hub-api\" = 5004 \"foundationallm-gatekeeper-integration-api\" = 5005 \"foundationallm-langchain-api\" = 5006 \"foundationallm-management-api\" = 5007 \"foundationallm-prompt-hub-api\" = 5008 \"foundationallm-semantic-kernel-api\" = 5009 \"foundationallm-vectorization-api\" = 5010 } $jobIds = @() try { foreach ($servicePortPairing in $services.GetEnumerator()) { Write-Host \"Starting Kubectl Tunnel for $($servicePortPairing.key)\" $job = Start-Job -ScriptBlock ([scriptblock]::Create(\"kubectl port-forward service/$($servicePortPairing.key) $($servicePortPairing.value):80\")) Write-Host \"Job: $($job.Command)\" $jobIds += $job.Id } Write-Host \"Press any key to kill the Kubernetes tunnels...\" $Host.UI.RawUI.ReadKey(\"NoEcho,IncludeKeyDown\") } catch {} finally { foreach ($jobId in $jobIds) { Write-Host \"Killing $jobId\" Stop-Job -Id $jobId } } Note You will need to rerun the script if you restart any nodes while the script is running. Verification Run the following script to ensure that all APIs are accessible through kubectl forwarding. #!/bin/pwsh Set-PSDebug -Trace 0 # Echo every command (0 to disable, 1 to enable, 2 to enable verbose) Set-StrictMode -Version 3.0 $ErrorActionPreference = \"Stop\" foreach ($servicePort in 5000..5010) { Write-Host \"Testing Port #$servicePort...\" curl \"http://localhost:$servicePort/status\" }"
  },
  "docs/development/contributing/git-workflow.html": {
    "href": "docs/development/contributing/git-workflow.html",
    "title": "Git workflow | FoundationaLLM",
    "summary": "Git workflow The general process for working with FoundationaLLM is: Fork on GitHub Clone your fork locally Configure the upstream repo (git remote add upstream git://github.com/foundationallm/foundationallm) Switch to the default branch (e.g. main, using git checkout main) Create a local branch from that (git checkout -b myBranch) Work on your feature Rebase if required (see below) Push the branch up to GitHub (git push origin myBranch) Send a Pull Request on GitHub - the PR should target (have as base branch) the default branch (e.g. main). You should never work on a clone of the default branch, and you should never send a pull request from it - always from a branch. The reasons for this are detailed below. Learning Git Workflow For an introduction to Git, check out GitHub Guides. For more information about GitHub Flow, please head over to the Understanding the GitHub Flow illustrated guide, both by the awesome people at GitHub. Handling Updates from the default branch While you're working away in your branch, it's quite possible that the upstream target branch may be updated. If this happens you should: Stash any uncommitted changes you need to git checkout vX_Y_Z git pull upstream main git rebase main myBranch git push origin main - (optional) this this makes sure your remote main branch is up to date This ensures that your history is \"clean\" i.e. you have one branch off from main followed by your changes in a straight line. Failing to do this ends up with several \"messy\" merges in your history, which we don't want. This is the reason why you should always work in a branch and you should never be working in, or sending pull requests from, main. Rebasing public commits is very problematic, which is why we require you to rebase any updates from upstream/main. If you're working on a long running feature then you may want to do this quite often, rather than run the risk of potential merge issues further down the line. Sending a Pull Request While working on your feature you may well create several branches, which is fine, but before you send a pull request you should ensure that you have rebased back to a single \"Feature branch\" - we care about your commits, and we care about your feature branch; but we don't care about how many or which branches you created while you were working on it :) When you're ready to go you should confirm that you are up to date and rebased with upstream (see \"Handling Updates from the default branch\" above), and then: git push origin myBranch Send a descriptive Pull Request on GitHub - making sure you have selected the correct branch in the GitHub UI!"
  },
  "docs/development/contributing/index.html": {
    "href": "docs/development/contributing/index.html",
    "title": "Contributing to FoundationaLLM | FoundationaLLM",
    "summary": "Contributing to FoundationaLLM We are happy to accept contributions to the FoundationaLLM project in the form of feedback, bug reports, and pull requests. If you want to make code contributions, please be sure to branch from the head of the default branch. For GitHub workflow guidance, please review the Git workflow document. Style guide Please abide by the style guide when contributing to the project."
  },
  "docs/development/contributing/repro.html": {
    "href": "docs/development/contributing/repro.html",
    "title": "Bug report reproduction guide | FoundationaLLM",
    "summary": "Bug report reproduction guide When customers plan to report an issue with FoundationaLLM, we will most likely ask them to provide a so called minimal reproduction project (repro). This serves two purposes: It helps issue reporters validate their assumptions by trying to recreate the behavior in a new project. It helps eliminate ambiguity and speeds up investigations. We may also be able to provide workarounds in certain scenarios. This document describes what a minimal repro project is, and why it's important to us. What is a minimal repro project? A repro (or a reproduction) is a project, which can be used to reproduce the reported behavior with minimal effort from a product team, which has the minimum code required to demonstrate the concerning behavior. Please provide a project hosted in a public GitHub repository as described below: Create a new project with the simplest structure you can get by with to reproduce your issue. Add the minimum amount of code necessary to reproduce the behavior you are reporting on this newly created project. Make sure you do not add any dependencies that are irrelevant to the behavior. Include sample data, if relevant. Host the project as a public repository on GitHub. Make sure you haven't included any binaries in your project (this is usually about the bin and obj folders in your project) Note: that this step is important and we won't be able to open zip attachments in your issues. Zip files are potential attack vectors that we try to avoid at all cost. Important considerations Never include any sensitive information in your reproduction project. Never include any code that is not intended to be public in a repro. Do not reference any external services or data sources."
  },
  "docs/development/contributing/style-guide.html": {
    "href": "docs/development/contributing/style-guide.html",
    "title": "Style guide | FoundationaLLM",
    "summary": "Style guide FoundationaLLM is a growing project with many contributors. To ensure that the code is consistent and easy to read, we have created this style guide. Please follow these guidelines when contributing to the project. General The solution contains different languages and frameworks. The following guidelines apply to all of them: Simple, clean, and readable is always better Dependency Injection is the name of the game Avoid passing data structures around, always use strongly-typed classes Use the latest language features Use well-established design patterns Always think about the bigger picture (how will my choice potentially impact everyone else?) When introducing a new pattern/approach in your code, share with the rest of the team Measure 10 times, cut once (i.e. design before you code) Primarily, we strive to use a uniform approach across all the modules of the platform. Commenting Use comments to explain why you are doing something, not what you are doing. Apply comments to all public classes, methods, and properties. Comments should be written in complete sentences with proper casing and punctuation. Add a space after the comment delimiter (//, #, etc.). Do: // If the Azure Cognitive Search index does not exists, create the index. Don't: //Create index C# Follow the C# identifier naming rules and conventions. Follow the standard C# coding conventions. Python Abide by the Python naming and coding rules (PEP (Python Enhancement Proposal) 8)."
  },
  "docs/development/development-approach.html": {
    "href": "docs/development/development-approach.html",
    "title": "FoundationaLLM DevOps rules and guidelines | FoundationaLLM",
    "summary": "FoundationaLLM DevOps rules and guidelines FoundationaLLM adheres to the trunk-based development philosophy. If you are a member of the FoundationaLLM core development team or you want to contribute to the code, we strongly encourage you to go through the following materials: Microsoft's Azure DevOps team branching strategy How Microsoft develops modern software with DevOps DORA's research program (Google) The state of DevOps - 2021 edition (DORA, Google) Trunk-based development (Google Cloud Architecture Center) Trunk-based development e-book by Paul Hammant Our move to trunk-based development is driven by the vision to build and deliver the FoundationaLLM solution using a solid DevOps process centered on a Git-centric release flow. How? The core rules of development: When a developer starts working on a feature or a bug fix, a new branch gets created from main. When the work ends, a PR is created to bring the work back into main. We encourage a granular approach, where each individual feature or bug fix has it's own branch. Work should be moved back into the trunk via a PR as soon as possible. NOTE: Committing directly into main is not allowed. All changes are merged via PRs which allows for consistent review, validation, and testing processes. When a sprint ends, a release branch is created. The release branch will be used to deploy the changes to production. Work on main ca resume right after the release branch was created. The release branch will live until the next release branch is created. In the case of critical bug fixes, the process will follow the same approach as described above at step 1. Once the PR is approved and the code merged to main, it will also be cherry-picked into the current release branch. From there, it will be deployed into production. NOTE: This approach ensures that no matter how critical a bug fix is and no matter how much pressure there is to fix an issue in production, the code for the fix always ends up in main. Why? Analysis of DevOps Research and Assessment (DORA) data from 2016, 2017 shows that teams achieve higher levels of software delivery and operational performance (delivery speed, stability, and availability) if they follow these practices: Have three or fewer active branches in the application's code repository. Merge branches to trunk at least once a day. Don't have code freezes and don't have integration phases. Common pitfalls Some common obstacles to full adoption of trunk-based development include the following: An overly heavy code-review process. Many organizations have a heavyweight code review process that requires multiple approvals before changes can be merged into trunk. When code review is laborious and takes hours or days, developers avoid working in small batches and instead batch up many changes. This in turn leads to a downward spiral where reviewers procrastinate with large code reviews due to their complexity. Consequently, merge requests often languish because developers avoid them. Because it is hard to reason about the impact of large changes on a system through inspection, defects are likely to escape the attention of reviewers, and the benefits of trunk-based development are diminished. Performing code reviews asynchronously. If your team practices pair programming, then the code has already been reviewed by a second person. If further reviews are required, they should be performed synchronously: when the developer is ready to commit the code, they should ask somebody else on the team to review the code right then. They should not ask for asynchronous review—for example, by submitting a request into a tool and then starting on a new task while waiting for the review. The longer a merge is delayed, the more likely it is to create merge conflicts and associated issues. Implementing synchronous reviews requires the agreement of the team to prioritize reviewing each others' code over other work. Not running automated tests before committing code. In order to ensure trunk is kept in a working state, it's essential that tests are run against code changes before commit. This can be done on developer workstations, and many tools also provide a facility to run tests remotely against local changes and then commit automatically when they pass. When developers know that they can get their code into trunk without a great deal of ceremony, the result is small code changes that are easy to understand, review, test, and which can be moved into production faster. Actions to take Develop in small batches (even smaller than we are used to) Speed up code review so that commits do not need to wait long times to get into main. Have comprehensive, automated testing Have a fast build Metrics Metric Goal Number of active branches (not including the ones developers create for their tasks) Three or less Code freeze periods (merge, stabilization, etc.) No code freeze Frequency of merging branches/forks to trunk At least once per day Time spent in code review (includes time waiting for code review) Average code review time per PR less than 30 min We strongly encourage you to go through the following: A synthetic description of the journey of our goals - sourced from https://devops.paulhammant.com/trunk-correlated-practices-chart/ A tool to calculate the DevOps performance our our team - sourced from https://paulhammant.com/2021/10/08/software-project-guidelines/ Feature flags To be used extensively to control the stability of the releases in production."
  },
  "docs/development/development-local.html": {
    "href": "docs/development/development-local.html",
    "title": "Configure local development environment | FoundationaLLM",
    "summary": "Configure local development environment Prerequisites Environment variables: Create a system environment variable for the Application Configuration Service connection string named FoundationaLLM_AppConfig_ConnectionString. This is used by the .NET projects. Create a system environment variable for the Application Configuration Service URI named FOUNDATIONALLM_APP_CONFIGURATION_URI. This is used by the Python projects. Create a system environment variable named FOUNDATIONALLM_VERSION and set it to the version of the FoundationaLLM deployment you are working with. This is used by the .NET projects to validate your environment configuration based on the version. Tip You can view the FoundationaLLM release versions by viewing the branches in the FoundationaLLM repository. The format is release/n.n.n, where n.n.n is the version number. The FOUNDATIONALLM_VERSION environment variable should be set to the version number without the release/ prefix (example: 0.4.0). Follow the instructions in Configure access control for services to grant your user account access to the Azure App Configuration and Key Vault services. You may need an Azure admin to perform these steps on your behalf. Backend (APIs and worker services): Visual Studio 2022 17.8 or later (required for passthrough Visual Studio authentication for the Docker container and .NET 8 support) with the Python workload installed .NET 8 SDK or greater Python 3.11 (learn more about Python environments in Visual Studio) Docker Desktop (with WSL for Windows machines) (Mac install or Windows install) Azure CLI (v2.51.0 or greater) Microsoft Azure PowerShell Helm 3.11.1 or greater Frontend (Vue.js (Nuxt) web app) Visual Studio Code (recommended for development) Node.js v18.0.0 or newer NPM v10.2.3 or newer Recommended way to install the latest version of NPM and node.js on Windows: Install NVM from https://github.com/coreybutler/nvm-windows Run nvm install latest Run nvm list (to see the versions of NPM/node.js available) Run nvm use latest (to use the latest available version) Setup RBAC permissions when running locally When you run the solution locally, you will need to set role-based access control (RBAC) permissions on the Azure Cosmos DB account. You can do this by running the following command in the Azure Cloud Shell or Azure CLI: Assign yourself to the \"Cosmos DB Built-in Data Contributor\" role: az cosmosdb sql role assignment create --account-name YOUR_COSMOS_DB_ACCOUNT_NAME --resource-group YOUR_RESOURCE_GROUP_NAME --scope \"/\" --principal-id YOUR_AZURE_AD_PRINCIPAL_ID --role-definition-id 00000000-0000-0000-0000-000000000002 UI User Portal The UserPortal project is a Vue.js (Nuxt) project. To configure it to run locally, follow these steps: Open the /src/ui/UserPortal folder in Visual Studio Code. Copy the .env.example file in the root directory to a new file named .env and update the values: The APP_CONFIG_ENDPOINT value should be the Connection String for the Azure App Configuration service. This should be the same value as the FoundationaLLM_AppConfig_ConnectionString environment variable. The LOCAL_API_URL should be the URL of the local Core API service (https://localhost:63279). Important: Only set this value if you wish to debug the entire solution locally and bypass the App Config service value for the CORE API URL. If you do not wish to debug the entire solution locally, leave this value empty or comment it out. Management Portal The ManagementPortal project is a Vue.js (Nuxt) project. To configure it to run locally, follow these steps: Open the /src/ui/ManagementPortal folder in Visual Studio Code. Copy the .env.example file in the root directory to a new file named .env and update the values: The APP_CONFIG_ENDPOINT value should be the Connection String for the Azure App Configuration service. This should be the same value as the FoundationaLLM_AppConfig_ConnectionString environment variable. The LOCAL_API_URL should be the URL of the local Management API service (https://localhost:63267). Important: Only set this value if you wish to debug the entire solution locally and bypass the App Config service value for the MANAGEMENT API URL. If you do not wish to debug the entire solution locally, leave this value empty or comment it out. .NET projects Core API Core API app settings Make sure the contents of the appsettings.json file has this structure and similar values: { \"DetailedErrors\": true, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } }, \"AllowedHosts\": \"*\", \"FoundationaLLM\": { \"AppConfig\": { \"ConnectionString\": \"\" } } } Create the appsettings.Development.json file or update it with the following content and replace all <...> placeholders with the values from your deployment: { \"FoundationaLLM\": { \"APIs\": { \"GatekeeperAPI\": { \"APIUrl\": \"<...>\" // Default local value: https://localhost:7180/ }, , \"OrchestrationAPI\": { \"APIUrl\": \"<...>\" // Default local value: \"https://localhost:7324/\" } } } } CoreWorker The CoreWorker project is a .NET worker service that acts as the Cosmos DB change feed processor. When you debug it locally, it runs within a Docker container. Because of this, it is important to make sure the App Configuration service connection string is set in the appsettings.Development.json file. This is because the Docker container will not have access to the environment variable. CoreWorker app settings Make sure the contents of the appsettings.json file has this structure and similar values: { \"DetailedErrors\": true, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } }, \"FoundationaLLM\": { \"AppConfig\": { \"ConnectionString\": \"\" } } } Create the appsettings.Development.json file or update it with the following content and replace all <...> placeholders with the values from your deployment: { \"FoundationaLLM\": { \"AppConfig\": { \"ConnectionString\": \"<...>\" } } } Gatekeeper API Gatekeeper API app settings Make sure the contents of the appsettings.json file has this structure and similar values: { \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" }, \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } } }, \"AllowedHosts\": \"*\", \"FoundationaLLM\": { \"AppConfig\": { \"ConnectionString\": \"\" } } } Create the appsettings.Development.json file or update it with the following content and replace all <...> placeholders with the values from your deployment: { \"FoundationaLLM\": { \"APIs\": { \"OrchestrationAPI\": { \"APIUrl\": \"<...>\" // Default local value: https://localhost:7324/ }, \"GatekeeperIntegrationAPI\": { \"APIUrl\": \"<...>\" // Default local value: http://localhost:8042/ } } } } Orchestration API Orchestration API app settings Make sure the contents of the appsettings.json file has this structure and similar values: { \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" }, \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } } }, \"AllowedHosts\": \"*\", \"FoundationaLLM\": { \"AppConfig\": { \"ConnectionString\": \"\" } } } Create the appsettings.Development.json file or update it with the following content and replace all <...> placeholders with the values from your deployment: { \"FoundationaLLM\": { \"APIs\": { \"LangChainAPI\": { \"APIUrl\": \"<...>\" // Default local value: http://localhost:8765/ }, \"SemanticKernelAPI\": { \"APIUrl\": \"<...>\" // Default local value: https://localhost:7062/ }, \"AgentHubAPI\": { \"APIUrl\": \"<...>\" // Default local value: http://localhost:8742/ }, \"PromptHubAPI\": { \"APIUrl\": \"<...>\" // Default local value: http://localhost:8642/ }, \"DataSourceHubAPI\": { \"APIUrl\": \"<...>\" // Default local value: http://localhost:8842/ } } } } Semantic Kernel API Semantic Kernel API app settings Make sure the contents of the appsettings.json file has this structure and similar values: { \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\", \"Microsoft.SemanticKernel\": \"Error\" }, \"ApplicationInsights\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\", \"Microsoft.SemanticKernel\": \"Error\" } } }, \"AllowedHosts\": \"*\", \"FoundationaLLM\": { \"CognitiveSearch\": { \"IndexName\": \"vector-index\", \"MaxVectorSearchResults\": 10 }, \"OpenAI\": { \"CompletionsDeployment\": \"completions\", \"CompletionsDeploymentMaxTokens\": 8096, \"EmbeddingsDeployment\": \"embeddings\", \"EmbeddingsDeploymentMaxTokens\": 8191, \"ChatCompletionPromptName\": \"RetailAssistant.Default\", \"ShortSummaryPromptName\": \"Summarizer.TwoWords\", \"PromptOptimization\": { \"CompletionsMinTokens\": 50, \"CompletionsMaxTokens\": 300, \"SystemMaxTokens\": 1500, \"MemoryMinTokens\": 1500, \"MemoryMaxTokens\": 3000, \"MessagesMinTokens\": 100, \"MessagesMaxTokens\": 3000 } }, \"DurableSystemPrompt\": { \"BlobStorageContainer\": \"prompts\" }, \"CognitiveSearchMemorySource\": { \"IndexName\": \"vector-index\", \"ConfigBlobStorageContainer\": \"memory-source\", \"ConfigFilePath\": \"ACSMemorySourceConfig.json\" }, \"BlobStorageMemorySource\": { \"ConfigBlobStorageContainer\": \"memory-source\", \"ConfigFilePath\": \"BlobMemorySourceConfig.json\" }, \"SemanticKernelOrchestration\": { \"APIKeySecretName\": \"foundationallm-semantickernel-api-key\" } } } Create the appsettings.Development.json file or update it with the following content and replace all <...> placeholders with the values from your deployment: { \"FoundationaLLM\": { \"CognitiveSearch\": { \"Endpoint\": \"https://<...>-cog-search.search.windows.net\", \"Key\": \"<...>\" }, \"OpenAI\": { \"Endpoint\": \"https://<...>-openai.openai.azure.com/\", \"Key\": \"<...>\" }, \"DurableSystemPrompt\": { \"BlobStorageConnection\": \"<...>\" }, \"CognitiveSearchMemorySource\": { \"Endpoint\": \"https://<...>-cog-search.search.windows.net\", \"Key\": \"<...>\", \"ConfigBlobStorageConnection\": \"<...>\" }, \"BlobStorageMemorySource\": { \"ConfigBlobStorageConnection\": \"<...>\" } } } Python projects Python Environment Variables Create local environment variables named: FOUNDATIONALLM_APP_CONFIGURATION_URI. The value should be the URI of the Azure App Configuration service and not the connection string. We use role-based access controls (RBAC) to access the Azure App Configuration service, so the connection string is not required. FOUNDATIONALLM_ENV: This is required by the OperationsManager to disable the verify setting on requests. By setting the value to dev, it allows calls to the State API from LangChain and other Python-based APIs when running locally (FOUNDATIONALLM_ENV = dev) by disabling the check for a valid SSL cert on requests. This is only necessary when running the State API locally. Otherwise, the setting should be set to prod. Name Value Description FOUNDATIONALLM_APP_CONFIGURATION_URI REDACTED Azure App Configuration URI FOUNDATIONALLM_ENV dev or prod Environment specification. Acceptable values are dev and prod. Defaults to prod if not set. Running the solution locally Configure and run the backend components The backend components consist of the .NET projects and the Python projects. The .NET projects are all ASP.NET Core Web API projects. The Python projects are all FastAPI projects. Open the solution in Visual Studio 2022 17.6 or later. The solution file is located at /src/FoundationaLLM.sln. Reference the API sections above to configure the app settings for each project. This primarily involves just creating the appsettings.development.json file for each of the .NET (located under the dotnet solution folder) API projects and adding the documented values within. For local development, use the localhost URLs for each of the API projects. Note The appsettings.development.json files are excluded from source control. This is to prevent sensitive information from being committed to source control. You will need to create these files locally. Expand the python solution folder. Expand the AgentHubAPI project, then expand the Python Environments folder underneath it. You will likely see a warning icon next to an environment named env. This is because the Python environment has not been created yet. Right-click the Python Environments folder and select Add Environment.... Ensure the Name field is set to env and the Version field is set to Python 3.11 (or your latest version). Also make sure the Install packages from file field is set to the requirements.txt file for the project. This will install the required Python packages after creating the environment. Click Create. You should now see the env environment listed under the Python Environments folder. The warning icon should be gone. Complete steps 4-7 for the DataSourceHubAPI, LangChainAPI, PromptHubAPI, and PythonSDK projects. You may optionally complete these steps for the Python test projects as well. Right-click the Solution in Visual Studio, then select Configure Startup Projects.... Select the Multiple startup projects option, then set the Action for the following projects to Start. Click OK. OrchestrationAPI AgentHubAPI CoreAPI DataSourceHubAPI GatekeeperAPI GatekeeperIntegrationAPI LangChainAPI PromptHubAPI SemanticKernelAPI ManagementAPI VectorizationAPI Press F5 to start debugging the solution. This will start all of the .NET projects and the Python projects. The Vue.js (Nuxt) web app will not be started by default. To start it, follow the steps below. Configure and run the frontend components The frontend components consist of the Vue.js (Nuxt) web apps. Run the User Portal The UserPortal project is a Vue.js (Nuxt) project. To configure it to run locally, follow these steps Open the /src/ui/UserPortal folder in Visual Studio Code. Open the .env file and update the LOCAL_API_URL value to the URL of the local Core API service (https://localhost:63279). Important: Only set this value if you wish to debug the entire solution locally and bypass the App Config service value for the CORE API URL. If you do not wish to debug the entire solution locally, leave this value empty or comment it out. Open a terminal in Visual Studio Code and run the following commands: npm install npm run dev The web app should now be running at http://localhost:3000. Run the Management Portal The ManagementPortal project is a Vue.js (Nuxt) project. To configure it to run locally, follow these steps Open the /src/ui/Management folder in Visual Studio Code. Open the .env file and update the LOCAL_API_URL value to the URL of the local Core API service (https://localhost:63267). Important: Only set this value if you wish to debug the entire solution locally and bypass the App Config service value for the MANAGEMENT API URL. If you do not wish to debug the entire solution locally, leave this value empty or comment it out. Open a terminal in Visual Studio Code and run the following commands: npm install npm run dev The web app should now be running at http://localhost:3001."
  },
  "docs/development/index.html": {
    "href": "docs/development/index.html",
    "title": "Development | FoundationaLLM",
    "summary": "Development Development approach Are you interested in contributing to the code? We would love your help! However, we do have some guidelines that we need contributors to follow so that we can keep things running smoothly. Please review the FoundationaLLM DevOps rules and guidelines document. Set up your development environment To successfully build and run the solution, you need to set up your development environment. Follow the instructions in the Configure local development environment document."
  },
  "docs/documentation-generation.html": {
    "href": "docs/documentation-generation.html",
    "title": "Documentation generation | FoundationaLLM",
    "summary": "Documentation generation There are various layers of documentation for this project. This document describes the different layers and how they are generated. Documentation layers The following layers of documentation are available for this project: Markdown files - These are the source files for FoundationaLLM documentation. They are located in the docs folder in the main branch of the repository. API documentation - This is the documentation for the FoundationaLLM API. Language-specific API documentation is generated using tools (more on tools below). GitHub Pages - This is the primary documentation for the project. It is generated from the markdown files in the docs folder. It also includes the API documentation. Documentation tools We use a number of tools to generate the documentation for this project. The following table lists the tools and their purpose: Tool Purpose DocFX Generates the .NET API documentation for the FoundationaLLM API. It also generates the docs website, which includes custom documentation (markdown files) and the combined .NET and Python API documentation. Sphinx Generates the Python API documentation for the FoundationaLLM API. Sphinx DocFX YAML Generates the DocFX YAML files for the Python API documentation. This allows the DocFx build process to incorporate the Python documentation as well. GitHub Pages Hosts the documentation website. It is configured to use the gh-pages branch of the repository as the source for the website. DocFX Execute the following command from the repository root to generate the .NET API documentation: docfx docs/docfx.json Previewing Documentation Locally To preview the documentation locally in your browser, use the --serve flag: docfx docs/docfx.json --serve This command will: Build the documentation (same as running docfx docs/docfx.json) Start a local web server (typically at http://localhost:8080) Open the documentation site in your default browser Note: The --serve command builds the documentation once and serves the static files. If you make changes to markdown files: Run docfx docs/docfx.json in another terminal to rebuild, or restart the serve command Refresh your browser to see the updated content"
  },
  "docs/index.html": {
    "href": "docs/index.html",
    "title": "About FoundationaLLM | FoundationaLLM",
    "summary": "About FoundationaLLM FoundationaLLM is a comprehensive platform for deploying secure, extensible AI agents inside your own cloud environment. Designed for scale, built for control, and ready to work. Core Areas Overview Get started with FoundationaLLM by understanding the architecture and core concepts. Learn about the platform's capabilities, design principles, and how it addresses enterprise AI needs. Chat User Portal The Chat User Portal is the primary interface for end users to interact with FoundationaLLM agents. Users can select agents, manage conversations, upload files, and use specialized tools like Code Interpreter and Knowledge Search. Management Portal The Management Portal provides administrators with tools to create and manage AI agents, connect data sources, build data pipelines, configure AI models, implement access control, and customize branding. APIs & SDKs FoundationaLLM provides comprehensive APIs and SDKs for programmatic access: Core API: User-facing interactions including chat completions and session management Management API: Administrative operations for agents, data sources, and configuration SDKs: .NET and Python SDKs for easier integration Platform Operations Platform Operations covers deployment, security, monitoring, and troubleshooting. Learn how to deploy FoundationaLLM, configure authentication and authorization, implement role-based access control, and maintain your deployment. Development The Development section provides guidance for developers: Calling APIs directly Local development environment setup Development approach and best practices Contributing to the project Reference documentation Release Notes Stay updated with the latest changes, new features, and breaking changes in the Release Notes."
  },
  "docs/management-portal/how-to-guides/agents/agent-creation-walkthrough.html": {
    "href": "docs/management-portal/how-to-guides/agents/agent-creation-walkthrough.html",
    "title": "Agent Creation UX Walkthrough | FoundationaLLM",
    "summary": "Agent Creation UX Walkthrough A step-by-step walkthrough of the agent creation experience in the Management Portal. Overview This guide walks you through the complete user experience of creating a new agent, from initial navigation through all configuration options to final creation. Prerequisites Before creating an agent, ensure you have: Access to the Management Portal with appropriate permissions At least one AI model configured under Models and Endpoints > AI Models (Optional) Prompts created for workflow and tool configurations (Optional) Data sources configured if your agent needs knowledge retrieval Starting Agent Creation Step 1: Navigate to Create New Agent Log into the Management Portal In the left sidebar, click Create New Agent The agent creation form loads Step 2: Overview of the Form The creation form is organized into expandable sections: Section Purpose General Information Basic agent details Agent Configuration Behavior settings User Portal Experience Feature toggles Workflow Processing configuration Tools Extended capabilities Section 1: General Information What You'll Configure Agent Name Type a unique identifier (e.g., sales-assistant) Allowed: letters, numbers, dashes, underscores Not allowed: spaces, special characters Real-time validation shows if the name is valid This name cannot be changed after creation Agent Display Name Type a user-friendly name (e.g., Sales Assistant) This is what users see in the agent selector Can include spaces and special characters Can be changed later Description Describe what the agent does Helps with discoverability Displayed in agent lists Welcome Message Rich text editor opens when you click the field Create a greeting for when users start conversations Supports formatting (bold, italic, lists, links) Preview shows how it will appear Validation Red error text appears below invalid fields Agent name must be unique — duplicates are rejected Required fields are marked with asterisks Section 2: Agent Configuration 2.1 Conversation History Locate the Conversation History section Toggle Enabled on or off If enabled, set Max Messages (number field) Default is 5 messages Higher values provide more context but use more tokens 2.2 Gatekeeper Settings Find the Gatekeeper section Choose your approach: Use system default: Applies instance-level settings Custom: Select specific options If using custom settings: Content Safety: Check platforms to enable Azure Content Safety Azure Content Safety Prompt Shield Lakera Guard Enkrypt Guardrails Data Protection: Check services to enable Microsoft Presidio 2.3 User Prompt Rewrite Find User Prompt Rewrite section Toggle Enabled if you want prompts rewritten If enabled, configure: Rewrite Model: Select from dropdown Rewrite Prompt: Select or create a prompt Rewrite Window Size: Number of messages to consider 2.4 Semantic Cache Find Semantic Cache section Toggle Enabled for caching similar responses If enabled, configure: Model: Embedding model for similarity Embedding Dimensions: Vector size (default: 2048) Minimum Similarity Threshold: Match threshold (default: 0.97) 2.5 Cost Center and Expiration Cost Center: Optional text field for department tracking Expiration Date: Optional date picker Click to open calendar Select when the agent should auto-disable Leave empty for no expiration Section 3: User Portal Experience Configure what features users can access: Toggle Default What It Controls Show Message Tokens Yes Display token consumption Allow Rating Yes Enable thumbs up/down ratings Show View Prompt Yes Allow viewing completion prompts Allow File Upload No Enable file attachments Click each toggle to enable/disable Changes are immediate (no save needed until final creation) Section 4: Workflow Configuration 4.1 Selecting Workflow Type Find the Workflow dropdown Select a workflow type: Type Description OpenAIAssistants Azure OpenAI Assistants API LangGraphReactAgent LangGraph with dynamic tool selection ExternalAgentWorkflow Custom Python workflows 4.2 Configuring Workflow Click Configure Workflow to expand settings Fill in required fields: Workflow Name: Identifier for this workflow Workflow Host: Select orchestration framework LangChain Other options depending on deployment Workflow Main Model: Click the dropdown Select from available AI models This is the primary model for responses Main Workflow Prompt: Click the dropdown Select an existing prompt Or create new prompt (opens prompt editor) Model Parameters (optional): Expand to set temperature, max tokens, etc. 4.3 Adding Workflow Resources For additional prompts needed by the workflow: Click Add Workflow Resource Select Resource Type: Model, Prompt, or other Select the specific Resource from dropdown Enter the Resource Role (e.g., router_prompt) Click Save Repeat for additional resources Section 5: Tools Configuration Adding a Tool In the Tools section, click Add New Tool A tool configuration panel opens Configuring a Tool Tool Name: Enter unique identifier Tool Description: Describe what it does (helps AI decide when to use it) Tool Package Name: Python package (e.g., foundationallm.tools) Tool Class Name: Python class (e.g., DALLEImageGeneration) Adding Tool Resources Click Add Resource in the tool panel Select resource type (Model, Prompt, Data Pipeline) Select the specific resource Set the resource role Save Adding Tool Properties Click Add Property Enter property name Enter property value Save Common Tools Tool Purpose Package Class DALL-E Image generation foundationallm.tools DALLEImageGeneration Code Interpreter Python execution foundationallm.tools FoundationaLLMCodeInterpreterTool Knowledge Search Search data sources foundationallm.tools FoundationaLLMKnowledgeTool Creating the Agent Final Review Scroll through all sections Verify required fields are completed: Agent Name ✓ Agent Display Name ✓ Workflow Main Model ✓ Main Workflow Prompt ✓ Check for any validation errors (red text) Submitting Click Create Agent at the bottom of the page A loading indicator appears Wait for the creation process Success: Redirected or confirmation message Failure: Error message with details After Creation The new agent appears in All Agents list Users with access can select it in the User Portal You can edit settings at any time Configure Access Control to grant permissions to others Validation Rules Summary Field Rules Agent Name Required, unique, alphanumeric with dashes/underscores Display Name Required, any text Workflow Main Model Required Main Workflow Prompt Required Expiration Date Must be future date if set Temperature 0.0 to 1.0 Max Messages Positive integer Common Issues and Solutions \"Agent name already exists\" Choose a different name Check All Agents for existing names Add a suffix to make unique \"Model not found\" Verify models are configured in Models and Endpoints Refresh the page to reload model list Contact administrator if no models appear \"Workflow prompt required\" Create a prompt first under Prompts Or use the inline prompt creation option Form not saving Check for validation errors Verify your session hasn't timed out Try a different browser Related Topics Create New Agent — Detailed configuration reference Agent Management Walkthrough — Managing existing agents Prompts — Creating and managing prompts Agents & Workflows Reference — Technical details"
  },
  "docs/management-portal/how-to-guides/agents/agent-management-walkthrough.html": {
    "href": "docs/management-portal/how-to-guides/agents/agent-management-walkthrough.html",
    "title": "Agent Management UX Walkthrough | FoundationaLLM",
    "summary": "Agent Management UX Walkthrough A complete walkthrough of agent management tasks in the Management Portal. Overview This guide walks you through the full user experience of managing agents in the Management Portal, from navigation to common tasks. Accessing Agent Management Step 1: Navigate to the Management Portal Open your browser and navigate to the Management Portal URL Sign in with your Microsoft Entra ID credentials The portal dashboard loads Step 2: Find the Agents Section Look at the left sidebar navigation Find the Agents section You'll see several options: Create New Agent — Create a new agent All Agents — View all agents you have access to My Agents — View only agents you own Viewing Agents The All Agents Page Click All Agents in the sidebar The agent list table loads with these columns: Column Description Name Agent name (with display name if different) Description Brief description of the agent's purpose Expiration Date When the agent expires (if set) Edit ⚙️ Settings icon to modify Delete \uD83D\uDDD1️ Trash icon to remove Set Default ⭐ Star icon to make default Identifying Default Agents The current default agent displays a Default chip badge next to its name Default agents have a filled star icon Searching for Agents Find the Search box above the table Type the agent name or description keywords The list filters in real-time as you type Clear the search to show all agents again Sorting the List Click any column header to sort by that column Click again to reverse the sort order An arrow indicator shows the current sort direction Pagination Use the rows per page dropdown to show 10, 25, 50, or 100 agents Navigate between pages using the pagination controls at the bottom The total count of agents is displayed Common Agent Management Tasks Task: Find a Specific Agent Scenario: You need to find an agent named \"Sales Assistant\" Go to All Agents Type \"Sales\" in the search box Review the filtered results Click on the agent row or Edit icon to view details Task: Edit Agent Configuration Scenario: You need to change an agent's welcome message Find the agent in the list Click the Settings icon (⚙️) in the Edit column The agent edit form opens Scroll to find the Welcome Message field Edit the content (rich text editor available) Click Save Changes A success message confirms the update Task: Delete an Agent Scenario: An agent is no longer needed and should be removed Find the agent in the list Click the Trash icon (\uD83D\uDDD1️) in the Delete column A confirmation dialog appears: \"Are you sure you want to delete the agent '[name]'?\" Click Yes to confirm or Cancel to abort The agent is removed from the list Warning: Deletion is permanent. Users will no longer be able to access the deleted agent. Task: Set an Agent as Default Scenario: You want a specific agent to be the default for all users Prerequisites: You need the User Access Administrator role Find the agent you want as default Click the Star icon (⭐) in the Set Default column Confirm by clicking Yes in the dialog The Default badge moves to this agent The previous default (if any) is cleared Task: View Only Your Agents Scenario: You want to focus on agents you own Click My Agents in the sidebar instead of All Agents The list shows only agents where you have the Owner role All the same management actions are available Access Control for Agents Viewing Agent Permissions Open an agent for editing Click Access Control at the top right of the page The role assignments panel opens showing: Current role assignments Assigned principals (users/groups) Role types (Owner, Contributor, Reader) Adding Role Assignments In Access Control, click Add Role Assignment Select the scope (agent or prompt) Choose the principal type (User, Group, Service Principal) Enter the principal ID or search for the user Select the role (Owner, Contributor, Reader) Click Save Removing Role Assignments In Access Control, find the assignment to remove Click the Delete button next to it Confirm the removal The access is revoked immediately Filtering and Organization Best Practices for Finding Agents Goal Approach Find by name Use search with partial name Find by purpose Search using description keywords Find recent agents Sort by creation date (if visible) Find your agents Use My Agents view Find expiring agents Sort by Expiration Date Organizing Your View Use search to quickly find specific agents Sort by name for alphabetical navigation Increase rows per page to see more at once Use My Agents when focusing on your own work Refreshing Data Manual Refresh Click the Refresh button (\uD83D\uDD04) at the top right of the table The list reloads with the latest data Useful after making changes in another tab Automatic Refresh The agent list doesn't auto-refresh Make changes in one session and refresh others to see updates Troubleshooting Common Issues Agent Not Appearing in List Wait a moment for data to load Click Refresh to reload the list Verify you have permission to view the agent Check if you're in All Agents vs My Agents Can't Edit an Agent The Edit icon appears grayed out You may only have Reader permission Contact the agent Owner for edit access Can't Delete an Agent The Delete icon appears grayed out You need Owner or Contributor permission Contact an administrator for help Can't Set Default Agent You need User Access Administrator role Contact your administrator to request this permission Or ask an administrator to set the default for you Changes Not Saving Check for validation errors (red text below fields) Ensure required fields are filled Verify your session hasn't timed out Try refreshing and re-entering changes Related Topics Create New Agent — Full guide to agent creation Agent Creation Walkthrough — Step-by-step creation UX All Agents — Reference for the All Agents page My Agents — Reference for the My Agents page"
  },
  "docs/management-portal/how-to-guides/agents/all-agents.html": {
    "href": "docs/management-portal/how-to-guides/agents/all-agents.html",
    "title": "All Agents | FoundationaLLM",
    "summary": "All Agents Learn how to view and manage all agents in the Management Portal. Overview The All Agents page displays every agent configured in your FoundationaLLM instance that you have permission to view. This is the central location for managing agent configurations. Accessing All Agents In the Management Portal sidebar, click All Agents under the Agents section The agent list table loads, showing all available agents Agent List Table The table displays the following columns: Column Description Name Agent name and display name (if set). Default agents show a star chip badge. Description Brief description of the agent's purpose Expiration Date Date when the agent becomes inactive (if set) Edit Settings icon to modify agent configuration Delete Trash icon to remove the agent Set Default Star icon to make this the default agent Searching and Filtering Search Use the search box at the top of the table to filter agents by: Agent name Description Type your search term and the list filters in real-time. Sorting Click any column header to sort by that column: Click once for ascending order Click again for descending order Pagination Configure how many agents to display per page: Use the dropdown to select 10, 25, 50, or 100 rows Navigate between pages using the pagination controls Managing Agents Editing an Agent Locate the agent in the list Click the Settings icon (⚙️) in the Edit column The agent edit page opens with all configuration sections Make your changes Click Save Changes Note: The Edit icon is disabled (grayed out) if you don't have write permission for the agent. Deleting an Agent Locate the agent in the list Click the Trash icon (\uD83D\uDDD1️) in the Delete column A confirmation dialog appears: \"Are you sure you want to delete the agent '[name]'?\" Click Yes to confirm or Cancel to abort Warning: Deleted agents are removed from the system. Users will no longer be able to interact with deleted agents. Note: The Delete icon is disabled if you don't have delete permission for the agent. Setting a Default Agent The default agent is automatically selected for new conversations in the Chat User Portal. Locate the agent you want to set as default Click the Star icon (⭐) in the Set Default column Confirm by clicking Yes in the dialog When an agent is the default, it displays a Default chip next to its name. Note: Only users with the \"User Access Administrator\" role can set default agents. Refreshing the List Click the Refresh button (\uD83D\uDD04) at the top right of the table to reload the agent list and see the latest changes. Permissions Your available actions depend on your role assignments: Action Required Permission View agents FoundationaLLM.Agent/agents/read Edit agents FoundationaLLM.Agent/agents/write Delete agents FoundationaLLM.Agent/agents/delete Set default agent User Access Administrator role Related Topics My Agents - View only agents you own Create New Agent - Create a new agent Instance Access Control - Manage permissions"
  },
  "docs/management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.html": {
    "href": "docs/management-portal/how-to-guides/agents/create-model-agnostic-agent-claude.html",
    "title": "Create a Model Agnostic Agent with Claude | FoundationaLLM",
    "summary": "Create a Model Agnostic Agent with Claude This step-by-step guide walks you through creating a model agnostic agent using Claude, with a code interpreter tool and a knowledge search tool for uploaded files. Overview A model agnostic agent uses the ExternalAgentWorkflow pattern with the FoundationaLLMFunctionCallingWorkflow class. This architecture provides: Code Interpreter: Execute Python code in isolated containers Knowledge Search: Search and retrieve content from uploaded files Flexible Routing: Dynamic tool selection based on user queries Prerequisites A Claude model deployed and configured in your FoundationaLLM instance Azure Container Apps configured for code execution (for Code Interpreter) Access to the Management Portal with agent creation permissions Part 1: Create the Agent Navigate to the Management Portal Click Create New Agent in the sidebar Basic Configuration Field Value Agent Name Your unique agent identifier (e.g., claude-maa-agent) Agent Display Name User-friendly name (e.g., Claude Analysis Agent) Description Purpose of the agent User Portal Experience Set \"Would you like to allow the user to upload files?\" to Yes Workflow Configuration Select ExternalAgentWorkflow from the workflow dropdown Click Configure Workflow Enter the following values: Field Value Workflow Name MAA-Workflow Workflow Package Name foundationallm_agent_plugins Workflow Class Name FoundationaLLMFunctionCallingWorkflow Workflow Host LangChain Workflow Main Model Select your Claude model Add a model parameter: Click Add Property Property Key: temperature Property Type: number Property Value: 0.5 Click Save For Main Workflow Prompt, use the prompt from: TODO: Obtain the workflow main prompt content from the FoundationaLLM packages repository at: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Main.txt Part 2: Create the Workflow Prompts Open a new browser tab and navigate to Prompts in the Management Portal. Note: Replace Your-Agent-Name with your actual agent name in all prompt names below. Create Workflow-Files Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Files Description Provides instructions for identifying files relevant to the question Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Files.txt Create Workflow-Final Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Final Description Instructions to build final response from tool results Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Final.txt Create Workflow-Router Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Router Description Instructions for tool selection Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Router.txt Part 3: Create the Tool Prompts Code Interpreter Prompts Tool-Code-Main: Field Value Prompt Name Your-Agent-Name-Tool-Code-Main Description Main instructions for the code interpreter tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Main.txt Tool-Code-Router: Field Value Prompt Name Your-Agent-Name-Tool-Code-Router Description Instructions for selecting the code interpreter tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Router.txt Knowledge Tool Prompts Tool-Knowledge-Main: Field Value Prompt Name Your-Agent-Name-Tool-Knowledge-Main Description Main instructions for the knowledge tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Main.txt Tool-Knowledge-Router: Field Value Prompt Name Your-Agent-Name-Tool-Knowledge-Router Description Instructions for selecting the knowledge tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Router.txt Part 4: Configure Workflow Resources Return to your agent configuration and add workflow resources: Add Router Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Router Resource Role: router_prompt Click Save Add Files Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Files Resource Role: files_prompt Click Save Add Final Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Final Resource Role: final_prompt Click Save Part 5: Configure the Code Interpreter Tool In the Tools section, click Add New Tool Basic Configuration Field Value Tool Name Code-01 (must match prompts exactly) Tool Description Answers questions that require dynamic generation of code Tool Package Name foundationallm_agent_plugins Tool Class Name FoundationaLLMCodeInterpreterTool Add Tool Resources Main Model: Click Add Tool Resource Select Resource Type: Model Select your Claude model Resource Role: main_model Click Save Expand the model resource and click Add Property: Property Key: model_parameters Property Type: Object / Array Property Value: {\"temperature\": 0.2} Click Save Main Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Code-Main Resource Role: main_prompt Click Save Router Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Code-Router Resource Role: router_prompt Click Save Add Tool Properties Add these properties one at a time: Property Key Property Type Property Value code_session_required Boolean True code_session_endpoint_provider String AzureContainerAppsCustomContainer code_session_language String Python Click Save in the Configure Tool dialog. Part 6: Configure the Knowledge Tool Click Add New Tool Basic Configuration Field Value Tool Name Knowledge-Conversation-Files (must match prompts exactly) Tool Description Retrieves content from files uploaded to conversations Tool Package Name foundationallm_agent_plugins Tool Class Name FoundationaLLMKnowledgeTool Add Tool Resources Main Model: Click Add Tool Resource Select Resource Type: Model Select your Claude model Resource Role: main_model Click Save Main Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Knowledge-Main Resource Role: main_prompt Click Save Router Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Knowledge-Router Resource Role: router_prompt Click Save Data Pipeline: Click Add Tool Resource Select Resource Type: Data Pipeline Select DefaultFileUpload Resource Role: file_upload_data_pipeline Click Save Vector Database: Click Add Tool Resource Select Resource Type: Vector Database Select ConversationFiles Resource Role: vector_database Click Save Add Tool Properties Property Key Property Type Property Value embedding_model String text-embedding-3-large embedding_dimensions Number 2048 Click Save in the Configure Tool dialog. Part 7: Create the Agent Review all configurations Click Create Agent Wait for confirmation Testing Your Agent Open the Chat User Portal Select your new agent Test the capabilities: Test File Upload and Knowledge Search Upload a document (PDF, Word, etc.) Ask questions about the document content Test Code Interpreter Ask for data analysis or calculations Request charts or visualizations Ask for code generation Troubleshooting Tool Not Being Selected Verify tool names match exactly: Code-01 and Knowledge-Conversation-Files Check that router prompts are correctly assigned File Upload Not Working Ensure \"Allow File Upload\" is enabled in User Portal Experience Verify the DefaultFileUpload data pipeline exists Code Execution Failing Check Azure Container Apps custom container configuration Verify code_session_endpoint_provider is set correctly Related Topics Create Model Agnostic Agent with GPT-4o Managing Prompts Agents & Workflows Reference"
  },
  "docs/management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.html": {
    "href": "docs/management-portal/how-to-guides/agents/create-model-agnostic-agent-gpt4o.html",
    "title": "Create a Model Agnostic Agent with GPT-4o | FoundationaLLM",
    "summary": "Create a Model Agnostic Agent with GPT-4o This step-by-step guide walks you through creating a model agnostic agent using GPT-4o, with a code interpreter tool and a knowledge search tool for uploaded files. Overview A model agnostic agent uses the ExternalAgentWorkflow pattern with the FoundationaLLMFunctionCallingWorkflow class. This architecture provides: Code Interpreter: Execute Python code in isolated containers Knowledge Search: Search and retrieve content from uploaded files Flexible Routing: Dynamic tool selection based on user queries Prerequisites A GPT-4o model deployed and configured in your FoundationaLLM instance Azure Container Apps configured for code execution (for Code Interpreter) Access to the Management Portal with agent creation permissions Part 1: Create the Agent Navigate to the Management Portal Click Create New Agent in the sidebar Basic Configuration Field Value Agent Name Your unique agent identifier (e.g., gpt4o-maa-agent) Agent Display Name User-friendly name (e.g., GPT-4o Analysis Agent) Description Purpose of the agent User Portal Experience Set \"Would you like to allow the user to upload files?\" to Yes Workflow Configuration Select ExternalAgentWorkflow from the workflow dropdown Click Configure Workflow Enter the following values: Field Value Workflow Name MAA-Workflow Workflow Package Name foundationallm_agent_plugins Workflow Class Name FoundationaLLMFunctionCallingWorkflow Workflow Host LangChain Workflow Main Model Select your GPT-4o model Add a model parameter: Click Add Property Property Key: temperature Property Type: number Property Value: 0.5 Click Save For Main Workflow Prompt, use the prompt from: TODO: Obtain the workflow main prompt content from the FoundationaLLM packages repository at: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Main.txt Part 2: Create the Workflow Prompts Open a new browser tab and navigate to Prompts in the Management Portal. Note: Replace Your-Agent-Name with your actual agent name in all prompt names below. Create Workflow-Files Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Files Description Provides instructions for identifying files relevant to the question Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Files.txt Create Workflow-Final Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Final Description Instructions to build final response from tool results Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Final.txt Create Workflow-Router Prompt Field Value Prompt Name Your-Agent-Name-Workflow-Router Description Instructions for tool selection Category Workflow Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Workflow-Router.txt Part 3: Create the Tool Prompts Code Interpreter Prompts Tool-Code-Main: Field Value Prompt Name Your-Agent-Name-Tool-Code-Main Description Main instructions for the code interpreter tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Main.txt Tool-Code-Router: Field Value Prompt Name Your-Agent-Name-Tool-Code-Router Description Instructions for selecting the code interpreter tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Code-Router.txt Knowledge Tool Prompts Tool-Knowledge-Main: Field Value Prompt Name Your-Agent-Name-Tool-Knowledge-Main Description Main instructions for the knowledge tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Main.txt Tool-Knowledge-Router: Field Value Prompt Name Your-Agent-Name-Tool-Knowledge-Router Description Instructions for selecting the knowledge tool Category Tool Prompt Prefix See note below TODO: Obtain prompt content from: https://github.com/foundationallm/foundationallm-packages/blob/main/ModelAgnosticAgent/artifacts/Agent-Tool-Knowledge-Router.txt Part 4: Configure Workflow Resources Return to your agent configuration and add workflow resources: Add Router Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Router Resource Role: router_prompt Click Save Add Files Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Files Resource Role: files_prompt Click Save Add Final Prompt Resource Click Add Workflow Resource Configure: Resource Type: Prompt Resource: Select Your-Agent-Name-Workflow-Final Resource Role: final_prompt Click Save Part 5: Configure the Code Interpreter Tool In the Tools section, click Add New Tool Basic Configuration Field Value Tool Name Code-01 (must match prompts exactly) Tool Description Answers questions that require dynamic generation of code Tool Package Name foundationallm_agent_plugins Tool Class Name FoundationaLLMCodeInterpreterTool Add Tool Resources Main Model: Click Add Tool Resource Select Resource Type: Model Select your GPT-4o model Resource Role: main_model Click Save Expand the model resource and click Add Property: Property Key: model_parameters Property Type: Object / Array Property Value: {\"temperature\": 0.2} Click Save Main Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Code-Main Resource Role: main_prompt Click Save Router Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Code-Router Resource Role: router_prompt Click Save Add Tool Properties Add these properties one at a time: Property Key Property Type Property Value code_session_required Boolean True code_session_endpoint_provider String AzureContainerAppsCustomContainer code_session_language String Python Click Save in the Configure Tool dialog. Part 6: Configure the Knowledge Tool Click Add New Tool Basic Configuration Field Value Tool Name Knowledge-Conversation-Files (must match prompts exactly) Tool Description Retrieves content from files uploaded to conversations Tool Package Name foundationallm_agent_plugins Tool Class Name FoundationaLLMKnowledgeTool Add Tool Resources Main Model: Click Add Tool Resource Select Resource Type: Model Select your GPT-4o model Resource Role: main_model Click Save Main Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Knowledge-Main Resource Role: main_prompt Click Save Router Prompt: Click Add Tool Resource Select Resource Type: Prompt Select Your-Agent-Name-Tool-Knowledge-Router Resource Role: router_prompt Click Save Data Pipeline: Click Add Tool Resource Select Resource Type: Data Pipeline Select DefaultFileUpload Resource Role: file_upload_data_pipeline Click Save Vector Database: Click Add Tool Resource Select Resource Type: Vector Database Select ConversationFiles Resource Role: vector_database Click Save Add Tool Properties Property Key Property Type Property Value embedding_model String text-embedding-3-large embedding_dimensions Number 2048 Click Save in the Configure Tool dialog. Part 7: Create the Agent Review all configurations Click Create Agent Wait for confirmation Testing Your Agent Open the Chat User Portal Select your new agent Test the capabilities: Test File Upload and Knowledge Search Upload a document (PDF, Word, etc.) Ask questions about the document content Test Code Interpreter Ask for data analysis or calculations Request charts or visualizations Ask for code generation Troubleshooting Tool Not Being Selected Verify tool names match exactly: Code-01 and Knowledge-Conversation-Files Check that router prompts are correctly assigned File Upload Not Working Ensure \"Allow File Upload\" is enabled in User Portal Experience Verify the DefaultFileUpload data pipeline exists Code Execution Failing Check Azure Container Apps custom container configuration Verify code_session_endpoint_provider is set correctly Related Topics Create Model Agnostic Agent with Claude Managing Prompts Agents & Workflows Reference"
  },
  "docs/management-portal/how-to-guides/agents/create-new-agent.html": {
    "href": "docs/management-portal/how-to-guides/agents/create-new-agent.html",
    "title": "Create a New Agent | FoundationaLLM",
    "summary": "Create a New Agent This comprehensive guide walks you through creating a new agent using the Management Portal. Prerequisites Before creating an agent, ensure: You have access to the Management Portal with appropriate permissions At least one AI model is configured under Models and Endpoints > AI Models (Optional) Prompts are created for workflow and tool configurations (Optional) Data sources are configured if your agent needs knowledge retrieval Accessing the Agent Creation Page In the Management Portal sidebar, click Create New Agent The agent creation form loads with all configuration sections Agent Configuration Sections 1. General Information Field Description Requirements Agent Name Unique identifier used internally Letters, numbers, dashes, underscores only. No spaces or special characters. Validated in real-time. Agent Display Name User-friendly name shown in portals Any text. This is what users see. Description Purpose of the agent Recommended for discoverability Welcome Message Initial greeting shown to users Supports rich text formatting via editor 2. Agent Configuration Conversation History Controls whether the agent remembers previous messages in a conversation. Setting Options Description Enabled Yes/No Toggle conversation memory Max Messages Number How many previous messages to include (default: 5) Gatekeeper The gatekeeper provides content moderation and data protection. Setting Description Use system default Apply instance-level gatekeeper settings Content Safety Select content moderation platforms Data Protection Select PII detection services Content Safety Options: Azure Content Safety Azure Content Safety Prompt Shield Lakera Guard Enkrypt Guardrails Data Protection Options: Microsoft Presidio User Prompt Rewrite Optionally rewrite user prompts before processing. Setting Description Enabled Toggle prompt rewriting Rewrite Model AI model for rewriting Rewrite Prompt Prompt template for rewriting Rewrite Window Size Messages to consider (default: 3) Semantic Cache Cache responses for semantically similar questions. Setting Description Enabled Toggle caching Model Embedding model for similarity Embedding Dimensions Vector size (default: 2048) Minimum Similarity Threshold Match threshold (default: 0.97) Cost Center and Expiration Field Description Cost Center Department for cost tracking (optional) Expiration Date Auto-disable date (optional) 3. User Portal Experience Control features available to users in the Chat User Portal. Setting Default Description Show Message Tokens Yes Display token consumption Allow Rating Yes Enable thumbs up/down ratings Show View Prompt Yes Allow viewing completion prompts Allow File Upload No Enable file attachments 4. Workflow The workflow defines how the agent processes requests and generates responses. Selecting a Workflow Type Type Description Best For OpenAIAssistants Azure OpenAI Assistants API Code Interpreter, File Search, Function Calling LangGraphReactAgent LangGraph with dynamic tool selection Flexible multi-tool agents ExternalAgentWorkflow Custom Python workflows Advanced custom logic Workflow Configuration Click Configure Workflow to expand settings: Field Description Workflow Name Identifier for this workflow Workflow Package Name Python package (for custom workflows) Workflow Class Name Python class (for custom workflows) Workflow Host Orchestration framework (e.g., LangChain) Workflow Main Model Primary AI model Workflow Main Model Parameters Model settings (temperature, etc.) Main Workflow Prompt System prompt defining behavior Adding Workflow Resources Additional prompts and resources for the workflow: Click Add Workflow Resource Select Resource Type: Model, Prompt, or other Select the specific Resource Enter the Resource Role (e.g., router_prompt, final_prompt) Click Save 5. Tools Tools extend the agent's capabilities beyond text generation. Adding a Tool In the Tools section, click Add New Tool Configure the tool: Field Description Tool Name Unique identifier Tool Description What the tool does (helps AI decide when to use it) Tool Package Name Python package containing the tool Tool Class Name Python class implementing the tool Add Tool Resources (models, prompts, data pipelines) Add Tool Properties (configuration values) Click Save Common Tools Tool Class Name Purpose DALL-E Image Generation DALLEImageGeneration Generate images Code Interpreter FoundationaLLMCodeInterpreterTool Execute Python code Knowledge Search FoundationaLLMKnowledgeTool Search knowledge sources 6. Security (After Creation) After creating an agent, you can configure security settings: Virtual Security Group ID A unique identifier for programmatic access to the agent. Agent Access Tokens Create tokens for API access without Entra ID authentication: Access the agent edit page Scroll to the Security section Create and manage access tokens See Agent Access Tokens for details. Creating the Agent Review all configuration sections Click Create Agent at the bottom of the page Wait for the creation process to complete Upon success, you'll be redirected or see a confirmation Editing Existing Agents Navigate to All Agents or My Agents Click the Edit icon for the agent Modify settings as needed Click Save Changes Note: The agent name cannot be changed after creation. Access Control Configure who can access and manage the agent: Open the agent for editing Click Access Control at the top right Add role assignments for: Agent scope: Access to this specific agent Prompt scope: Access to the agent's prompt Form Validation The form validates required fields before allowing creation: Agent name must be unique and properly formatted Required workflow settings must be configured Model selections must be made where required Validation errors appear as red text below the relevant field. Related Topics Quick Start: Creating Your First Agent Create Model Agnostic Agent with Claude Create Model Agnostic Agent with GPT-4o Agents & Workflows Reference"
  },
  "docs/management-portal/how-to-guides/agents/my-agents.html": {
    "href": "docs/management-portal/how-to-guides/agents/my-agents.html",
    "title": "My Agents | FoundationaLLM",
    "summary": "My Agents Learn how to view and manage the agents you own. Overview The My Agents page displays agents where you have the Owner role. This is a filtered view that shows only agents you've created or been granted ownership of. Accessing My Agents In the Management Portal sidebar, click My Agents under the Agents section The table loads showing only agents you own Difference from All Agents View Shows All Agents All agents you have permission to view (based on Reader, Contributor, or Owner roles) My Agents Only agents where you have the Owner role Use My Agents when you want to focus on agents you're directly responsible for. Agent List Features The My Agents view uses the same interface as All Agents: Table Columns Column Description Name Agent name and display name (if set) Description Brief description of the agent's purpose Expiration Date Date when the agent becomes inactive (if set) Edit Modify agent configuration Delete Remove the agent Set Default Make this the default agent Search and Filter Use the search box to filter by name or description Click column headers to sort Use pagination controls to navigate large lists Owner Capabilities As an agent owner, you can: Edit all configuration settings Delete the agent Set as Default (if you also have User Access Administrator role) Manage Access Control - Add or remove role assignments for other users Checking Your Role To verify your role for a specific agent: Click the Edit icon for the agent Look for the Access Control button in the top right Open Access Control to see all role assignments Becoming an Owner You become an agent owner when: You create the agent - Creators automatically receive Owner role Ownership is assigned - An existing owner or administrator grants you the Owner role Related Topics All Agents - View all accessible agents Create New Agent - Create a new agent Instance Access Control - Managing permissions"
  },
  "docs/management-portal/how-to-guides/agents/prompts.html": {
    "href": "docs/management-portal/how-to-guides/agents/prompts.html",
    "title": "Managing Prompts | FoundationaLLM",
    "summary": "Managing Prompts Learn how to create and manage prompts in the Management Portal. Overview Prompts are reusable text templates that define agent behavior, tool instructions, and data pipeline processing. They provide a centralized way to manage the instructions that guide AI model responses. Accessing Prompts In the Management Portal sidebar, click Prompts under the Agents section The prompt list loads, showing all available prompts Prompt List The table displays: Column Description Name Unique identifier for the prompt Category Type of prompt (Workflow, Tool, DataPipeline) Description Purpose of the prompt Edit Settings icon to modify the prompt Searching and Sorting Use the search box to filter by name or description Click column headers to sort Use pagination controls to navigate large lists Prompt Categories Category Usage Agent Workflow Main prompts for agent orchestration, routing, and response generation Agent Tool Instructions for specific tools (code interpreter, knowledge search) Data Pipeline Prompts used in data processing workflows Creating a Prompt Click Create Prompt at the top right of the page Configure the prompt settings: Prompt Configuration Fields Field Description Requirements Prompt Name Unique identifier Letters, numbers, dashes, underscores only. No spaces. Description Purpose and usage notes Recommended for discoverability Category Type of prompt Select from dropdown Prompt Prefix Main prompt content The actual instructions for the AI Click Create Prompt to save Name Validation The prompt name validates in real-time: ✔️ Green checkmark: Name is available ❌ Red X: Name is invalid or already taken Editing Prompts Locate the prompt in the list Click the Settings icon (⚙️) in the Edit column Modify the configuration Click Save Changes Note: The prompt name cannot be changed after creation. To rename, create a new prompt and update references. Writing Effective Prompts Best Practices Be specific: Clearly define the expected behavior Include context: Specify the persona, tone, and constraints Use examples: Include sample inputs and outputs when helpful Structure clearly: Use formatting (headers, lists) for complex prompts Example Workflow Prompt You are a helpful assistant that provides accurate information about company policies. Guidelines: - Answer questions concisely and professionally - If information is not available, clearly state so - Cite relevant policy documents when applicable - Do not make assumptions beyond the provided data Response Format: - Start with a direct answer - Provide supporting details - Include relevant references Example Tool Prompt You are a code interpreter tool. Generate Python code to answer the user's question. Requirements: - Write clean, well-commented code - Handle edge cases appropriately - Return results in a clear format - Do not execute dangerous operations Using Prompts in Agents Prompts are referenced in agent configurations: Workflow Main Prompt Defined directly in the agent's workflow configuration as the primary system prompt. Additional Workflow Resources Added via the \"Add Workflow Resource\" dialog: Resource Type: Prompt Resource: Select the prompt Resource Role: Purpose identifier (e.g., router_prompt, final_prompt) Tool Prompts Added via the \"Add Tool Resource\" dialog when configuring agent tools: main_prompt: Primary tool instructions router_prompt: Instructions for tool selection Common Prompt Roles Role Purpose main_prompt Primary instructions for the workflow or tool router_prompt Instructions for selecting tools or routing files_prompt Instructions for file identification final_prompt Instructions for generating final responses Access Control Prompt permissions can be managed for each prompt: Open the prompt for editing Click Access Control at the top right Add or remove role assignments Permission Description FoundationaLLM.Prompt/prompts/read View the prompt FoundationaLLM.Prompt/prompts/write Edit the prompt Related Topics Prompts & Resources Reference Agents & Workflows Create New Agent"
  },
  "docs/management-portal/how-to-guides/configuring-quotas.html": {
    "href": "docs/management-portal/how-to-guides/configuring-quotas.html",
    "title": "Configuring Quotas | FoundationaLLM",
    "summary": "Configuring Quotas Learn how to configure and manage quotas to control resource usage in FoundationaLLM. Overview Quotas define limits on resource usage to: Prevent abuse: Stop runaway usage or malicious activity Manage costs: Control AI model token consumption Ensure fairness: Distribute resources among users Maintain performance: Prevent system overload Quota Types API Raw Request Rate Limits the number of raw API calls made to FoundationaLLM services. Metric Description api_requests Total API calls Period Minute, Hour, Day Agent Request Rate Limits the number of agent completion requests (conversations with AI). Metric Description agent_requests Agent completion calls Period Minute, Hour, Day Quota Configuration TODO: Document the UI for quota configuration in the Management Portal if available. Currently, quotas are configured via App Configuration. Via Azure App Configuration Quota definitions are stored in Azure App Configuration and the storage account. Quota Definition Structure { \"name\": \"api-rate-limit\", \"metric\": \"api_requests\", \"limit\": 100, \"period\": \"minute\", \"partitioning\": \"user_principal_name\" } Field Description name Unique quota identifier metric What to measure (api_requests, agent_requests) limit Maximum allowed within period period Time window (minute, hour, day) partitioning How to segment limits (optional) Partitioning Options Partitioning determines how quotas are applied: Partitioning Description None Global limit shared by all users user_identifier Per-user by internal user ID user_principal_name Per-user by Azure AD UPN/email Configuration Examples Global Rate Limit 100 API requests per minute for all users combined: { \"name\": \"global-api-limit\", \"metric\": \"api_requests\", \"limit\": 100, \"period\": \"minute\" } Per-User Rate Limit 50 requests per user per minute: { \"name\": \"user-api-limit\", \"metric\": \"api_requests\", \"limit\": 50, \"period\": \"minute\", \"partitioning\": \"user_principal_name\" } Daily Agent Request Limit 1000 agent requests per user per day: { \"name\": \"daily-agent-limit\", \"metric\": \"agent_requests\", \"limit\": 1000, \"period\": \"day\", \"partitioning\": \"user_principal_name\" } Applying Quotas Instance-Level Quotas Apply to the entire FoundationaLLM instance. Agent-Level Quotas TODO: Document agent-level quota configuration if available. Monitoring Quota Usage In the Chat User Portal Token consumption is displayed per message (if enabled) Users can see their current usage In Azure Monitor Review API metrics and logs Set up alerts for quota threshold warnings In the Management Portal TODO: Document quota monitoring dashboards in the Management Portal if available. Quota Exceeded Behavior When a quota is exceeded: API Response: Returns HTTP 429 (Too Many Requests) Error Message: Indicates quota type and reset time User Experience: Chat User Portal shows an appropriate error Best Practices Setting Appropriate Limits Use Case Recommendation Development Higher limits for testing Production Balanced limits for normal usage Public apps Stricter limits to prevent abuse Monitoring and Adjustment Start with conservative limits Monitor actual usage patterns Adjust limits based on real needs Set up alerts before limits are hit Communication Document quota limits for users Provide clear error messages Offer escalation paths for legitimate high-usage scenarios Troubleshooting Users Hitting Limits Unexpectedly Review current quota configuration Check for automated tools consuming quota Consider if limits are appropriately set Quotas Not Enforcing Verify quota configuration is properly deployed Check the quota applies to the correct scope Review service logs for quota processing Performance Impact Quota checking adds minimal latency Use appropriate partitioning to distribute checks Consider caching for high-volume scenarios Related Topics Quotas Reference Monitoring Token Consumption Configuration Reference"
  },
  "docs/management-portal/how-to-guides/data/data-pipeline-runs.html": {
    "href": "docs/management-portal/how-to-guides/data/data-pipeline-runs.html",
    "title": "Data Pipeline Runs | FoundationaLLM",
    "summary": "Data Pipeline Runs Learn how to view, filter, and analyze data pipeline execution history. Overview The Data Pipeline Runs page provides visibility into pipeline execution history, allowing you to monitor processing status, investigate failures, and track data ingestion progress. Accessing Pipeline Runs In the Management Portal sidebar, click Data Pipeline Runs under the Data section The runs list loads, showing execution history sorted by most recent first Pipeline Runs Table The table displays: Column Description Item Pipeline that was executed Status Current state (Running, Completed, Failed) Success Whether the run succeeded Start Time When execution began End Time When execution completed (if finished) Duration Total execution time Filtering Runs Use the filters at the top of the table to narrow results: Available Filters Filter Description Item Select a specific pipeline Status Filter by execution status Success Filter by success/failure Start Time Time range presets Start Time From/To Custom date range (when \"Custom\" is selected) Time Range Options Option Description All Show all runs Last Hour Runs started in the last hour Last 24 Hours Runs started in the last day Last 7 Days Runs started in the last week Last 30 Days Runs started in the last month Custom Specify start/end dates manually Using Custom Date Range Select Custom from the Start Time dropdown Use the Start Time From calendar to set the begin date Use the Start Time To calendar to set the end date The list automatically filters Clearing Filters Click Clear Filters to reset all filters to their default values. Refreshing the List Click the Refresh button (\uD83D\uDD04) to reload the pipeline runs list with the latest data. Run Status Indicators Status Description Running Pipeline is currently executing Completed Pipeline finished execution Failed Pipeline encountered an error Cancelled Pipeline was manually stopped Viewing Run Details TODO: Document how to view detailed run information when clicking on a specific run. This may include step-by-step execution logs, items processed, errors, and performance metrics. Common Scenarios Finding Failed Runs Set the Success filter to Failed Review the list of failed runs Click on a run to investigate the cause Monitoring Active Runs Set the Status filter to Running Monitor progress of active pipelines Use refresh to see updated status Reviewing Recent Activity Set Start Time to Last 24 Hours Review all pipeline activity from the last day Troubleshooting Pipeline Takes Too Long Check if the data source has more data than expected Review stage configurations for optimization opportunities Consider breaking large datasets into smaller batches Frequent Failures Check data source connectivity Review error messages in run details Verify authentication credentials haven't expired Runs Not Appearing Click refresh to update the list Verify your filters aren't excluding the runs Check that you have permission to view the pipeline Related Topics Invoking Data Pipelines Monitoring Data Pipelines Creating Data Pipelines"
  },
  "docs/management-portal/how-to-guides/data/data-pipelines/creating-data-pipelines.html": {
    "href": "docs/management-portal/how-to-guides/data/data-pipelines/creating-data-pipelines.html",
    "title": "Creating Data Pipelines | FoundationaLLM",
    "summary": "Creating Data Pipelines Learn how to create and configure data pipelines for processing and indexing your data. Overview Data pipelines define the processing workflow for transforming raw data into indexed, searchable content that agents can use. A pipeline consists of: Data Source: Where to read data from Stages: Processing steps (text extraction, splitting, embedding, indexing) Configuration: Parameters for each stage Accessing Data Pipelines In the Management Portal sidebar, click Data Pipelines under the Data section The pipelines list shows all configured pipelines Pipeline List The table displays: Column Description Name Pipeline identifier Description Purpose of the pipeline Active Whether the pipeline is enabled Edit Settings icon to modify configuration Run Execute the pipeline manually Delete Remove the pipeline Creating a Pipeline Click Create Pipeline at the top right Configure the pipeline settings Basic Information Field Description Pipeline Name Unique identifier (letters, numbers, dashes, underscores) Display Name User-friendly name Description Purpose and what data it processes Select Data Source Choose an existing data source from the dropdown. This determines where the pipeline reads input data. After selecting a data source: Field Description Data Source Name Override name for this pipeline's data reference Data Source Description Additional description Data Source Plugin Plugin for processing this data source type Configure Data Source Plugin The plugin determines how data is read. Configure default values for plugin parameters: Parameter Type Input Method String/Int/Float/DateTime Text input Boolean Toggle switch Array Chips (comma-separated values) Resource Object ID Dropdown selection Pipeline Stages Stages define the processing workflow. Common stages include: Stage Types Stage Purpose Text Extraction Extract text content from documents Text Partitioning Split text into chunks Text Embedding Generate vector embeddings Indexing Store in vector database Configuring Stages For each stage: Select Plugin: Choose the processing plugin Configure Parameters: Set stage-specific settings Define Outputs: Specify where results go Stage Parameters Common parameters by stage type: Text Partitioning: Parameter Description Chunk Size Maximum size of each text chunk Overlap Size Characters overlapping between chunks Tokenizer How to count tokens Text Embedding: Parameter Description Model Embedding model to use Dimensions Vector size Batch Size Documents per batch Indexing: Parameter Description Vector Store Target vector database Index Name Name for the index Stage Ordering Stages can be reordered: Drag and drop stages to change order Use the visual connectors to see flow Ensure logical progression (extraction → partitioning → embedding → indexing) Advanced Configuration Trigger Settings Configure when the pipeline runs: Trigger Type Description Manual Run on demand only Scheduled Run on a cron schedule Event-driven Run when data changes TODO: Document specific trigger configuration options when available in the UI. Prompt Configuration Some stages use prompts for processing: Select a prompt from the available options Configure prompt parameters Set the prompt role (e.g., main_prompt) Saving the Pipeline Review all configuration sections Click Create Pipeline or Save Changes Wait for validation and saving Best Practices Design Start with simple pipelines and add complexity as needed Use descriptive names for stages Document expected input and output formats Performance Tune chunk sizes based on your content type Use appropriate batch sizes for embedding Consider parallel processing where supported Maintenance Test with sample data before production Monitor pipeline runs for errors Keep data sources and plugins updated Related Topics Data Sources Invoking Data Pipelines Monitoring Data Pipelines Data Pipelines Reference"
  },
  "docs/management-portal/how-to-guides/data/data-pipelines/invoking-data-pipelines.html": {
    "href": "docs/management-portal/how-to-guides/data/data-pipelines/invoking-data-pipelines.html",
    "title": "Invoking Data Pipelines | FoundationaLLM",
    "summary": "Invoking Data Pipelines Learn how to manually trigger data pipeline execution. Overview Data pipelines can be invoked manually to process new or updated data on demand. This is useful for: Initial data loading Processing specific datasets Testing pipeline configurations Ad-hoc data updates Accessing Pipeline Invocation Navigate to Data Pipelines in the sidebar Locate the pipeline you want to run Use the run action to invoke it Manual Invocation From the Pipelines List Find the pipeline in the list Click the Run icon (▶️) in the actions column Confirm the invocation if prompted The pipeline begins execution From Pipeline Edit Page Open the pipeline for editing Click Run Pipeline (if available) The pipeline starts processing Invocation Options TODO: Document specific invocation options available in the UI, such as: Full vs. incremental processing Folder/file selection for partial runs Override parameters for this run Monitoring the Run After invoking a pipeline: Navigate to Data Pipeline Runs Find your pipeline in the list (sorted by most recent) Monitor the status as it progresses Review results when complete Run Scheduling TODO: Document scheduled/automatic pipeline invocation if supported, including: Cron-based scheduling Event-triggered runs Continuous processing modes Best Practices Before Running Verify the data source is accessible Check that target storage/index has capacity Review pipeline configuration for correctness During Execution Monitor the run status in Pipeline Runs Check for early errors Be prepared to cancel if issues arise After Completion Verify data was processed correctly Check the target index/storage for new content Test agent queries against the updated data Troubleshooting Pipeline Won't Start Verify you have permission to run the pipeline Check if another run is already in progress Ensure the pipeline is in an active state Run Fails Immediately Check data source connectivity Verify authentication credentials Review error messages in run details Processing Slower Than Expected Large datasets take longer to process Check embedding model throughput Review stage configurations Related Topics Creating Data Pipelines Monitoring Data Pipelines Data Pipeline Runs"
  },
  "docs/management-portal/how-to-guides/data/data-pipelines/monitoring-data-pipelines.html": {
    "href": "docs/management-portal/how-to-guides/data/data-pipelines/monitoring-data-pipelines.html",
    "title": "Monitoring Data Pipelines | FoundationaLLM",
    "summary": "Monitoring Data Pipelines Learn how to monitor data pipeline execution, track processing status, and understand performance characteristics. Overview Monitoring data pipelines helps you: Track processing progress in real-time Identify and diagnose failures quickly Understand processing performance and latency Plan capacity and scheduling optimization Measure the effectiveness of performance improvements Performance and Latency Understanding Processing Latency Data pipeline latency refers to the time between when data is submitted and when it becomes available for agent queries. FoundationaLLM optimizes this through: Optimization Benefit Parallel Stage Processing Multiple stages can run concurrently where dependencies allow Batch Processing Documents are processed in optimized batches Efficient Embedding Text embedding uses optimized batch sizes Incremental Indexing Only changed content is reprocessed Factors Affecting Latency Factor Impact Mitigation Document Size Larger documents take longer to process Split large documents Document Count More documents increase total time Use parallel processing Embedding Model Model complexity affects speed Balance quality vs. speed Index Size Large indexes may slow indexing Use index partitions Network Latency Remote services add overhead Use regional deployments Performance Configuration To optimize pipeline performance: Adjust Batch Sizes: Larger batches can improve throughput but increase memory usage Configure Parallelism: Set appropriate concurrent processing limits Tune Chunk Sizes: Balance between context quality and processing speed Schedule Off-Peak: Run large pipelines during low-usage periods Monitoring Locations Pipeline monitoring is available in two places: Data Pipelines - View pipeline configurations and status Data Pipeline Runs - View detailed execution history Pipeline Status Indicators In the Pipelines List Column Description Active Whether the pipeline is enabled Last Run Most recent execution status (if shown) In Pipeline Runs Status Description Running Currently processing data Completed Finished successfully Failed Encountered an error Cancelled Manually stopped Real-Time Monitoring Watching Active Runs Navigate to Data Pipeline Runs Filter by Status: Running Use the refresh button to update status Watch for completion or failures Progress Tracking TODO: Document real-time progress indicators if available, such as: Items processed count Current stage indicator Estimated time remaining Processing rate metrics Run Details Click on a specific run to view detailed information: Execution Log TODO: Document the detailed execution log view, including: Stage-by-stage progress Timestamps for each step Items processed per stage Error messages and stack traces Performance Metrics TODO: Document available performance metrics: Total duration Time per stage Items per second Resource utilization Alerting and Notifications TODO: Document alerting capabilities if available: Failure notifications Completion notifications Integration with Azure Monitor or other systems Historical Analysis Viewing Trends Use the Pipeline Runs page filters to analyze patterns: Filter to a specific pipeline Set a time range (e.g., Last 30 Days) Review success rates and durations Identify recurring issues Common Patterns Pattern Possible Cause Intermittent failures Network issues, resource contention Increasing duration Growing data volume, performance degradation Consistent failures Configuration error, permission issue Success after retry Transient errors, timeout issues Troubleshooting from Monitoring Identifying Issues Failed Status: Check error messages in run details Long Duration: Review stage timing for bottlenecks Repeated Failures: Look for patterns in failure timing/type Common Issues Issue Investigation Steps Connection failures Check data source configuration, network Timeout errors Increase timeout settings, reduce batch size Resource errors Check storage capacity, API quotas Data errors Review source data quality, parsing settings Best Practices Regular Monitoring Check pipeline runs daily during initial setup Set up alerts for critical pipelines Review trends weekly or monthly Proactive Management Address warnings before they become failures Plan maintenance during low-activity periods Monitor storage and quota utilization Documentation Record common failure patterns and solutions Document expected processing times Track changes that affect performance Related Topics Data Pipeline Runs Creating Data Pipelines Invoking Data Pipelines"
  },
  "docs/management-portal/how-to-guides/data/data-sources.html": {
    "href": "docs/management-portal/how-to-guides/data/data-sources.html",
    "title": "Data Sources | FoundationaLLM",
    "summary": "Data Sources Learn how to configure and manage data sources in the Management Portal. Overview Data sources define the storage locations from which agents and data pipelines can retrieve information. They provide the connection configuration needed to access your organizational data. Accessing Data Sources In the Management Portal sidebar, click Data Sources under the Data section The data sources list loads, showing all configured sources Data Source List The table displays: Column Description Name Unique identifier for the data source Source Type Type of storage (Azure Data Lake, SharePoint, etc.) Edit Settings icon to modify configuration Delete Trash icon to remove the data source Searching and Managing Use the search box to filter by name Click the refresh button to reload the list Click column headers to sort Supported Data Source Types Type Description Azure Data Lake Azure Data Lake Storage Gen2 containers OneLake Microsoft Fabric OneLake storage SharePoint Online SharePoint document libraries Azure SQL Database Azure SQL databases Web Public web content via URLs Creating a Data Source Click Create Data Source at the top right of the page Configure the data source settings Basic Information Field Description Requirements Data Source Name Unique identifier Letters, numbers, dashes, underscores only Description Purpose and contents Optional but recommended Select Data Source Type Choose the appropriate type from the dropdown: Azure Data Lake OneLake SharePoint Online Azure SQL Database Web Configure Connection Details Connection settings vary by data source type: Azure Data Lake Field Description Authentication Type How to authenticate (Connection String, Account Key, Azure Identity) Connection String Full connection string (if using Connection String auth) API Key Storage account key (if using Account Key auth) Endpoint Storage account endpoint URL Account Name Storage account name (if using Azure Identity) Folders Container/folder paths to access (press Enter after each) Authentication Options: Type Description Best For Connection String Full connection string with SAS or key Development, testing Account Key Storage account access key Simple deployments Azure Identity Managed Identity authentication Production, secure access OneLake Similar to Azure Data Lake, with authentication options for Fabric workspaces. SharePoint Online Field Description Site URL SharePoint site URL Document Library Library to access Authentication Microsoft Graph authentication settings Azure SQL Database Field Description Server SQL Server hostname Database Database name Authentication SQL or Azure AD authentication Tables/Views Specific tables or views to access Web Field Description URL(s) Web pages to crawl Crawl Depth How deep to follow links Specifying Folders For storage-based sources, specify folders using chips: Type the folder path (e.g., container/folder) Press Enter or , to add the chip Repeat for additional folders Click the X on a chip to remove it Editing Data Sources Locate the data source in the list Click the Settings icon (⚙️) Modify settings as needed Click Save Changes Note: The data source name cannot be changed after creation. Deleting Data Sources Click the Trash icon (\uD83D\uDDD1️) for the data source Confirm deletion in the dialog Warning: Deleting a data source affects any data pipelines or agents using it. Verify dependencies before deleting. Access Control Configure who can access and manage the data source: Open the data source for editing Click Access Control at the top right Add or remove role assignments Permission Description FoundationaLLM.DataSource/dataSources/read View the data source FoundationaLLM.DataSource/dataSources/write Edit configuration FoundationaLLM.DataSource/dataSources/delete Delete the data source Best Practices Security Use Azure Identity (Managed Identity) for production environments Avoid embedding secrets in connection strings when possible Apply principle of least privilege to folder access Organization Use descriptive names that indicate the data content Document the purpose in the description field Group related data in logical folder structures Performance Limit folder scope to only necessary paths Consider separate data sources for different data types Troubleshooting Connection Test Fails Verify authentication credentials are correct Check network connectivity to the storage endpoint Ensure firewall rules allow access from FoundationaLLM services Data Not Accessible Verify folder paths are correct Check that the service principal or managed identity has appropriate permissions Review Azure Storage access policies Related Topics Azure Data Lake Knowledge Source SharePoint Online Creating Data Pipelines"
  },
  "docs/management-portal/how-to-guides/data/knowledge-sources/azure-data-lake.html": {
    "href": "docs/management-portal/how-to-guides/data/knowledge-sources/azure-data-lake.html",
    "title": "Azure Data Lake as a Knowledge Source | FoundationaLLM",
    "summary": "Azure Data Lake as a Knowledge Source Learn how to configure Azure Data Lake Storage Gen2 as a knowledge source for FoundationaLLM agents. Overview Azure Data Lake Storage Gen2 (ADLS) provides scalable, secure storage for large volumes of documents. It's ideal for: Large document repositories Structured data files (CSV, JSON, Parquet) Enterprise content management Multi-team data sharing Prerequisites Before configuring ADLS as a knowledge source: Azure Data Lake Storage Gen2 account with hierarchical namespace enabled Container created for your documents Access permissions configured (see Authentication Options) Network connectivity from FoundationaLLM services to storage account Firewall rules allowing access (if enabled) Configuring ADLS as a Data Source Step 1: Navigate to Data Sources In the Management Portal sidebar, click Data Sources Click Create Data Source Select Azure Data Lake from the type dropdown Step 2: Enter Basic Information Field Description Data Source Name Unique identifier (e.g., corporate-docs-adls) Description Purpose and contents (e.g., \"Corporate policy documents\") Step 3: Select Authentication Type Choose the authentication method: Type Description Best For Azure Identity Managed Identity authentication Production deployments Account Key Storage account access key Development, testing Connection String Full connection string Legacy configurations Step 4: Configure Connection Details For Azure Identity (Managed Identity) Field Value Authentication Type Azure Identity Account Name Your storage account name (e.g., mystorageaccount) For Account Key Field Value Authentication Type Account Key API Key Storage account access key Endpoint Storage endpoint URL (e.g., https://mystorageaccount.dfs.core.windows.net) For Connection String Field Value Authentication Type Connection String Connection String Full storage connection string Step 5: Specify Folders Enter the container and folder paths containing your documents: Type the path (e.g., mycontainer/documents/policies) Press Enter or , to add it Repeat for additional paths Path Format Examples: Container root: mycontainer Single folder: mycontainer/documents Nested folder: mycontainer/departments/hr/policies Step 6: Save the Data Source Click Create Data Source to save the configuration. Authentication Configuration Using Managed Identity (Recommended) Managed Identity provides secure, credential-free access: Ensure the FoundationaLLM deployment has a managed identity Grant the managed identity the Storage Blob Data Reader role on the storage account Select Azure Identity authentication in the data source configuration Azure CLI Example: az role assignment create \\ --assignee <managed-identity-object-id> \\ --role \"Storage Blob Data Reader\" \\ --scope /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Storage/storageAccounts/<storage-account> Using Account Keys For development or when managed identity isn't available: Navigate to your storage account in Azure Portal Go to Access Keys Copy a key value Enter it in the API Key field Warning: Account keys provide full access. Rotate them regularly and use managed identity in production. Using Connection Strings Connection strings are useful for quick configuration: Navigate to your storage account in Azure Portal Go to Access Keys Copy the connection string Enter it in the Connection String field Folder Structure Best Practices Organizing Documents mycontainer/ ├── departments/ │ ├── hr/ │ │ ├── policies/ │ │ └── procedures/ │ └── finance/ │ ├── reports/ │ └── guidelines/ └── shared/ └── templates/ Path Selection Tips Specific paths reduce processing time and improve relevance Multiple paths can be specified for different content types Avoid very deep nesting which complicates management Using in Data Pipelines After creating the data source: Navigate to Data Pipelines Create a new pipeline Select your ADLS data source Configure processing stages Run the pipeline to index documents Supported File Types ADLS data sources typically support: Category Extensions Documents PDF, DOCX, DOC, PPTX, PPT, XLSX, XLS Text TXT, MD, HTML, XML, JSON Data CSV, TSV, Parquet Specific support depends on the data pipeline configuration. Troubleshooting Authentication Failures Verify managed identity has correct role assignments Check account key hasn't been rotated Ensure connection string is complete and correct Access Denied Errors Verify folder paths are correct Check container exists and is accessible Review storage account firewall settings Network Connectivity Issues Ensure FoundationaLLM services can reach the storage endpoint Check VNet configurations if using private endpoints Review firewall allow lists Related Topics Data Sources Creating Data Pipelines Private Storage"
  },
  "docs/management-portal/how-to-guides/data/knowledge-sources/image-description.html": {
    "href": "docs/management-portal/how-to-guides/data/knowledge-sources/image-description.html",
    "title": "Image Description | FoundationaLLM",
    "summary": "Image Description Learn about LLM-generated image description capabilities for processing visual content in FoundationaLLM. Overview FoundationaLLM leverages Large Language Models (LLMs) with vision capabilities to process images, extract textual content, and generate rich descriptions. This makes visual content searchable and accessible to agents, enabling knowledge retrieval from image-based documents. LLM-Generated Image Descriptions FoundationaLLM uses vision-capable LLMs to analyze images and generate detailed textual descriptions. These descriptions: Are generated using models like GPT-4 Vision or Claude Vision Can describe image content up to the model's context window limits Are stored as searchable text in your knowledge base Enable semantic search across visual content Description Quality Factors Factor Impact Model Size Larger models produce more detailed, accurate descriptions Token Allocation More tokens allow longer, richer descriptions Image Resolution Higher resolution enables finer detail recognition Image Complexity Simple images are described more accurately Capabilities Capability Description OCR (Optical Character Recognition) Extract text visible in images LLM Image Description Generate natural language descriptions using vision models Visual Q&A Answer questions about image content Content Summarization Create concise summaries of complex visual content Supported Image Formats Format Extension Notes JPEG .jpg, .jpeg Most common photo format PNG .png Supports transparency GIF .gif Static images only BMP .bmp Uncompressed bitmap TIFF .tiff, .tif High-quality images WebP .webp Modern web format Use Cases Document Processing Scanned Documents: Extract text from scanned PDFs and images Forms: Process filled-out forms and extract field values Receipts/Invoices: Digitize paper documents Visual Content Indexing Diagrams: Make technical diagrams searchable Charts: Extract data from chart images Screenshots: Index screenshot content Accessibility Alt Text Generation: Create descriptions for accessibility Image Cataloging: Describe and categorize image libraries Configuration Prerequisites A vision-capable AI model configured (e.g., GPT-4 Vision, Claude Vision) Data pipeline with image processing stages Data Pipeline Configuration To process images in a data pipeline: Create or Edit a Data Pipeline Select a Text Extraction Plugin that supports images Configure Vision Model for image processing Set Quality Parameters: Parameter Description Detail Level Low, medium, or high detail extraction Max Tokens Token limit for generated descriptions Stage Configuration TODO: Document specific image processing stage configuration options in data pipelines. Model Considerations Token Limits and Model Size Image processing consumes tokens from your AI model allocation. The quality and length of generated descriptions depends on your model configuration: Model Tier Typical Token Limit Description Quality GPT-4 Vision Up to 128K tokens Highly detailed, comprehensive descriptions GPT-4o Up to 128K tokens Fast, accurate descriptions Claude 3 Vision Up to 200K tokens Extended context for complex images Key considerations: Higher detail levels use more tokens per image Large images may require resizing before processing Batch processing can optimize throughput Token limits apply to both input (image) and output (description) Model Capabilities Model OCR Description Analysis Max Image Size GPT-4 Vision ✅ ✅ ✅ 20MB GPT-4o ✅ ✅ ✅ 20MB Claude 3 Vision ✅ ✅ ✅ 20MB Note: Actual limits depend on your specific model deployment configuration. Contact your administrator for deployment-specific limits. Resolution Considerations Resolution Processing Quality Cost Low Fast Basic Lower Medium Moderate Good Medium High Slower Best Higher Best Practices Image Quality Use high-resolution images when text extraction is critical Ensure good contrast between text and background Avoid heavily compressed images Processing Optimization Batch similar images together Use appropriate detail levels for your use case Consider preprocessing (cropping, enhancement) for poor quality sources Content Organization Store images with meaningful filenames Group related images in folders Include metadata when available Integration with Agents After processing, image content becomes available to agents through: Vector Search: Descriptions are embedded and searchable Direct Analysis: Vision-capable agents can analyze images directly Contextual Answers: Agents can reference image content in responses Limitations Limitation Description Handwriting Variable quality depending on legibility Complex Layouts Tables and forms may need specialized processing Image Quality Poor quality reduces extraction accuracy Language Support OCR accuracy varies by language Troubleshooting Poor OCR Results Check image quality and resolution Ensure text has good contrast Consider preprocessing to enhance clarity Missing Descriptions Verify vision model is properly configured Check pipeline stage configuration Review model token limits Processing Failures Check image format is supported Verify image isn't corrupted Review size limits for your deployment Related Topics Creating Data Pipelines Data Sources AI Models Configuration"
  },
  "docs/management-portal/how-to-guides/data/knowledge-sources/knowledge-graph-integration.html": {
    "href": "docs/management-portal/how-to-guides/data/knowledge-sources/knowledge-graph-integration.html",
    "title": "Knowledge Graph Integration | FoundationaLLM",
    "summary": "Knowledge Graph Integration Learn how to use knowledge graphs as a knowledge source for FoundationaLLM agents. Overview Knowledge graphs provide structured, relationship-rich data that enhances agent capabilities: Semantic Relationships: Understand connections between entities Contextual Answers: Provide richer, more accurate responses Graph Traversal: Follow relationships to find related information Supported Graph Formats Format Description Neo4j Popular graph database Azure Cosmos DB (Gremlin API) Azure-native graph database RDF/SPARQL W3C standard semantic web format Custom JSON-LD Linked data format Use Cases Scenario Benefits Organizational Data Employee relationships, reporting structures Product Catalogs Product relationships, accessories, alternatives Research Citation networks, topic relationships Customer Data Customer relationships, interaction history Configuration Overview Prerequisites A knowledge graph database populated with your data Network connectivity from FoundationaLLM to the graph database Authentication credentials configured Integration Steps Prepare Your Knowledge Graph - Ensure data follows expected schema Create a Data Source - Configure connection to the graph database Create a Data Pipeline - Process graph data for agent use Configure Agent - Associate the knowledge source with your agent Data Source Configuration TODO: Document specific data source configuration steps for knowledge graph types when available in the UI, including connection parameters, authentication, and query configuration. Connection Settings Setting Description Endpoint Graph database endpoint URL Authentication Credentials or managed identity Database/Graph Name Specific database or graph to query Schema Requirements Node Structure Nodes should include identifiable properties: { \"id\": \"unique-node-identifier\", \"type\": \"Person\", \"properties\": { \"name\": \"John Smith\", \"title\": \"Software Engineer\", \"department\": \"Engineering\", \"description\": \"Senior developer specializing in AI systems\" } } Required Properties: Property Description id Unique identifier for the node type Entity type/label name Human-readable name description Text description for search/context Relationship Structure Relationships connect nodes with semantic meaning: { \"source\": \"person-123\", \"target\": \"person-456\", \"type\": \"REPORTS_TO\", \"properties\": { \"since\": \"2023-01-15\" } } Relationship Components: Component Description source Starting node ID target Ending node ID type Relationship label (e.g., REPORTS_TO, KNOWS, CONTAINS) properties Optional metadata about the relationship Data Pipeline Configuration Graph Processing Stages TODO: Document specific pipeline stages for processing knowledge graphs. Common stages include: Graph Query - Execute queries to extract relevant data Entity Extraction - Convert nodes to searchable documents Embedding - Generate vector embeddings for semantic search Indexing - Store in vector database for retrieval Query Patterns Cypher (Neo4j) Example: MATCH (p:Person)-[r:WORKS_IN]->(d:Department) RETURN p.name, p.description, d.name AS department Gremlin (Cosmos DB) Example: g.V().hasLabel('Person').out('works_in').hasLabel('Department') Best Practices Schema Design Practice Description Consistent Types Use standard entity type names Meaningful Relationships Clear, semantic relationship labels Rich Properties Include descriptive text for search Avoid Deep Nesting Flatten complex structures when possible Query Optimization Limit result set sizes Use indexes on frequently queried properties Consider materialized views for complex queries Content Quality Include descriptive text in node properties Keep descriptions concise but informative Update graph data regularly Integration Patterns Direct Query Agents query the graph database directly for specific relationships. Vector Search Graph data is processed into vector embeddings for semantic search. Hybrid Approach Combine vector search with graph traversal for comprehensive answers. Troubleshooting Connection Issues Verify endpoint URL is correct Check authentication credentials Review network/firewall configuration Query Failures Validate query syntax for your graph database Check for schema mismatches Review timeout settings for large queries Poor Results Ensure node descriptions are meaningful Check that relevant data is being indexed Review embedding quality Limitations Limitation Description Graph Size Very large graphs may require query optimization Real-time Sync Graph changes may not immediately reflect Complex Queries Some multi-hop queries may be slow Related Topics Data Pipelines Data Sources Azure Data Lake Knowledge Source"
  },
  "docs/management-portal/how-to-guides/data/knowledge-sources/private-storage.html": {
    "href": "docs/management-portal/how-to-guides/data/knowledge-sources/private-storage.html",
    "title": "Private Storage for Agents | FoundationaLLM",
    "summary": "Private Storage for Agents Learn how to configure private storage for agent-specific data in FoundationaLLM. Overview Private storage provides dedicated, isolated storage containers for individual agents. This enables: Data Isolation: Keep agent-specific data separate from shared resources Access Control: Granular permissions for who can access agent data Governance: Meet compliance requirements for data segregation Use Cases Scenario Description Department-specific agents HR, Finance, Legal agents with sensitive data Project isolation Separate data for different projects or clients Compliance requirements Data that must be isolated for regulatory reasons Development/Testing Isolated environments for agent development Accessing Private Storage Configuration Private storage is configured at the agent level: Navigate to Agents in the sidebar Edit an existing agent or create a new one Look for the Private Storage button in the agent configuration header Configuring Private Storage Prerequisites An agent using a workflow that supports private storage OpenAI Assistants workflow FoundationaLLM Function Calling workflow Appropriate permissions to configure the agent Setup Steps Access Private Storage Settings Open the agent for editing Click the Private Storage button The Private Storage configuration panel appears Configure Storage Location TODO: Document specific storage configuration options available in the Private Storage dialog, including: Storage account selection Container naming Folder structure Set Access Permissions Define who can read/write to the private storage Configure role-based access as needed Save Configuration Apply the changes Storage is provisioned for the agent Tool Association Private storage is typically associated with specific tools: When configuring tools (e.g., Knowledge Tool), you can specify the private storage The Tools parameter in the Private Storage dialog shows which tools use this storage Data Management Uploading Data Data in private storage can be populated through: File Uploads: Users upload files through the Chat User Portal Data Pipelines: Process and index data into the private storage API Integration: Programmatic data upload via APIs Data Organization Best practices for organizing private storage: agent-private-storage/ ├── uploads/ # User-uploaded files ├── processed/ # Pipeline-processed content └── indexes/ # Vector indexes and embeddings Access Control Agent-Level Permissions Private storage inherits the agent's access control settings: Users with agent access can interact with its private storage Owners can manage storage configuration Contributors can upload and access data Resource-Level Permissions TODO: Document specific resource-level permissions for private storage if available. Security Considerations Practice Description Least Privilege Grant only necessary access to private storage Regular Review Periodically audit who has access Data Classification Understand what data sensitivity level is stored Encryption Ensure data is encrypted at rest and in transit Monitoring and Maintenance Storage Usage TODO: Document how to monitor private storage usage and capacity. Cleanup Remove unused files periodically Archive old data as needed Follow retention policies Troubleshooting Storage Not Available Verify the agent workflow supports private storage Check that storage was properly provisioned Review permissions configuration Upload Failures Check storage capacity Verify user has upload permissions Review file size limits Data Not Searchable Ensure data pipeline processed the content Check indexing completed successfully Verify tool configuration points to correct storage Related Topics Azure Data Lake as a Knowledge Source Data Sources Instance Access Control Create New Agent"
  },
  "docs/management-portal/how-to-guides/data/knowledge-sources/sharepoint-online.html": {
    "href": "docs/management-portal/how-to-guides/data/knowledge-sources/sharepoint-online.html",
    "title": "SharePoint Online as a Knowledge Source | FoundationaLLM",
    "summary": "SharePoint Online as a Knowledge Source Learn how to use SharePoint Online and OneDrive with FoundationaLLM agents. Overview FoundationaLLM provides two ways to integrate with SharePoint Online and OneDrive: Approach Description Best For End-User OneDrive Upload Users upload files directly from OneDrive in the Chat User Portal Ad-hoc document analysis, individual user files Backend Knowledge Source Administrators configure SharePoint as a data source for agents Organization-wide knowledge bases, departmental content This guide covers both approaches to help you choose the right integration method for your needs. Approach 1: End-User OneDrive Integration End users can upload files directly from their OneDrive (Work or School) account through the Chat User Portal. This allows users to share personal documents with agents for analysis without requiring administrator configuration. How It Works Users click the paperclip icon (\uD83D\uDCCE) in the Chat User Portal Click Select file from OneDrive (or Connect to OneDrive if not yet connected) Authenticate with their work/school Microsoft account Browse and select files from their OneDrive Files are uploaded to the conversation for agent analysis User Experience When using OneDrive upload: Files are uploaded to the current conversation only Documents are processed by the agent for that session This is ideal for ad-hoc questions about specific documents Files are not permanently indexed into a knowledge base Prerequisites for OneDrive Upload For users to access the OneDrive upload feature: OneDrive for Business must be enabled for your organization Microsoft Graph permissions must be configured for the User Portal Users must have work or school accounts (not personal Microsoft accounts) Enabling OneDrive Integration OneDrive integration is configured at the platform level. Contact your administrator to: Ensure the User Portal app registration has the required Graph permissions Verify OneDrive for Business is available to users Check that any conditional access policies allow the integration For More Information See Uploading Files to a Conversation for detailed user instructions. Approach 2: Backend SharePoint Knowledge Source Administrators can configure SharePoint Online as a backend data source to provide persistent knowledge for agents. This approach indexes SharePoint document libraries into a searchable knowledge base. When to Use Backend Integration This approach is ideal for: Team collaboration documents Policy and procedure libraries Project documentation Departmental content that should be accessible to all users of an agent Prerequisites Before configuring SharePoint as a backend knowledge source: SharePoint Online site with documents to index Microsoft Entra ID (Azure AD) app registration with appropriate permissions Consent granted for the app to access SharePoint Network connectivity to Microsoft 365 services Management Portal access with appropriate permissions Required Permissions The app registration needs the following Microsoft Graph permissions: Permission Type Description Sites.Read.All Application Read items in all site collections Files.Read.All Application Read all files user can access Note: Permissions may vary based on your organization's requirements. Consult with your Microsoft 365 administrator. Configuring SharePoint as a Data Source Step 1: Navigate to Data Sources In the Management Portal sidebar, click Data Sources Click Create Data Source Select SharePoint Online Site from the type dropdown Step 2: Enter Basic Information Field Description Data Source Name Unique identifier (e.g., hr-sharepoint) Description Purpose and contents (e.g., \"HR department documents\") Step 3: Configure Connection Settings Field Description Site URL Full URL to the SharePoint site (e.g., https://contoso.sharepoint.com/sites/HR) Authentication Type Authentication method (Managed Identity or App Registration) Step 4: Configure Authentication Using Managed Identity: Ensure your deployment has a managed identity Grant the managed identity access to SharePoint via Microsoft Graph Select Azure Identity or Managed Identity authentication Using App Registration: Create or use an existing Azure AD app registration Configure the required permissions Grant admin consent Enter the client credentials in the data source configuration TODO: Document specific fields for app registration authentication (Client ID, Client Secret, Tenant ID) when visible in the UI. Step 5: Specify Document Library Indicate which document libraries or folders to access: Field Description Document Library Name of the library (e.g., Documents, Shared Documents) Folder Path Specific folder within the library (optional) Step 6: Save the Data Source Click Create Data Source to save the configuration. Creating an App Registration Navigate to Azure Portal > Microsoft Entra ID > App registrations Click New registration Enter a name (e.g., FoundationaLLM-SharePoint-Reader) Select Accounts in this organizational directory only Click Register Configuring Permissions In the app registration, go to API permissions Click Add a permission Select Microsoft Graph Choose Application permissions Add required permissions (Sites.Read.All, Files.Read.All) Click Add permissions Click Grant admin consent (requires admin privileges) Creating Client Secret In the app registration, go to Certificates & secrets Click New client secret Enter a description and expiration Copy the secret value immediately (it won't be shown again) Document Library Paths Common Path Formats Library Type Path Format Default Documents Shared Documents or Documents Custom Library Library display name Subfolder Documents/Policies Finding the Correct Path Navigate to your SharePoint site Open the document library Note the library name from the URL or navigation Using in Data Pipelines After creating the data source: Navigate to Data Pipelines Create a new pipeline Select your SharePoint data source Configure processing stages (text extraction, embedding, etc.) Run the pipeline to index documents Supported File Types SharePoint data sources typically support: Category Extensions Office Documents DOCX, DOC, XLSX, XLS, PPTX, PPT PDF PDF Text TXT, MD, HTML Sync and Updates Initial Indexing: The first pipeline run indexes all documents in the specified libraries. Incremental Updates: TODO: Document incremental sync capabilities and how to handle document updates/deletions. Comparison: Which Approach to Use Feature OneDrive Upload Backend Knowledge Source Setup Required Minimal (user-driven) Administrator configuration Persistence Per-conversation only Permanent knowledge base Scope Individual user files Organization/team documents Use Case Ad-hoc analysis Structured knowledge retrieval Data Pipeline Not required Required for indexing File Access User's own OneDrive Configured SharePoint sites Best Practice Recommendations Use OneDrive Upload when users need to analyze their own documents on-demand Use Backend Knowledge Source when you need agents to have access to shared organizational content You can use both approaches together — agents can search backend knowledge while users also upload additional documents Troubleshooting OneDrive Upload Issues Can't connect to OneDrive: Verify you're using a work or school account Check that OneDrive for Business is enabled Clear browser cache and try again Files not showing in picker: Ensure files are in your OneDrive, not SharePoint Check file permissions Verify OneDrive sync is complete Backend Data Source Issues Authentication Failures: Verify app registration credentials Check admin consent was granted Ensure client secret hasn't expired Confirm tenant ID is correct Access Denied: Verify the site URL is correct Check the app has Sites.Read.All permission Ensure the document library name is correct Documents Not Found: Verify folder paths are correct Check documents aren't in a subfolder not included Ensure documents aren't checked out or locked Connectivity Issues: Verify network allows access to SharePoint Online Check for conditional access policies blocking access Review Microsoft 365 service health Security Considerations Use application permissions rather than delegated for background processing Apply principle of least privilege Regularly rotate client secrets Monitor access through Azure AD audit logs Review which SharePoint sites are exposed to agents Related Topics Uploading Files to a Conversation Data Sources Creating Data Pipelines Azure Data Lake Knowledge Source"
  },
  "docs/management-portal/how-to-guides/fllm-platform/branding.html": {
    "href": "docs/management-portal/how-to-guides/fllm-platform/branding.html",
    "title": "Branding Configuration | FoundationaLLM",
    "summary": "Branding Configuration Learn how to customize the look and feel of your FoundationaLLM deployment. Overview Branding allows you to personalize both the Management Portal and Chat User Portal to match your organization's visual identity. Customizable elements include logos, colors, text, and messages. Accessing Branding Settings In the Management Portal sidebar, click Branding under the FLLM Platform section The branding configuration page loads with all customizable options Branding Configuration Interface The branding page is organized into sections: General Settings - Logo, text, and messages Color Settings - Color scheme configuration Action Buttons - Reset, Set Default, and Save Contrast Information Toggle Enable Show contrast information to see WCAG accessibility compliance indicators for your color combinations. General Branding Settings Setting Description Company Name Organization name displayed in the portal FavIcon URL Path to the browser tab icon Footer Text Text displayed in the portal footer (supports rich text) Kiosk Mode Toggle for kiosk/public display mode Logo Text Text to display if logo image is unavailable Logo URL Path to your organization's logo image Page Title Browser tab/window title Agent Icon URL Default icon for agents No Agents Message Message shown when no agents are available (supports rich text) Default Agent Welcome Message Default greeting for agents without custom messages (supports rich text) Rich Text Fields Footer Text, No Agents Message, and Default Agent Welcome Message support rich text formatting: Bold, italic, underline Links Lists Basic HTML Color Settings Colors are grouped by their functional area: Color Groups Group Background Text Accent Accent Color Accent Text Color Background Background Color - Primary Primary Color Primary Text Color Secondary Secondary Color Secondary Text Color Primary Button Primary Button Background Primary Button Text Secondary Button Secondary Button Background Secondary Button Text Color Input Methods Colors can be entered as: Hex: #RRGGBB or #RGB (e.g., #131833) RGB: rgb(R, G, B) (e.g., rgb(19, 24, 51)) Use the color picker for visual selection or enter values directly. WCAG Accessibility Compliance When Show contrast information is enabled: Indicator Description Contrast Ratio Numerical ratio (e.g., \"4.5:1\") AA Pass/Fail for WCAG AA standard (4.5:1 minimum) AAA Pass/Fail for WCAG AAA standard (7:1 minimum) Green = Passes the standard Red = Fails the standard Default Color Values Setting Default Value Primary Color #131833 Primary Text Color #fff Secondary Color #334581 Secondary Text Color #fff Accent Color #fff Accent Text Color #131833 Background Color #fff Primary Button Background #5472d4 Primary Button Text #fff Secondary Button Background #70829a Secondary Button Text #fff Logo Configuration Logo Requirements Format: SVG, PNG, or JPG recommended Size: Max height ~100px in header display Background: Consider transparency for SVG/PNG Logo Preview The page shows a live preview of your logo against the Primary Color background. Logo Path The logo URL is relative to the portal's public directory. Common patterns: foundationallm-logo-white.svg (default) custom-logo.png (custom uploaded) TODO: Document the process for uploading custom logo files to the portal. Applying Changes Save Changes Make your branding modifications Click Save to apply changes Changes are saved to Azure App Configuration Users see updates after cache refresh (may require browser refresh) Reset Changes Reset: Reverts all unsaved changes to previously saved values Confirmation dialog appears before reset Set Default Set Default: Reverts all branding values to FoundationaLLM defaults Confirmation dialog appears before reset Still requires Save to apply Best Practices Visual Identity Use consistent colors from your brand guidelines Ensure logos are high quality and appropriate size Test appearance on both light and dark backgrounds (if applicable) Accessibility Maintain sufficient contrast ratios (aim for WCAG AA minimum) Test with the contrast information toggle enabled Consider users with visual impairments Testing Preview changes in the Management Portal Check the Chat User Portal appearance Test on different screen sizes/devices Verify readability of all text elements Configuration Methods Branding can be configured through multiple methods: Method Use Case Management Portal Interactive, visual configuration Azure App Configuration Programmatic, infrastructure-as-code REST API Automation and integration See reference documentation for alternative methods. Troubleshooting Changes Not Appearing Click Save to ensure changes are persisted Refresh the browser to clear cached styles Check browser developer tools for CSS loading issues Logo Not Displaying Verify the logo URL path is correct Check that the file exists in the public directory Ensure the file format is supported Colors Look Wrong Verify hex/RGB format is correct Check for transparency issues with logo Test in different browsers Related Topics Branding Reference Using App Configuration for Branding Using Management Portal for Branding Using REST API for Branding"
  },
  "docs/management-portal/how-to-guides/fllm-platform/configuration.html": {
    "href": "docs/management-portal/how-to-guides/fllm-platform/configuration.html",
    "title": "Configuration | FoundationaLLM",
    "summary": "Configuration Learn how to manage platform configuration and portal access settings. Overview The Configuration page allows administrators to manage portal access permissions and instance-level settings for the FoundationaLLM deployment. Accessing Configuration In the Management Portal sidebar, click Configuration under the FLLM Platform section The configuration page loads with access control settings Portal Access Configuration The Configuration page provides access control for both portals: User Portal Access Control who can access the Chat User Portal: Setting Description Scope providers/FoundationaLLM.Configuration/appConfigurationSets/UserPortal Role Reader (required for portal access) Management Portal Access Control who can access the Management Portal: Setting Description Scope providers/FoundationaLLM.Configuration/appConfigurationSets/ManagementPortal Role Reader (required for portal access) Managing Portal Access Granting Access Click Access Control for the desired portal section In the dialog, click Add Role Assignment Configure the assignment: Principal Type: User, Group, or Service Principal Principal ID/Email: The Azure AD identifier Role: Reader (for portal access) Click Save Revoking Access Click Access Control for the portal section Find the role assignment in the list Click the delete icon Confirm removal Role Requirements Portal Required Role Result User Portal Reader on UserPortal config Can access Chat User Portal Management Portal Reader on ManagementPortal config Can access Management Portal Note: Users without at least Reader role on the appropriate configuration set will not be able to access the portal. Bulk Access Management For managing access for multiple users: Create an Azure AD security group Add users to the group Grant the Reader role to the group All group members gain portal access Access Patterns Self-Service User Portal Grant Reader role to All Users or a broad group for self-service agent access. Restricted Management Portal Limit Management Portal access to: IT Administrators Platform operators Development teams Department-Level Access Create groups per department: FLLM-Users-HR FLLM-Users-Finance FLLM-Admins Troubleshooting User Cannot Access Portal Verify the user's email/principal ID Check role assignments in Access Control Ensure the user is in an assigned group Verify Azure AD sync is current Access Control Changes Not Applying User may need to sign out and sign back in Token cache may need to clear (wait 5-10 minutes) Check for conflicting role assignments Groups Not Appearing Verify the group exists in Azure AD Check that group sync is enabled Enter the group's Object ID directly Related Topics Instance Access Control Permissions and Roles Reference Deployment Information"
  },
  "docs/management-portal/how-to-guides/fllm-platform/deployment-information.html": {
    "href": "docs/management-portal/how-to-guides/fllm-platform/deployment-information.html",
    "title": "Deployment Information | FoundationaLLM",
    "summary": "Deployment Information Learn how to view deployment details and monitor API health for your FoundationaLLM instance. Overview The Deployment Information page provides visibility into your FoundationaLLM deployment, including instance identification and API health status. Accessing Deployment Information In the Management Portal sidebar, click Deployment Information under the FLLM Platform section The deployment information page displays Instance Information Instance ID The page displays your unique Instance ID: Used to identify your specific FoundationaLLM deployment Required when contacting support or troubleshooting Used internally for API routing and configuration API Status The page displays status cards for core APIs: Management API Field Description Name Management API Description Used by the Management Portal to manage the FoundationaLLM platform Status Endpoint /{instanceId}/status Authorization API Field Description Name Authorization API Description Manages role-based access control (RBAC) and auth-related functions Status Endpoint /status API Status Indicators Each API card shows: Indicator Meaning Green/Online API is responding normally Red/Offline API is not responding Yellow/Degraded API is responding but with issues Monitoring API Health Real-Time Status The API status cards refresh automatically or can be manually refreshed: View the current status on each card Check the timestamp of the last check Refresh the page to get updated status Status Endpoints You can also check API status directly: Management API: GET {ManagementAPIUrl}/instances/{instanceId}/status Authorization API: GET {AuthorizationAPIUrl}/status Use Cases Troubleshooting When experiencing issues: Check API status on this page Note which APIs are offline or degraded Provide this information to support Monitoring For ongoing health monitoring: Periodically check this page Set up external monitoring to ping status endpoints Configure alerts for API unavailability Documentation When documenting your deployment: Record the Instance ID Note API endpoints for reference Document expected normal status Additional APIs TODO: Document additional API status cards if they become available, such as: Core API Orchestration API Gateway API Troubleshooting API Shows Offline Check network connectivity Verify the API service is running in Azure Review Azure service health Check for deployment/update in progress Status Not Updating Refresh the browser page Check browser console for errors Verify authentication is still valid Cannot Access Page Verify you have Management Portal access Check your role assignments Try signing out and back in Related Topics Configuration APIs & SDKs Overview"
  },
  "docs/management-portal/how-to-guides/fllm-platform/status-messages.html": {
    "href": "docs/management-portal/how-to-guides/fllm-platform/status-messages.html",
    "title": "Publishing Status Messages | FoundationaLLM",
    "summary": "This article is still being authored. Some sections contain placeholder content that requires additional information. Publishing Status Messages Learn how to publish status, outage, and maintenance messages to inform users about system availability. Overview Status messages allow IT administrators to communicate important information to users of the Chat User Portal. Use status messages to: Announce planned maintenance windows Notify users of ongoing outages or issues Communicate system updates or changes Provide important operational information Message Types Type Icon Purpose Use When Information ℹ️ General announcements Sharing news, tips, or updates Warning ⚠️ Potential issues Planned maintenance, degraded performance Error/Outage ❌ Active problems System outages, critical issues Success ✅ Resolution notices Issues resolved, maintenance completed Accessing Status Message Management TODO: Document the exact navigation path to status message management in the Management Portal. Log into the Management Portal Navigate to FLLM Platform > Status Messages (or equivalent) The status message management page opens Creating a Status Message Step 1: Start New Message Click Create Status Message or New Message The message creation form opens Step 2: Configure Message Content Field Description Requirements Title Brief headline for the message Required. Keep concise. Message Body Full content of the status message Required. Supports rich text. Message Type Severity/category of the message Required. See types above. Priority Display priority if multiple messages Optional. Higher = more prominent. Step 3: Set Display Options Visibility Settings Setting Description Start Date/Time When the message becomes visible End Date/Time When the message automatically hides Active Toggle to immediately show/hide the message Target Audience TODO: Document audience targeting options if available (e.g., all users, specific groups, specific agents). Step 4: Preview and Publish Review the message content and settings Use the preview option to see how it will appear Click Publish or Save to activate the message Managing Existing Messages Viewing Active Messages The status messages list shows: Column Description Title Message headline Type Message severity/category Status Active, Scheduled, or Expired Start/End Display window Actions Edit, Deactivate, Delete Editing a Message Find the message in the list Click the Edit button Modify content or settings Save changes Changes to active messages take effect immediately. Deactivating a Message To temporarily hide a message without deleting it: Find the message in the list Click Deactivate or toggle the Active switch off The message stops displaying in the User Portal Deleting a Message Find the message in the list Click the Delete button Confirm the deletion Warning: Deleted messages cannot be recovered. Display Rules in User Portal Where Messages Appear Status messages are displayed in the Chat User Portal: TODO: Document specific display locations (e.g., banner at top, notification area, login page). Banner Display: Messages may appear as banners at the top of the portal Notification Area: Users may see notifications in the sidebar Login Page: Critical messages may appear before login Message Behavior Behavior Description Auto-dismiss Some message types may auto-hide after being read Persistent Critical messages may stay visible until resolved Dismissible Users may be able to dismiss informational messages Multiple Messages When multiple messages are active: Higher priority messages display more prominently Messages may stack or rotate Critical messages typically take precedence Scheduling Messages Planning Ahead For planned maintenance: Create the message in advance Set the Start Date/Time to when you want it visible Set the End Date/Time to when maintenance ends (plus buffer) The message will automatically appear and disappear Maintenance Window Example For a planned maintenance window on Saturday 2 AM - 6 AM: Setting Value Title Scheduled Maintenance Message The system will be unavailable for scheduled maintenance. Type Warning Start Saturday 1:45 AM (15 min early notice) End Saturday 6:30 AM (30 min buffer) Best Practices Message Content Be Clear: Use plain language everyone can understand Be Specific: Include times, durations, and affected services Be Actionable: Tell users what they should do (wait, contact support, etc.) Be Timely: Update messages as situations evolve Message Timing Scenario Advance Notice Major planned maintenance 1-2 weeks Minor maintenance 2-3 days Emergency maintenance As soon as known Outage notification Immediately Message Management Review and clean up old messages regularly Use scheduling for recurring maintenance Have templates ready for common scenarios Coordinate with support teams on messaging Templates Planned Maintenance Title: Scheduled Maintenance - [Date] The FoundationaLLM system will be unavailable for scheduled maintenance on [Date] from [Start Time] to [End Time] [Timezone]. During this time, the Chat User Portal will not be accessible. Please plan your work accordingly. We apologize for any inconvenience. Active Outage Title: Service Disruption - [Brief Description] We are currently experiencing issues with [affected service/feature]. Our team is actively working to resolve this issue. Status: [Investigating/Identified/Fixing] Estimated Resolution: [Time if known, or \"TBD\"] We will provide updates as more information becomes available. Resolution Notice Title: Resolved - [Original Issue] The [issue description] reported on [date/time] has been resolved. Normal service has been restored. Thank you for your patience. Troubleshooting Message Not Appearing Verify the message is set to Active Check the Start Date/Time hasn't passed incorrectly Confirm the message hasn't been filtered by audience settings Clear browser cache and refresh the User Portal Message Won't Save Check all required fields are filled Verify date/time formats are correct Ensure you have appropriate permissions Users Not Seeing Updates Changes may take a few minutes to propagate Users may need to refresh their browser Check if caching is affecting message display Related Topics Branding Configuration — Customize portal appearance Configuration Settings — Platform configuration options Viewing Status Messages — End-user guide"
  },
  "docs/management-portal/how-to-guides/managing-plugins.html": {
    "href": "docs/management-portal/how-to-guides/managing-plugins.html",
    "title": "Managing Plugins | FoundationaLLM",
    "summary": "Managing Plugins Learn how to manage plugins and plugin packages to extend FoundationaLLM functionality. Overview Plugins extend FoundationaLLM with custom capabilities: Custom Workflows: Agent orchestration logic Custom Tools: Specialized agent capabilities Data Processing: Custom pipeline stages and extractors Plugin Architecture Plugin Types Type Language Purpose Agent Workflow Python Custom orchestration workflows Agent Tool Python Custom tools for agents Data Source .NET Custom data source connectors Pipeline Stage .NET Custom data pipeline processing Text Extraction .NET Custom content extraction Text Partitioning .NET Custom text chunking Plugin Packages Plugins are distributed as packages: Package Type Format Platform Python ZIP (Wheel planned) Agent workflows and tools .NET NuGet Data pipeline components Managing Plugin Packages TODO: Document the specific UI for plugin management in the Management Portal when available. Viewing Available Plugins Navigate to plugin management (if available in UI) View list of installed packages Check plugin status and versions Installing Plugin Packages Obtain the plugin package file Upload to the plugin storage location Configure package settings Enable the plugin Updating Plugins Upload the new version Update configuration if needed Test with affected agents/pipelines Enable the updated version Removing Plugins Verify no active resources use the plugin Disable the plugin Remove from storage Using Plugins in Agents Workflow Plugins Reference in agent workflow configuration: Field Description Workflow Package Name Python package name Workflow Class Name Class implementing the workflow Example: Package: foundationallm_agent_plugins Class: FoundationaLLMFunctionCallingWorkflow Tool Plugins Reference in agent tool configuration: Field Description Tool Package Name Python package name Tool Class Name Class implementing the tool Examples: Tool Package Class Code Interpreter foundationallm_agent_plugins FoundationaLLMCodeInterpreterTool Knowledge Tool foundationallm_agent_plugins FoundationaLLMKnowledgeTool Using Plugins in Data Pipelines Data Source Plugins Configure when creating data sources: Select the data source type Choose the appropriate plugin Configure plugin parameters Pipeline Stage Plugins Configure in pipeline stage settings: Add a pipeline stage Select the stage plugin Configure stage parameters Text Processing Plugins For content extraction and partitioning: Plugin Type Use Case Text Extraction Extract text from documents Text Partitioning Split text into chunks Image Description Generate text from images Built-in Plugins FoundationaLLM includes several built-in plugins: Agent Plugins (Python) Plugin Description FoundationaLLMFunctionCallingWorkflow Model agnostic function calling FoundationaLLMCodeInterpreterTool Python code execution FoundationaLLMKnowledgeTool Knowledge base search Data Pipeline Plugins (.NET) TODO: Document specific built-in .NET plugins for data pipelines. Custom Plugin Development Python Plugins For agent workflows and tools: Implement the required interface Package as a Python wheel or ZIP Deploy to the plugin storage Reference in agent configuration .NET Plugins For data pipeline components: Implement the required interface Package as a NuGet package Deploy to the plugin storage Configure in pipeline settings Note: See developer documentation for detailed plugin development guides. Plugin Configuration Environment Variables Some plugins use environment variables: Variable Description PLUGIN_STORAGE_PATH Location of plugin packages PLUGIN_CONFIG_PATH Plugin configuration files Plugin Settings Configure plugin-specific settings in: Agent configuration (workflow/tool parameters) Pipeline configuration (stage parameters) App Configuration (global settings) Troubleshooting Plugin Not Loading Verify package is in correct location Check package format is valid Review logs for loading errors Plugin Errors in Execution Check plugin dependencies are installed Verify configuration parameters Review agent/pipeline logs Version Conflicts Check for dependency version conflicts Ensure compatible package versions Consider isolated environments Best Practices Plugin Management Version plugins for traceability Test plugins in development before production Document plugin requirements and configuration Security Review plugin code before deployment Limit plugin permissions appropriately Monitor plugin activity Updates Test updates in non-production first Have rollback plans Document changes Related Topics Plugins & Packages Reference Agents & Workflows Creating Data Pipelines"
  },
  "docs/management-portal/how-to-guides/models-endpoints/ai-models.html": {
    "href": "docs/management-portal/how-to-guides/models-endpoints/ai-models.html",
    "title": "AI Models | FoundationaLLM",
    "summary": "AI Models Learn how to configure and manage AI model deployments in the Management Portal. Overview AI Models define the large language models (LLMs) and other AI models available to your FoundationaLLM deployment. These models power agent conversations, embeddings, and specialized capabilities like code interpretation and image generation. Accessing AI Models In the Management Portal sidebar, click AI Models under the Models and Endpoints section The models list loads, showing all configured models Model List The table displays: Column Description Name Model identifier Source Type Model provider (Azure OpenAI, Anthropic, etc.) Edit Settings icon to modify configuration Delete Trash icon to remove the model Searching and Managing Use the search box to filter by name Click the refresh button to reload the list Click column headers to sort Model Types Type Description Use Cases Chat/Completion Text generation models Agent conversations, responses Embedding Vector embedding models Semantic search, similarity Image Generation Image creation models DALL-E, image generation tools Vision Image understanding models Image analysis, OCR Creating a Model Click Create Model at the top right of the page Configure the model settings Model Configuration TODO: Document specific model configuration fields when available in the UI, including: Field Description Model Name Unique identifier Source Type Provider/platform (Azure OpenAI, Anthropic, etc.) Deployment Name Cloud deployment identifier API Endpoint Endpoint URL reference Model Parameters Default parameters (temperature, max tokens, etc.) Azure OpenAI Models For Azure OpenAI deployments: Select Azure OpenAI as the source type Configure: API endpoint reference Deployment name Model version Other Model Providers For other providers (Anthropic, custom): Select the appropriate source type Configure provider-specific settings Enter authentication details Model Configuration Details API Endpoint Association Models are associated with API endpoint configurations: Create or select an existing API endpoint Link the model to the endpoint The endpoint provides connection details Model Parameters Configure default model behavior: Parameter Description Typical Range temperature Response randomness 0.0 - 2.0 max_tokens Maximum response length 1 - model limit top_p Nucleus sampling 0.0 - 1.0 Editing Models Locate the model in the list Click the Settings icon (⚙️) Modify settings as needed Click Save Changes Deleting Models Click the Trash icon (\uD83D\uDDD1️) for the model Confirm deletion in the dialog Warning: Deleting a model affects any agents using it. Verify dependencies before deleting. Using Models in Agents Models are referenced in agent configurations: Workflow Main Model The primary model for agent conversations: In agent creation/editing, find the Workflow section Select Workflow Main Model from the dropdown Only compatible models appear Tool Models Models assigned to specific tools: In tool configuration, add a Model resource Select the model Assign a role (e.g., main_model) Access Control Configure who can access and manage models: Permission Description FoundationaLLM.AIModel/aiModels/read View models FoundationaLLM.AIModel/aiModels/write Edit models FoundationaLLM.AIModel/aiModels/delete Delete models Best Practices Naming Conventions Use descriptive names indicating model type and purpose Include version information when relevant Example: gpt-4o-chat-main, text-embedding-3-large Model Selection Use appropriate models for each task (chat vs. embedding) Consider cost and performance tradeoffs Test models before production use Parameter Configuration Set reasonable defaults Override at agent/tool level when needed Document parameter choices Troubleshooting Model Not Available in Dropdown Verify the model exists and is active Check your permissions Ensure the model type is compatible with the selection Model Responses Failing Verify API endpoint configuration Check authentication credentials Review Azure OpenAI deployment status Performance Issues Review token limits and quotas Check for rate limiting Consider model tier/capacity Related Topics API Endpoints Create New Agent Configuration Reference"
  },
  "docs/management-portal/how-to-guides/models-endpoints/api-endpoints.html": {
    "href": "docs/management-portal/how-to-guides/models-endpoints/api-endpoints.html",
    "title": "API Endpoints | FoundationaLLM",
    "summary": "API Endpoints Learn how to configure and manage API endpoint configurations in the Management Portal. Overview API Endpoints define the connection configurations for external services used by FoundationaLLM, including AI model providers, authentication services, and other backend systems. Accessing API Endpoints In the Management Portal sidebar, click API Endpoints under the Models and Endpoints section The endpoints list loads, showing all configured endpoints Endpoint List The table displays: Column Description Name Endpoint identifier Category Type of endpoint Subcategory More specific classification Edit Settings icon to modify configuration Delete Trash icon to remove the endpoint Searching and Managing Use the search box to filter by name or description Click the refresh button to reload the list Click column headers to sort Endpoint Categories Category Description LLM Large Language Model endpoints Embedding Embedding model endpoints Authorization Authentication and authorization services Storage Storage service endpoints Search Search service endpoints Creating an API Endpoint Click Create API Endpoint at the top right of the page Configure the endpoint settings Endpoint Configuration TODO: Document specific API endpoint configuration fields when available in the UI, including: Field Description Endpoint Name Unique identifier Category Endpoint type Subcategory Specific classification URL Service endpoint URL Authentication Auth configuration Azure OpenAI Endpoints For Azure OpenAI services: Field Description Endpoint URL https://<resource>.openai.azure.com/ API Version API version (e.g., 2024-02-01) Authentication API Key or Managed Identity Other Service Endpoints For other services, configure: Service URL Authentication method Required headers or parameters Authentication Configuration API Key Authentication Select API Key authentication Enter the key value (stored securely) Configure header name if non-standard Managed Identity Select Managed Identity authentication Ensure the identity has required permissions No credentials required in configuration OAuth/Token Authentication Configure token endpoint Set client credentials Configure scopes as needed Editing Endpoints Locate the endpoint in the list Click the Settings icon (⚙️) Modify settings as needed Click Save Changes Deleting Endpoints Click the Trash icon (\uD83D\uDDD1️) for the endpoint Confirm deletion in the dialog Warning: Deleting an endpoint affects any models or services referencing it. Verify dependencies before deleting. Endpoint References API Endpoints are referenced by: AI Models: Models use endpoints for API connectivity Data Sources: Some sources reference endpoints for authentication Internal Services: Platform services use configured endpoints Access Control Configure who can access and manage endpoints: Permission Description FoundationaLLM.Configuration/apiEndpointConfigurations/read View endpoints FoundationaLLM.Configuration/apiEndpointConfigurations/write Edit endpoints FoundationaLLM.Configuration/apiEndpointConfigurations/delete Delete endpoints Best Practices Naming Conventions Use descriptive names indicating service and purpose Include environment when relevant (dev, prod) Example: azure-openai-eastus-prod, search-service-main Security Use Managed Identity when possible Rotate API keys regularly Avoid hardcoding credentials Organization Group related endpoints logically Document endpoint purposes Maintain consistent naming Troubleshooting Connection Failures Verify endpoint URL is correct Check authentication credentials Review network/firewall configuration Authentication Errors Verify API key or credentials Check managed identity permissions Review token expiration Endpoint Not Available Verify the endpoint exists Check your permissions Ensure the service is running Related Topics AI Models Configuration Reference Data Sources"
  },
  "docs/management-portal/how-to-guides/security/instance-access-control.html": {
    "href": "docs/management-portal/how-to-guides/security/instance-access-control.html",
    "title": "Instance Access Control | FoundationaLLM",
    "summary": "Instance Access Control Learn how to manage access control and role assignments for your FoundationaLLM instance. Overview Instance Access Control allows you to manage who can access your FoundationaLLM deployment and what actions they can perform. The system uses Role-Based Access Control (RBAC) to define permissions. Accessing Instance Access Control In the Management Portal sidebar, click Instance Access Control under the Security section The role assignments page loads, showing all current assignments Role Assignments Table The table displays role assignments grouped by role: Column Description Name Principal name and email (if available) Type Principal type (User, Group, Service Principal) Scope Resource scope for the assignment Delete Remove the assignment Principal Type Icons Icon Type \uD83D\uDC64 User \uD83D\uDC65 Group ✓ Service Principal Role Groups Assignments are grouped by role, with an expandable header showing: Role display name Role description (hover over info icon) Available Roles Role Description Owner Full access to all resources Contributor Create and manage resources (cannot manage access) Reader View resources only User Access Administrator Manage role assignments Creating a Role Assignment Click Create Role Assignment at the top right of the page Navigate to the role assignment creation form Role Assignment Configuration TODO: Document the specific fields in the role assignment creation form, which may include: Field Description Principal Type User, Group, or Service Principal Principal ID Azure AD Object ID or email Role Role to assign Scope Resource scope for the assignment Specifying Principals Users Select User as the principal type Enter the user's email address or Object ID The system validates the principal exists Groups Select Group as the principal type Enter the security group's Object ID All group members inherit the role Service Principals Select Service Principal as the principal type Enter the service principal's Object ID Used for application/service access Selecting Scope Scope determines which resources the role applies to: Scope Level Description Instance Entire FoundationaLLM instance Resource Provider All resources of a type (e.g., all agents) Resource Specific resource (e.g., one agent) Scope Format Examples: Instance: / All Agents: providers/FoundationaLLM.Agent Specific Agent: providers/FoundationaLLM.Agent/agents/my-agent Deleting a Role Assignment Locate the assignment in the table Click the Trash icon (\uD83D\uDDD1️) Confirm deletion in the dialog: \"Are you sure you want to delete the role assignment for [principal name]?\" Click Yes to confirm Warning: Removing access immediately prevents the principal from performing actions. Ensure this is intended before confirming. Refreshing the List Click the Refresh button (\uD83D\uDD04) at the top of the table to reload role assignments. Common Access Patterns Administrator Access Grant full administrative access: Role: Owner Scope: Instance level (/) Principal: Admin user or admin group Developer Access Grant agent development access: Role: Contributor Scope: Agent resource provider Principal: Developer group Read-Only Access Grant view-only access: Role: Reader Scope: Appropriate level Principal: Viewer group Portal Access For portal access specifically: See Configuration for portal access settings Best Practices Use Groups Over Individual Users Create Azure AD security groups for role-based teams Assign roles to groups instead of individual users Simplifies management as team membership changes Apply Least Privilege Grant the minimum permissions needed Use specific scopes rather than instance-wide Review and remove unnecessary assignments regularly Document Assignments Maintain records of who has what access and why Review assignments during security audits Update documentation when roles change Regular Review Periodically audit role assignments Remove access for departed team members Verify service principal access is still needed Troubleshooting User Cannot Access Expected Resources Verify role assignment exists Check the scope includes the resource Ensure the user/group is correct User may need to sign out and back in Role Assignment Not Working Verify principal ID is correct Check for typos in scope Ensure Azure AD sync is current Review for conflicting assignments Cannot Create Role Assignment Verify you have User Access Administrator role Check you have permission on the target scope Ensure the principal exists in Azure AD Related Topics Permissions and Roles Reference Configuration (Portal Access) Agent Access Tokens"
  },
  "docs/management-portal/index.html": {
    "href": "docs/management-portal/index.html",
    "title": "Management Portal | FoundationaLLM",
    "summary": "Management Portal The Management Portal is the administrative interface for configuring and managing your FoundationaLLM deployment. Overview The Management Portal provides IT administrators, developers, and power users with a comprehensive set of tools to: Create and manage AI agents - Configure agent behavior, workflows, and capabilities Connect data sources - Integrate with Azure Data Lake, SharePoint, and other repositories Build data pipelines - Process and index data for agent retrieval Configure AI models - Manage model deployments and API endpoints Control access - Implement role-based access control (RBAC) Customize branding - Personalize the portal appearance Target Audience The Management Portal is designed for: Role Primary Tasks IT Administrators Instance configuration, security settings, deployment management Developers Agent creation, workflow configuration, API integration Power Users Agent management, prompt engineering, data source configuration Getting Started Quick Start Tour of the Management Portal - Learn the interface Creating Your First Agent - Build your first AI agent Prerequisites Microsoft Entra ID account with appropriate permissions Access to a deployed FoundationaLLM instance Reader or higher role assignment for Management Portal access Key Features Agent Management Create and configure AI agents with: Multiple workflow types (OpenAI Assistants, LangGraph, Custom) Tool integration (Code Interpreter, DALL-E, Knowledge Search) Customizable system prompts Gatekeeper content safety Learn more: Create New Agent All Agents Prompts Data Management Connect and process organizational data: Configure data source connections Build data processing pipelines Monitor pipeline execution Learn more: Data Sources Creating Data Pipelines Knowledge Sources Security and Access Control Implement comprehensive security: Role-based access control (RBAC) Agent-level permissions Agent access tokens for API integration Learn more: Instance Access Control Permissions and Roles Reference Platform Configuration Customize your deployment: Branding and visual identity Portal access configuration Deployment monitoring Learn more: Branding Configuration Configuration Deployment Information Portal Sections Section Description Agents Create, view, and manage AI agents and prompts Data Configure data sources, pipelines, and knowledge sources Models and Endpoints Manage AI model deployments and API endpoints Security Configure role assignments and access control FLLM Platform Branding, configuration, and deployment information Reference Documentation Agents & Workflows Data Pipelines Resource Management Configuration Reference Related Topics Chat User Portal Documentation - End-user chat interface APIs & SDKs - Programmatic access Platform Operations - Deployment and operations"
  },
  "docs/management-portal/quick-start/creating-first-agent.html": {
    "href": "docs/management-portal/quick-start/creating-first-agent.html",
    "title": "Creating Your First Agent | FoundationaLLM",
    "summary": "Creating Your First Agent A step-by-step guide to creating your first AI agent using the Management Portal. Prerequisites Before creating an agent, ensure: You have access to the Management Portal with appropriate permissions At least one AI model is configured (under Models and Endpoints > AI Models) (Optional) Data sources are configured if your agent needs to access organizational data Overview of the Agent Creation Process Creating an agent involves configuring several sections: General Information - Name, description, and welcome message Agent Configuration - Conversation history, gatekeeper, and display settings Workflow - The AI workflow type and model settings Tools - Additional capabilities like code interpreter or image generation Security - Access tokens for API access (optional) Step-by-Step Guide Step 1: Navigate to Create New Agent In the Management Portal sidebar, click Create New Agent under the Agents section The agent creation form appears with all configuration sections Step 2: Enter General Information Field Description Requirements Agent Name Unique identifier for the agent Letters, numbers, dashes, and underscores only. No spaces or special characters. Agent Display Name User-friendly name shown in the portal Any text. This is what users will see. Description Purpose of the agent Optional but recommended for discoverability Welcome Message Initial message shown to users Supports rich text formatting The agent name field validates in real-time: ✔️ Green checkmark indicates the name is available ❌ Red X indicates the name is invalid or already taken Step 3: Configure Agent Behavior Conversation History Setting Default Description Enabled No When enabled, the agent remembers previous messages in the conversation Max Messages 5 Number of previous messages to include in context Click the step item to expand and edit these settings. Toggle to Yes to enable conversation history, then set the maximum messages as needed. Gatekeeper The gatekeeper provides content safety and data protection: Setting Options Description Use system default Yes/No Use instance-level gatekeeper settings Content Safety Azure Content Safety, Lakera Guard, Enkrypt Guardrails AI safety platform for content moderation Data Protection Microsoft Presidio PII detection and redaction If you disable \"Use system default,\" you can select specific content safety and data protection options. User Prompt Rewrite Setting Description Enabled When enabled, rewrites user prompts before processing Rewrite Model AI model to use for rewriting Rewrite Prompt Prompt template for rewriting Rewrite Window Size Number of messages to consider (default: 3) Semantic Cache Setting Description Enabled When enabled, caches semantically similar responses Model Embedding model for similarity comparison Embedding Dimensions Vector dimensions (default: 2048) Minimum Similarity Threshold Similarity score required for cache hit (default: 0.97) Cost Center and Expiration Field Description Cost Center Assign to a department for cost tracking (optional) Expiration Date Date when the agent becomes inactive (optional) Step 4: Configure User Portal Experience These settings control what features are available to users in the Chat User Portal: Setting Default Description Show Message Tokens Yes Display token consumption for each message Allow Rating Yes Let users rate agent responses (thumbs up/down) Show View Prompt Yes Allow users to view the full completion prompt Allow File Upload No Enable file attachments in conversations Step 5: Select and Configure Workflow Select a Workflow Type from the dropdown: Workflow Type Description Best For OpenAIAssistants Azure OpenAI Assistants API with Code Interpreter, File Search, and Function Calling Complex tasks requiring code execution or file analysis LangGraphReactAgent LangGraph-based agent with dynamic tool selection Flexible agents that choose tools based on context ExternalAgentWorkflow Custom Python workflows registered with the platform Advanced custom logic Click Configure Workflow to expand the workflow settings Configure workflow details: Field Description Workflow Name Identifier for this workflow configuration Workflow Package Name Python package name (for custom workflows) Workflow Class Name Python class name (for custom workflows) Workflow Host Orchestration framework (typically LangChain) Workflow Main Model Primary AI model for the agent Workflow Main Model Parameters Model settings (temperature, max_tokens, etc.) Main Workflow Prompt System prompt defining the agent's persona and behavior Example system prompt: You are a helpful assistant named [Agent Name] that helps users with [specific purpose]. Provide concise, accurate answers. If you don't know something, say so clearly. Step 6: Add Tools (Optional) Tools extend the agent's capabilities. Click Add New Tool to configure: Tool Property Description Name Tool identifier (must be unique) Package Name Python package containing the tool Description What the tool does (helps the AI decide when to use it) Tool Resources Additional resources the tool needs (models, data sources) Common Tools: DALLEImageGeneration - Generate images using DALL-E Code Interpreter - Execute Python code for analysis Knowledge Search - Search configured knowledge sources Note: For DALL-E image generation, the tool name must be exactly DALLEImageGeneration and requires an AI model with the main_model object role in Tool Resources. Step 7: Create the Agent Review all settings Click Create Agent at the bottom of the page Wait for the creation process to complete Upon success, you'll see a confirmation message Testing Your Agent After creation: Open the Chat User Portal Select your new agent from the agent dropdown Send a test message to verify it responds correctly Test any configured tools (file upload, code execution, etc.) Next Steps Detailed Agent Creation Guide - In-depth configuration options Managing Prompts - Create reusable prompt templates Agent Access Tokens - Configure API access Agents & Workflows Reference - Technical details Troubleshooting Agent Name Validation Fails Ensure no spaces or special characters Check if the name is already in use Use only letters, numbers, dashes (-), and underscores (_) Workflow Model Not Available Verify AI models are configured under Models and Endpoints > AI Models Check that your account has access to the required models Agent Not Appearing in Chat Portal Verify the agent was created successfully Check user permissions for the agent Ensure the agent hasn't expired (if expiration was set)"
  },
  "docs/management-portal/quick-start/portal-tour.html": {
    "href": "docs/management-portal/quick-start/portal-tour.html",
    "title": "Tour of the Management Portal | FoundationaLLM",
    "summary": "Tour of the Management Portal A guided tour of the FoundationaLLM Management Portal interface and its key features. Overview The Management Portal is the administrative interface for configuring and managing your FoundationaLLM deployment. It provides IT administrators, developers, and power users with tools to create agents, configure data pipelines, manage security, and customize the platform. Accessing the Management Portal Navigate to your Management Portal URL (typically https://<your-instance>-portal.azurewebsites.net) Sign in with your Microsoft Entra ID credentials Upon successful authentication, you'll see the main dashboard Main Navigation The sidebar on the left side of the screen provides access to all portal sections. The sidebar can be collapsed by clicking the arrow button in the header. Agents Section Menu Item Description Create New Agent Step-by-step wizard for creating and configuring new AI agents All Agents View and manage all agents in the instance My Agents View agents you've created or own Prompts Manage prompt templates used by agents Data Section Menu Item Description Data Sources Configure connections to data repositories (Azure Data Lake, SharePoint, etc.) Data Pipelines Create and manage data processing pipelines Data Pipeline Runs Monitor pipeline execution and view run history Knowledge Sources Configure knowledge bases for agent retrieval Models and Endpoints Section Menu Item Description AI Models Configure AI model deployments (Azure OpenAI, etc.) API Endpoints Manage API endpoint configurations Security Section Menu Item Description Instance Access Control Manage role assignments and access permissions FLLM Platform Section Menu Item Description Branding Customize the portal appearance (colors, logos, text) Configuration Configure portal access and instance-level settings Deployment Information View instance ID, API status, and deployment details Common UI Elements Page Headers Each page includes: Page title: Identifies the current section Subheader: Brief description of the page's purpose Action buttons: Context-specific actions (Create, Refresh, etc.) Data Tables Resource lists use consistent table layouts: Sortable columns: Click column headers to sort Search: Filter results by name or other fields Pagination: Navigate through large datasets (10, 25, 50, or 100 rows per page) Edit/Delete actions: Row-level actions for managing resources Forms and Wizards Configuration pages use: Step items: Expandable cards that show current settings and allow editing Toggle buttons: Yes/No switches for enabling features Dropdowns: Selection lists for choosing options Input validation: Real-time feedback on required fields and format requirements Access Control Resources with access control show: Access Control button: Opens role assignment dialog Scope selection: Choose the level at which permissions apply Principal selection: Add users, groups, or service principals User Account The bottom of the sidebar displays: User avatar: Your profile image Username: Your display name (hover for full email) Sign Out button: Log out of the portal Keyboard Navigation The portal supports keyboard navigation: Tab / Shift+Tab: Navigate between elements Enter / Space: Activate buttons and links Escape: Close dialogs and cancel operations Getting Started Workflow For first-time users, the typical workflow is: Configure AI Models: Set up your AI model deployments under Models and Endpoints Create Data Sources: Connect to your data repositories under Data Create Prompts: Define prompt templates under Agents > Prompts Create an Agent: Use the Create New Agent wizard Configure Access: Set up role assignments under Security Test: Access the Chat User Portal to test your agent Related Topics Creating Your First Agent Instance Access Control Branding Configuration"
  },
  "docs/management-portal/reference/branding/index.html": {
    "href": "docs/management-portal/reference/branding/index.html",
    "title": "Branding Reference | FoundationaLLM",
    "summary": "Branding Reference Reference documentation for customizing the visual appearance of your FoundationaLLM deployment. Overview FoundationaLLM provides comprehensive branding customization for both the Management Portal and Chat User Portal. You can customize logos, colors, text, and messages to match your organization's visual identity. Configuration Methods Method Description Best For Management Portal Interactive UI-based configuration Real-time changes, visual preview App Configuration Azure App Configuration values Infrastructure-as-code, deployment automation REST API Programmatic updates Custom tooling, automation Branding Elements Logo Configuration Setting Key Default Logo URL FoundationaLLM:Branding:LogoUrl foundationallm-logo-white.svg Logo Text FoundationaLLM:Branding:LogoText FoundationaLLM FavIcon URL FoundationaLLM:Branding:FavIconUrl favicon.ico Agent Icon URL FoundationaLLM:Branding:AgentIconUrl (empty) Text Configuration Setting Key Default Company Name FoundationaLLM:Branding:CompanyName FoundationaLLM Page Title FoundationaLLM:Branding:PageTitle FoundationaLLM User Portal Footer Text FoundationaLLM:Branding:FooterText FoundationaLLM © All rights reserved. No Agents Message FoundationaLLM:Branding:NoAgentsMessage System default message Default Welcome Message FoundationaLLM:Branding:DefaultAgentWelcomeMessage Start the conversation using the text box below. Color Configuration Setting Key Default Primary Color FoundationaLLM:Branding:PrimaryColor #131833 Primary Text Color FoundationaLLM:Branding:PrimaryTextColor #fff Secondary Color FoundationaLLM:Branding:SecondaryColor #334581 Secondary Text Color FoundationaLLM:Branding:SecondaryTextColor #fff Accent Color FoundationaLLM:Branding:AccentColor #fff Accent Text Color FoundationaLLM:Branding:AccentTextColor #131833 Background Color FoundationaLLM:Branding:BackgroundColor #fff Primary Button Background FoundationaLLM:Branding:PrimaryButtonBackgroundColor #5472d4 Primary Button Text FoundationaLLM:Branding:PrimaryButtonTextColor #fff Secondary Button Background FoundationaLLM:Branding:SecondaryButtonBackgroundColor #70829a Secondary Button Text FoundationaLLM:Branding:SecondaryButtonTextColor #fff Mode Configuration Setting Key Default Kiosk Mode FoundationaLLM:Branding:KioskMode false Color Format Support Colors can be specified in: Hex: #RRGGBB or #RGB (e.g., #131833, #fff) RGB: rgb(R, G, B) (e.g., rgb(19, 24, 51)) Accessibility Guidelines WCAG Contrast Requirements Standard Minimum Ratio Recommendation AA 4.5:1 Required for text AAA 7:1 Recommended for enhanced accessibility Testing Contrast The Management Portal branding page includes contrast testing: Enable \"Show contrast information\" Review the ratio between background and text colors Check AA and AAA compliance indicators Rich Text Fields The following fields support HTML formatting: Footer Text No Agents Message Default Agent Welcome Message Supported HTML elements: <strong>, <b> - Bold text <em>, <i> - Italic text <a href=\"\"> - Links <ul>, <ol>, <li> - Lists <p> - Paragraphs Related Topics Branding Configuration (How-To) Using Management Portal for Branding Using App Configuration for Branding Using REST API for Branding"
  },
  "docs/management-portal/reference/branding/using-app-configuration.html": {
    "href": "docs/management-portal/reference/branding/using-app-configuration.html",
    "title": "Using App Configuration for Branding | FoundationaLLM",
    "summary": "Using App Configuration for Branding Configure branding settings through Azure App Configuration for infrastructure-as-code deployments. Overview Azure App Configuration provides a centralized store for branding settings, enabling: Infrastructure-as-code management Deployment automation Environment-specific configurations Version control of settings Configuration Keys All branding keys follow the pattern: FoundationaLLM:Branding:{SettingName} Text Settings Key Type Description FoundationaLLM:Branding:CompanyName String Organization name FoundationaLLM:Branding:PageTitle String Browser tab title FoundationaLLM:Branding:LogoText String Text if logo unavailable FoundationaLLM:Branding:LogoUrl String Logo image path FoundationaLLM:Branding:FavIconUrl String Favicon path FoundationaLLM:Branding:AgentIconUrl String Default agent icon path FoundationaLLM:Branding:FooterText String Footer HTML FoundationaLLM:Branding:NoAgentsMessage String No agents message HTML FoundationaLLM:Branding:DefaultAgentWelcomeMessage String Default welcome HTML Color Settings Key Type Format FoundationaLLM:Branding:PrimaryColor String Hex or RGB FoundationaLLM:Branding:PrimaryTextColor String Hex or RGB FoundationaLLM:Branding:SecondaryColor String Hex or RGB FoundationaLLM:Branding:SecondaryTextColor String Hex or RGB FoundationaLLM:Branding:AccentColor String Hex or RGB FoundationaLLM:Branding:AccentTextColor String Hex or RGB FoundationaLLM:Branding:BackgroundColor String Hex or RGB FoundationaLLM:Branding:PrimaryButtonBackgroundColor String Hex or RGB FoundationaLLM:Branding:PrimaryButtonTextColor String Hex or RGB FoundationaLLM:Branding:SecondaryButtonBackgroundColor String Hex or RGB FoundationaLLM:Branding:SecondaryButtonTextColor String Hex or RGB Mode Settings Key Type Values FoundationaLLM:Branding:KioskMode String true or false Setting Values via Azure Portal Navigate to your Azure App Configuration resource Click Configuration explorer Click + Create > Key-value Enter the key (e.g., FoundationaLLM:Branding:CompanyName) Enter the value Click Apply Setting Values via Azure CLI # Set a text value az appconfig kv set \\ --name <app-config-name> \\ --key \"FoundationaLLM:Branding:CompanyName\" \\ --value \"Contoso\" # Set a color value az appconfig kv set \\ --name <app-config-name> \\ --key \"FoundationaLLM:Branding:PrimaryColor\" \\ --value \"#1a2b3c\" Setting Values via Bicep/ARM resource appConfig 'Microsoft.AppConfiguration/configurationStores@2023-03-01' existing = { name: appConfigName } resource brandingCompanyName 'Microsoft.AppConfiguration/configurationStores/keyValues@2023-03-01' = { parent: appConfig name: 'FoundationaLLM:Branding:CompanyName' properties: { value: 'Contoso' } } resource brandingPrimaryColor 'Microsoft.AppConfiguration/configurationStores/keyValues@2023-03-01' = { parent: appConfig name: 'FoundationaLLM:Branding:PrimaryColor' properties: { value: '#1a2b3c' } } Environment-Specific Configuration Use labels for environment-specific settings: # Development az appconfig kv set \\ --name <app-config-name> \\ --key \"FoundationaLLM:Branding:PageTitle\" \\ --value \"FoundationaLLM (Dev)\" \\ --label \"dev\" # Production az appconfig kv set \\ --name <app-config-name> \\ --key \"FoundationaLLM:Branding:PageTitle\" \\ --value \"FoundationaLLM\" \\ --label \"prod\" Value Format Notes Color Values Supported formats: #RRGGBB (6-digit hex) #RGB (3-digit hex, expanded to 6-digit) rgb(R, G, B) (RGB function) HTML Values For rich text fields (Footer, Messages), use properly escaped HTML: Escape special characters if needed Ensure HTML is valid Cache Considerations App Configuration values are cached: Portal applications refresh periodically Users may need to refresh browser to see changes Consider cache timing when deploying updates Related Topics Branding Reference Using Management Portal for Branding Using REST API for Branding"
  },
  "docs/management-portal/reference/branding/using-management-portal.html": {
    "href": "docs/management-portal/reference/branding/using-management-portal.html",
    "title": "Using Management Portal for Branding | FoundationaLLM",
    "summary": "Using Management Portal for Branding Configure branding settings interactively through the Management Portal UI. Overview The Management Portal provides an interactive interface for branding customization with: Real-time preview of color changes WCAG accessibility compliance indicators Rich text editing for text fields Visual logo preview Accessing Branding Settings Navigate to the Management Portal In the sidebar, click Branding under FLLM Platform Interface Overview The branding page is organized into sections: General Settings Section Configure non-color branding elements: Company Name FavIcon URL Footer Text (rich text editor) Kiosk Mode toggle Logo Text Logo URL (with preview) Page Title Agent Icon URL No Agents Message (rich text editor) Default Agent Welcome Message (rich text editor) Color Settings Section Color configuration organized by functional area: Accent colors (background + text) Background color Primary colors (background + text) Secondary colors (background + text) Primary button colors Secondary button colors Making Changes Text Fields Locate the setting to change Enter the new value in the text input Changes are staged (not saved until you click Save) Rich Text Fields For Footer Text, No Agents Message, and Default Welcome Message: Click in the editor field Use the toolbar for formatting (bold, italic, links, etc.) Enter your content Changes are staged until saved Colors Locate the color setting to change Either: Click the color swatch to open the color picker Enter a hex or RGB value directly Review the preview square Check contrast compliance if enabled Logo Enter the logo URL (relative to public directory) View the preview against the Primary Color background Ensure the logo is visible and appropriate size Accessibility Testing Enable accessibility testing: Toggle Show contrast information at the top of the page For each color pair, you'll see: Contrast ratio (e.g., \"4.5:1\") AA compliance (Pass/Fail) AAA compliance (Pass/Fail) Saving Changes Save Button Click Save to apply all changes: Changes are persisted to Azure App Configuration Users see updates after browser refresh Reset Button Click Reset to discard all unsaved changes: Reverts to last saved values Confirmation dialog appears Set Default Button Click Set Default to restore factory defaults: Reverts all values to FoundationaLLM defaults Still requires Save to apply Confirmation dialog appears Validation The interface validates: Color format (hex/RGB) URL formats (for logo/favicon) Required fields Best Practices Test on both portals - Changes affect both Management and User portals Check accessibility - Enable contrast information to verify WCAG compliance Preview logos - Use the preview to verify logo appearance Save incrementally - Save after major changes to avoid losing work Related Topics Branding Reference Using App Configuration for Branding Branding Configuration (How-To)"
  },
  "docs/management-portal/reference/branding/using-rest-api.html": {
    "href": "docs/management-portal/reference/branding/using-rest-api.html",
    "title": "Using REST API for Branding | FoundationaLLM",
    "summary": "Using REST API for Branding Configure branding settings programmatically through the Management API. Overview The Management API provides endpoints for branding configuration, enabling: Custom tooling integration Automation scripts CI/CD pipelines External system integration API Endpoints Base URL {Management API URL}/instances/{instanceId}/providers/FoundationaLLM.Configuration Authentication All requests require authentication: Bearer token (Azure AD) Appropriate permissions for configuration management Retrieve All Branding Settings GET /instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations Response [ { \"resource\": { \"type\": \"app-configuration\", \"name\": \"Branding-PrimaryColor\", \"key\": \"FoundationaLLM:Branding:PrimaryColor\", \"value\": \"#131833\", \"description\": \"The primary color for the portal UI.\" }, \"actions\": [\"read\", \"write\"] }, ... ] Update a Branding Setting PUT /instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations/{settingName} Content-Type: application/json Request Body { \"type\": \"app-configuration\", \"name\": \"Branding-PrimaryColor\", \"display_name\": \"Primary Color\", \"description\": \"The primary color for the portal UI.\", \"key\": \"FoundationaLLM:Branding:PrimaryColor\", \"value\": \"#1a2b3c\", \"content_type\": \"text/plain\" } Response { \"success\": true, \"resource\": { \"type\": \"app-configuration\", \"name\": \"Branding-PrimaryColor\", \"key\": \"FoundationaLLM:Branding:PrimaryColor\", \"value\": \"#1a2b3c\" } } Code Examples Python import requests base_url = \"https://your-management-api.azurewebsites.net\" instance_id = \"your-instance-id\" access_token = \"your-bearer-token\" headers = { \"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\" } # Get branding settings response = requests.get( f\"{base_url}/instances/{instance_id}/providers/FoundationaLLM.Configuration/appConfigurations\", headers=headers ) branding = response.json() # Update a setting update_data = { \"type\": \"app-configuration\", \"name\": \"Branding-CompanyName\", \"key\": \"FoundationaLLM:Branding:CompanyName\", \"value\": \"Contoso\", \"content_type\": \"text/plain\" } response = requests.put( f\"{base_url}/instances/{instance_id}/providers/FoundationaLLM.Configuration/appConfigurations/Branding-CompanyName\", headers=headers, json=update_data ) PowerShell $baseUrl = \"https://your-management-api.azurewebsites.net\" $instanceId = \"your-instance-id\" $accessToken = \"your-bearer-token\" $headers = @{ \"Authorization\" = \"Bearer $accessToken\" \"Content-Type\" = \"application/json\" } # Get branding settings $response = Invoke-RestMethod -Uri \"$baseUrl/instances/$instanceId/providers/FoundationaLLM.Configuration/appConfigurations\" ` -Headers $headers -Method Get # Update a setting $body = @{ type = \"app-configuration\" name = \"Branding-CompanyName\" key = \"FoundationaLLM:Branding:CompanyName\" value = \"Contoso\" content_type = \"text/plain\" } | ConvertTo-Json Invoke-RestMethod -Uri \"$baseUrl/instances/$instanceId/providers/FoundationaLLM.Configuration/appConfigurations/Branding-CompanyName\" ` -Headers $headers -Method Put -Body $body JavaScript/Node.js const baseUrl = \"https://your-management-api.azurewebsites.net\"; const instanceId = \"your-instance-id\"; const accessToken = \"your-bearer-token\"; // Get branding settings const response = await fetch( `${baseUrl}/instances/${instanceId}/providers/FoundationaLLM.Configuration/appConfigurations`, { headers: { \"Authorization\": `Bearer ${accessToken}`, \"Content-Type\": \"application/json\" } } ); const branding = await response.json(); // Update a setting const updateResponse = await fetch( `${baseUrl}/instances/${instanceId}/providers/FoundationaLLM.Configuration/appConfigurations/Branding-CompanyName`, { method: \"PUT\", headers: { \"Authorization\": `Bearer ${accessToken}`, \"Content-Type\": \"application/json\" }, body: JSON.stringify({ type: \"app-configuration\", name: \"Branding-CompanyName\", key: \"FoundationaLLM:Branding:CompanyName\", value: \"Contoso\", content_type: \"text/plain\" }) } ); Setting Name Mapping When using the API, setting names follow this pattern: Key Setting Name FoundationaLLM:Branding:CompanyName Branding-CompanyName FoundationaLLM:Branding:PrimaryColor Branding-PrimaryColor FoundationaLLM:Branding:LogoUrl Branding-LogoUrl General pattern: Branding-{SettingName} (colons replaced with hyphens, only the last segment) Error Handling Status Code Description 200 Success 400 Invalid request body 401 Unauthorized 403 Forbidden (insufficient permissions) 404 Setting not found 500 Internal server error Related Topics Branding Reference Using Management Portal for Branding Using App Configuration for Branding"
  },
  "docs/management-portal/reference/concepts/agent-access-tokens.html": {
    "href": "docs/management-portal/reference/concepts/agent-access-tokens.html",
    "title": "Agent Access Token | FoundationaLLM",
    "summary": "Agent Access Token To allow the flexibility of using an agent without requiring the user to be authenticated using Entra ID credentials, you can create an Agent Access Token. This is particularly useful for public applications that want to provide access to the agent without requiring users to log in with their Entra ID credentials. How to create an Agent Access Token In the Security section of the agent configuration, click on the Create Access Token button to create a new access token. In the Create Access Token dialog, enter a description for the access token and an expiration date then click on the Create Access Token button. The access token will be created and displayed in a dialog, make sure to save it or copy it for future use. Assigning permission to the new Virtual Security Group ID of the agent Copy the GUID of the new Virtual Security Group ID of the agent. At the top of the page while editing the agent, click on the Access Control. In the Access Control page, click on the Add Role Assignment for this resource button. Verify that the Scope is set to providers/FoundationaLLM.Agent/agents/{agent_name}. Choose Group as the Prinicipal Type. Paste the GUID of the new Virtual Security Group ID of the agent in the Principal ID field. Choose Reader as the Role to assign. Accessing the agent using the Agent Access Token To share an example of how to use the Agent Access Token, we will use the API collection from POSTMAN that is publicly available and part of the FoundationaLLM documentation. There are two steps required to use the agent using the Agent Access Token in POSTMAN by accessing the FoundationaLLM.Core.API collection: Under sessions, pick the POST Creates a new Chat session with Agent Access Token Under completions, expand completion and pick POST Requests a completion with Agent Access Token You have to start with the Creates a new Chat session with Agent Access Token request to create a new chat session as the request for completion requires a sessionId. Start by setting the Authorization type to No Auth On the Headers tab, set a new key X-AGENT-ACCESS-TOKEN with the value of the Access Token that you were asked to save or copy previously On the Body tab, enter a name for your chat conversation session in the name key in raw JSON format Click on the Send button to send the request and save the sessionId from the response as it will be needed in the next step. The next step is to POST to the completion endpoint to get a response from the agent utilizing the sessionId received in the previous step. 1- Set the Authorization type to No Auth 2- On the Headers tab, set a new key X-AGENT-ACCESS-TOKEN with the value of the Access Token that you were asked to save or copy previously 3- On the Body tab, enter the sessionId received in the previous POST in the sessionId key in raw JSON format 4- Click on the Send button to send the request and receive a response from the agent explaining why the sky is blue."
  },
  "docs/management-portal/reference/concepts/agents-workflows.html": {
    "href": "docs/management-portal/reference/concepts/agents-workflows.html",
    "title": "Agents and Workflows | FoundationaLLM",
    "summary": "Agents and Workflows Agents are the core of FoundationaLLM, providing users with customized AI-powered conversational experiences based on their configuration. This reference documents the agent resource structure and workflow configuration options. Agent Resource Structure An agent consists of several configuration sections: Section Purpose General Basic information (name, description, welcome message) Agent Configuration Behavior settings (history, gatekeeper, cost center, expiration, portal displays) User Portal Experience Portal feature visibility settings Workflow Orchestration and model configuration Tools Tool capabilities (Code Interpreter, Knowledge Search, DALL-E) Security Access tokens and virtual security groups Agent JSON Structure { \"type\": \"agent\", \"name\": \"my-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\", \"display_name\": \"My Agent\", \"description\": \"Agent description\", \"inline_context\": false, \"conversation_history_settings\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper_settings\": { \"use_system_setting\": false, \"options\": [\"ContentSafety\", \"Presidio\"] }, \"orchestration_settings\": { \"agent_parameters\": {} }, \"workflow_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/my-workflow\", \"tool_object_ids\": [], \"cost_center\": \"\", \"expiration_date\": null } Workflow Types Workflows define how agents process requests and generate responses. OpenAIAssistants Uses Azure OpenAI Assistants API for orchestration. Feature Description Code Interpreter Execute Python code for data analysis File Search Search through uploaded documents Function Calling Call external functions/tools Best For: Complex multi-step tasks, code execution, file analysis LangGraphReactAgent Uses LangGraph for dynamic tool selection with ReAct pattern. Feature Description Dynamic Tool Selection Agent chooses tools based on context ReAct Pattern Reasoning + Acting loop Tool Orchestration Multiple tools in sequence Best For: Dynamic multi-tool scenarios, reasoning chains ExternalAgentWorkflow Enables custom Python-based workflow implementations. Feature Description Custom Logic Implement any workflow pattern Plugin-Based Registered as workflow plugins Full Control Complete control over orchestration Best For: Custom business logic, specialized workflows Workflow Configuration Workflow Resource Structure { \"type\": \"workflow\", \"name\": \"my-workflow\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/my-workflow\", \"workflow_type\": \"OpenAIAssistants\", \"workflow_host\": \"LangChain\", \"package_name\": \"FoundationaLLM.Workflow\", \"main_ai_model_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.AIModel/aiModels/gpt-4o\", \"main_prompt_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Prompt/prompts/system-prompt\", \"ai_model_object_ids\": {}, \"prompt_object_ids\": {}, \"resource_object_ids\": {} } Key Workflow Settings Setting Description workflow_type One of: OpenAIAssistants, LangGraphReactAgent, ExternalAgentWorkflow workflow_host Currently LangChain for all workflows package_name Plugin package name for external workflows main_ai_model_object_id Primary model for generation main_prompt_object_id System prompt defining agent persona ai_model_object_ids Additional models by role prompt_object_ids Additional prompts by role resource_object_ids External resources (e.g., Azure AI Project) Agent Configuration Options Conversation History Controls context retention across conversation turns. Setting Type Description enabled boolean Enable conversation history max_history integer Number of message pairs to retain (default: 5) Gatekeeper Controls content moderation and safety features. Option Description ContentSafety Azure Content Safety integration ContentSafetyPromptShield Azure Content Safety with prompt injection detection LakeraGuard Lakera Guard integration EnkryptGuardrails Enkrypt Guardrails integration Presidio Microsoft Presidio for PII detection User Prompt Rewrite Enables query transformation for improved retrieval. Setting Type Description enabled boolean Enable prompt rewriting ai_model_object_id string Model for rewriting prompt_object_id string Prompt for rewriting instructions Semantic Cache Caches responses for similar queries. Setting Type Description enabled boolean Enable semantic caching score_threshold float Similarity threshold (0-1) Cost Center & Expiration Setting Type Description cost_center string Cost allocation identifier expiration_date datetime Agent expiration date (null = no expiration) User Portal Experience Settings Control what features are visible in the Chat User Portal. Setting Description Show Token Usage Display token counts for requests/responses Show Prompt Allow users to view the compiled prompt Enable Rating Allow users to rate responses Enable File Upload Allow users to upload files Tool Configuration Tool Resource Structure { \"type\": \"tool\", \"name\": \"code-interpreter\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/tools/code-interpreter\", \"tool_type\": \"code-interpreter\", \"description\": \"Execute Python code\", \"ai_model_object_ids\": {}, \"prompt_object_ids\": {}, \"resource_object_ids\": {}, \"properties\": {} } Built-in Tool Types Tool Type Description Configuration code-interpreter Python code execution Conversation file scope knowledge-search Vector search through knowledge sources Indexing profile, data sources dalle-image-generation Generate images with DALL-E AI model with main_model role Tool Resource Mapping Tools can reference models, prompts, and resources by role: { \"ai_model_object_ids\": { \"main_model\": \"/instances/.../aiModels/dalle-3\" }, \"prompt_object_ids\": { \"tool_prompt\": \"/instances/.../prompts/tool-instructions\" }, \"resource_object_ids\": { \"indexing_profile\": \"/instances/.../indexingProfiles/default\" } } Security Configuration Virtual Security Group ID Identifies the agent for external access scenarios. { \"virtual_security_group_id\": \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\" } Agent Access Tokens Enable unauthenticated access to specific agents. See Agent Access Tokens for details. Related Topics Create New Agent Agent Access Tokens Prompts Resources Resource Management"
  },
  "docs/management-portal/reference/concepts/api-limits.html": {
    "href": "docs/management-portal/reference/concepts/api-limits.html",
    "title": "API and Token Limits | FoundationaLLM",
    "summary": "API and Token Limits Reference documentation for API rate limits, token usage limits, and quota enforcement in FoundationaLLM. Overview FoundationaLLM enforces various limits to ensure fair usage, protect system stability, and manage costs. Understanding these limits helps you build applications that work reliably within the platform's constraints. Types of Limits Limit Type Purpose Scope API Rate Limits Prevent excessive API calls Per user or global Token Usage Limits Control model consumption Per request/agent/user Quota Limits Manage resource allocation Per agent or platform API Rate Limits Overview API rate limits restrict how many requests can be made within a time window. When limits are exceeded, the API returns a 429 (Too Many Requests) response. Rate Limit Structure Rate limits are defined by: Metric Limit: Maximum number of requests Time Window: Period over which requests are counted (typically 60 seconds) Lockout Duration: How long you're blocked after exceeding limits Partition: Whether limits apply globally, per user, or per agent Common Rate Limits Controller Typical Limit Description Completions 100/minute/user Chat completion requests CompletionsStatus 500/minute/user Polling for async completion status Sessions 60/minute/user Session management operations Files 30/minute/user File upload operations Rate Limit Response When you exceed a rate limit: { \"quota_exceeded\": true, \"quota_name\": \"CoreAPICompletionsRateLimit\", \"retry_after_seconds\": 60, \"message\": \"Rate limit exceeded. Try again later.\" } Best Practices Implement retry logic with exponential backoff Monitor 429 responses in your application Use async endpoints for long operations to reduce polling Cache responses where appropriate Token Usage Limits What Are Tokens? Tokens are the units AI models use to process text: ~1 token ≈ 4 characters in English ~1 token ≈ ¾ of a word 100 tokens ≈ 75 words Token Counting Each completion request consumes tokens in three categories: Category Description Prompt Tokens Tokens in your input (system prompt + user message + context) Completion Tokens Tokens in the model's response Total Tokens Sum of prompt and completion tokens Per-Request Token Limits The max_new_tokens parameter limits response length: { \"user_prompt\": \"Explain quantum computing\", \"settings\": { \"model_parameters\": { \"max_new_tokens\": 2000 } } } Parameter Typical Range Default max_new_tokens 1-4000+ Model-dependent Model Context Limits Different models have different context window sizes: Model Context Window Notes GPT-4 8K-128K tokens Varies by deployment GPT-4o 128K tokens Large context support GPT-3.5-turbo 4K-16K tokens Varies by version Claude 3 200K tokens Extended context If your request exceeds the context window, the API returns an error. Token Limit Errors When token limits are exceeded: { \"error\": { \"code\": \"TokenLimitExceeded\", \"message\": \"Request exceeds maximum token limit for this model.\" } } API Key Limits Agent Access Token Limits Agent Access Tokens have configurable constraints: Limit Description Configuration Expiration Token validity period Set at creation time Agent Scope Which agent(s) can be accessed Bound to single agent Permission Scope What actions are allowed Defined by role assignments Creating Tokens with Appropriate Limits When creating Agent Access Tokens: Set appropriate expiration dates (shorter = more secure) Limit scope to necessary agents only Review and rotate tokens periodically Quota Configuration Quota Definition Structure Quotas are defined with these properties: { \"name\": \"CoreAPICompletionsRateLimit\", \"description\": \"100 requests per minute per user\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 100, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } Quota Types Type Description RawRequestRateLimit Limits total API requests AgentRequestRateLimit Limits requests to specific agents Partition Strategies Partition Description Use Case None Global limit for all users System protection UserPrincipalName Per-user limits Standard user access UserIdentifier Per-identity limits Service accounts Agent-Specific Limits Per-Agent Rate Limits Specific agents can have individual rate limits: { \"name\": \"KnowledgeAgentRateLimit\", \"context\": \"CoreAPI:Completions:knowledge-agent\", \"type\": \"AgentRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 50, \"metric_window_seconds\": 60 } Agent Token Configuration Agents can be configured with: Maximum response tokens Temperature constraints Context window usage limits Monitoring and Managing Limits Viewing Current Usage Users can monitor token consumption in the User Portal: Enable Show Message Tokens in agent configuration Token counts appear on each message Review consumption patterns over time Administrator Monitoring Administrators can: Review quota definitions in storage Monitor API logs for 429 responses Adjust quotas based on usage patterns Adjusting Limits To modify quota limits: Access the quota configuration in the main storage account Edit the quota-store.json file in the quota container Update metric limits, windows, or partitions Changes take effect on next API service restart Limit Recommendations For Developers Scenario Recommendation Interactive chat Use default limits, implement retry Batch processing Implement rate limiting in your code High-volume API Request quota increase from admin Long operations Use async endpoints For Administrators Goal Configuration Prevent abuse Set per-user limits with lockouts Protect critical agents Add agent-specific rate limits Allow high-volume integrations Consider higher limits for service accounts Cost management Monitor and set token-based quotas Error Handling Examples Handling Rate Limits (Python) import time import requests def call_api_with_retry(url, headers, data, max_retries=3): for attempt in range(max_retries): response = requests.post(url, headers=headers, json=data) if response.status_code == 429: retry_after = response.json().get('retry_after_seconds', 60) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue return response raise Exception(\"Max retries exceeded\") Handling Token Limits def truncate_to_token_limit(text, max_tokens=4000): # Approximate: 4 chars per token max_chars = max_tokens * 4 if len(text) > max_chars: return text[:max_chars] + \"...\" return text Related Topics Quotas Reference — Detailed quota configuration Core API Reference — API endpoint details Monitoring Token Consumption — User token monitoring"
  },
  "docs/management-portal/reference/concepts/data-pipelines.html": {
    "href": "docs/management-portal/reference/concepts/data-pipelines.html",
    "title": "Data Pipelines | FoundationaLLM",
    "summary": "Data Pipelines Overview The following diagram illustrates the high level structure of a FoundationaLLM data pipeline: Data pipelines have three main components: Data Pipeline Data Source: The source of the data that will be processed by the pipeline. Data Pipeline Stages: The stages that the data will go through in the pipeline. The stages are structure as a forest, with one or more starting stages. Data Pipeline Trigger: The trigger that will start the pipeline execution. Trigger can be a schedule, an event (e.g., new content added to the data source), or a manual action. The data source and the stages define parameters that are required for the pipeline execution. The parameters metadata is provided by the plugin that implements that particular data source or stage. Parameters can have default values, which can be overridden when the pipeline is executed. Note Triggers of type Schedule or Event must provide a complete set of parameter values. This is because the trigger will use these values to start the pipeline execution and there is no user interaction to provide missing values. In the case of a manual trigger, the user will be prompted to provide the missing values. Here is an example of a simple data pipeline that reads data from an Azure Data Lake storage account, extracts text from files, partitions the text into chunks of a certain size, embeds the chunks and deposits them into an Azure AI Search index: { \"type\": \"data-pipeline\", \"name\": \"DataPipeline01\", \"display_name\": \"Data Pipeline 01\", \"description\": \"Data Pipeline demo.\", \"active\": false, \"data_source\": { \"name\": \"VGDataLake\", \"description\": \"Victorious Ground data lake storage.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/AzureDataLakeDataSource\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"Folders\", \"type\": \"array\", \"description\": \"A list of strings defining data lake folders.\" }, \"default_value\": null } ], \"data_source_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.DataSource/dataSources/alchemy\" }, \"starting_stages\": [ { \"name\": \"Extract\", \"description\": \"Extract text from binary content items.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TextExtractionDataPipelineStage\", \"plugin_parameters\": null, \"next_stages\": [ { \"name\": \"Partition\", \"description\": \"Partition text into chunks.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TextPartitioningDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"PartitioningStrategy\", \"type\": \"string\", \"description\": \"The partitioning strategy to be used (can be Token or Semantic).\" }, \"default_value\": \"Token\" } ], \"plugin_dependencies\": [ { \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/TokenContentTextPartitioning\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"PartitionSizeTokens\", \"type\": \"int\", \"description\": \"The size in tokens of the text partitions.\" }, \"default_value\": 400 }, { \"parameter_metadata\": { \"name\": \"PartitionOverlapTokens\", \"type\": \"int\", \"description\": \"The size in tokens of text partitions overlap.\" }, \"default_value\": 100 } ] } ], \"next_stages\": [ { \"name\": \"Embed\", \"description\": \"Embed chunks of text.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/GatewayTextEmbeddingDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"EmbeddingModel\", \"type\": \"string\", \"description\": \"The embedding model used for embedding.\" }, \"default_value\": \"text-embedding-3-large\" }, { \"parameter_metadata\": { \"name\": \"EmbeddingDimensions\", \"type\": \"int\", \"description\": \"The number of dimensions used for embedding.\" }, \"default_value\": 2048 } ], \"next_stages\": [ { \"name\": \"Index\", \"description\": \"Persist embeddings to a vector store.\", \"plugin_object_id\": \"instances/{{instanceId}}/providers/FoundationaLLM.Plugin/plugins/AzureAISearchIndexingDataPipelineStage\", \"plugin_parameters\": [ { \"parameter_metadata\": { \"name\": \"APIEndpointConfigurationObjectId\", \"type\": \"resource-object-id\", \"description\": \"The FoundationaLLM resource object identifier of the API Endpoint Configuration resource that represents the target Azure AI Search instance.\" }, \"default_value\": \"instances/{{instanceId}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\" }, { \"parameter_metadata\": { \"name\": \"IndexName\", \"type\": \"string\", \"description\": \"The name of the Azure AI Search index.\" }, \"default_value\": \"demo-index\" }, { \"parameter_metadata\": { \"name\": \"IndexPartitionName\", \"type\": \"string\", \"description\": \"The name of the index partition (to be added as a metadata entry and used for logical separation within a given physical index.\" }, \"default_value\": \"Dune\" } ] } ] } ] } ] } ], \"triggers\": [ { \"name\": \"Default pipeline schedule\", \"trigger_type\": \"Schedule\", \"trigger_cron_schedule\": \"0 6 * * *\", \"parameter_values\": { \"DataSource.VGDataLake.Folders\": [ \"vectorization-input/Dune\" ], \"Stage.Partition.PartitioningStrategy\": \"Token\", \"Stage.Partition.Dependency.TokenContentTextPartitioning.PartitionSizeTokens\": 400, \"Stage.Partition.Dependency.TokenContentTextPartitioning.PartitionOverlapTokens\": 100, \"Stage.Embed.EmbeddingModel\": \"text-embedding-3-large\", \"Stage.Embed.EmbeddingDimensions\": 2048, \"Stage.Index.APIEndpointConfigurationObjectId\": \"instances/{{instanceId}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\", \"Stage.Index.IndexName\": \"demo-index\", \"Stage.Index.IndexPartitionName\": \"Dune\" } } ] } The most important rules that govern the structure of data pipelines are: Both the data source and the stages must specify the configuration of the plugins that implement them. This is done by using the following properties: plugin_object_id: The FoundationaLLM resource identifier of the plugin that implements the data source or stage. plugin_parameters: The parameters that are required by the plugin. The parameters are defined by the plugin and can have default values. The values in the parameter_metadata are provided by the plugin definition. For more details about plugin parameters, see Plugins. plugin_dependencies: The dependencies that the plugin has. Dependencies are other plugins that the plugin requires to function properly. Dependencies can have their own parameters. The metadata required to select dependency plugins is provided by the plugin definition. For more details about plugin dependencies, see Plugins. The active flag indicates whether the pipeline can be triggered or not. If the pipeline is not active, it will not be executed. The data source must specify the FoundationaLLM resource identifier of the data source that it represents. This is done by using the data_source_object_id property. The data source object must be compatible with the pluging specified by the plugin_object_id property. The data source name and the stage names must be unique within the data pipeline pipeline. The data source name and the stage names can only contain alphanumerical characters, underlines, or hyphens. The stages must specify the next stages that the data will go through. This is done by using the next_stages property. The stages can have multiple next stages, forming a forest of stages. The stages must specify the configuration of the plugins that implement them, as described above. The triggers must specify the parameter values that are required by the data source and the stages. The parameter values are provided as a dictionary where the key is the parameter name and the value is the parameter value. The parameter values must be provided for all the parameters that are required by the data source and the stages. The parameter values are used to start the pipeline execution. Trigger parameter values naming convention The trigger parameter values are named using the following convention: For the data source, the naming structure is DataSource.{DataSourceName}.{ParameterName}. In the example above, the data source is named VGDataLake, so the parameter values for the data source are named DataSource.VGDataLake.{ParameterName}. For the stages, the naming structure is Stage.{StageName}.{ParameterName}. In the example above, the stages are named Extract, Partition, Embed, and Index, so the parameter values for the stages are named Stage.{StageName}.{ParameterName}. For stages that have dependency plugins, the naming structure is Stage.{StageName}.Dependency.{DependencyPluginName}.{ParameterName}. In the example above, the Partition stage has a dependency plugin named TokenContentTextPartitioning, so the parameter values for the dependency plugin are named Stage.Partition.Dependency.TokenContentTextPartitioning.{ParameterName}."
  },
  "docs/management-portal/reference/concepts/index.html": {
    "href": "docs/management-portal/reference/concepts/index.html",
    "title": "FoundationaLLM Concepts | FoundationaLLM",
    "summary": "FoundationaLLM Concepts The following mindmap provides a high-level overview of the core FoundationaLLM concepts. mindmap root((FoundationaLLM)) :::link:https://docs.foundationallm.ai (Authorization) Secret Key Role Based Access Control Policy Based Access Control Security Principal Virtual Security Principal Virtual User Virtual Security Group (Agent) Agent Workflow Agent Tool Agent Access Token Agent Access Token Virtual User Agent Virtual Security Group (Chargeback) (Data Pipeline) (Plugin) Plugin Package Plugin (Prompt) (Quota) Quota Definition API Raw Request Rate Agent Request Rate (Resource Provider) (Tool) (Workflow) Workflow Host FoundationaLLM Concepts Authorization Secret Key Role Based Access Control Policy Based Access Control Security Principal Virtual Security Principal Virtual User Virtual Security Group Agent Agent Workflow Agent Tool Agent Access Token Agent Access Token Virtual User Agent Virtual Security Group Chargeback Data Pipeline Plugin Plugin Package Plugin Prompt Quota Quota Definition API Raw Request Rate Agent Request Rate Resource Provider Tool Workflow Workflow Host Authorization Secret Key A secret key is a unique string that is used to authenticate an agent to the FoundationaLLM platform. The secret key is generated by the FoundationaLLM platform and is used to sign requests made by the agent. Role Based Access Control Role-based access control (RBAC) is a method of restricting access to resources based on the roles of users. In RBAC, roles are created and assigned to users, and permissions are assigned to roles. Policy Based Access Control Policy-based access control (PBAC) is a method of restricting access to resources based on policies. In PBAC, policies are created and assigned to users, and permissions are assigned to policies. Security Principal A security principal is an entity that can be authenticated and authorized to access resources. Examples of security principals include users, groups, and service identities. Virtual Security Principal A virtual security principal is a security principal that is created dynamically by the FoundationaLLM platform. Virtual security principals are used to represent agents and other entities that do not have a direct mapping to a user or group. Virtual User A virtual user is a virtual security principal that represents an agent access token. Virtual users are created by the FoundationaLLM platform and are used to authenticate agents via agent access tokens. Virtual Security Group A virtual security group is a virtual security principal that represents the group of agent access tokens created for a user. Virtual security groups are created by the FoundationaLLM platform and are used to manage access control for agents via agent access tokens. Agent Agent Workflow An agent workflow drives the core interactions of an agent with its associated tools. It can range from a simple call to an LLM that has function calling capabilities to a complex flow like the LangChain React Agent or an Open AI Assistant. Agent Tool An agent tool is a tool that is associated with an agent. Agent tools can be used to perform various tasks, such as executing code, interacting with APIs, or processing data. Agent Access Token An agent access token is a unique string that is used to authenticate an agent to the FoundationaLLM platform. The agent access token is generated by the FoundationaLLM platform and is used to create a virtual user. Agent Access Token Virtual User An agent access token virtual user is a virtual user that represents an agent access token. Agent access token virtual users are created by the FoundationaLLM platform and are used to authenticate agents via agent access tokens. Agent Virtual Security Group An agent virtual security group is a virtual security group that represents the group of agent access tokens created for an agent. Agent virtual security groups are created by the FoundationaLLM platform and are used to manage access control for agents via agent access tokens. Chargeback Chargeback is a mechanism for tracking and billing the usage of resources by agents, users, or other entities that can be associated with cost centers. Chargeback allows organizations to allocate costs based on the actual usage of resources by cost center-enabled entities. Data Pipeline A FoundationaLLM data pipeline defines a process that aims to extract data from a data source, transform it to make it suitable for LLM-based workloads (e.g., extracting text, partitioning text, extracting entities and relationships, embedding text, creating knowledge graphs, etc.), and load it into a target system (e.g., vector store, knowledge store, etc.). Data pipelines can be managed using the FoundationaLLM Management Portal (interactively) or the FoundationaLLM Management API (programmatically). FoundationaLLM data pipelines are built from the ground up to address the following key requirements for enterprise-grade Generative AI workloads: Scalability - handle large volumes of data and scale horizontally to meet the demands of enterprise workloads. FoundationaLLM data pipelines are designed to handle anything from a few documents or records to millions of documents or records. Flexibility - have a flexible and modular structure that is suitable from complex processing, parallelization of work, scalability, and reusability of components. Local and Global Processing - support local processing of data (e.g., text extraction, text partitioning, entity extraction, relationship extraction, etc.) and global processing of data (e.g., assembly of knowledge graphs). Parameterization - allow the configuration of data pipelines using parameters that can be set at runtime. This avoids the need to create multiple data pipelines for different configurations (e.g., a single data pipeline definition can be used to handle all user file uploads in the User Portal). Extensiblity - enable the addition of new data pipeline stages and the modification of existing data pipeline stages to meet the specific requirements of different use cases. All data pipeline components are implemented using plugins. For more details, see Data Pipelines. Plugin A plugin is a software component that can be used to extend the functionality of the FoundationaLLM platform. Plugins can be used to add new features, integrate with external systems, or perform other tasks. Plugins can be managed using the FoundationaLLM Management Portal (interactively) or the FoundationaLLM Management API (programmatically). Plugin Package A FoundationaLLM plugin package is a unit of versioning and deployment of one or more plugins. The Management Portal and Management API allow you to manage plugin packages. The following types of plugin packages are supported by the FoundationaLLM platform: Platform Package Type Description Python ZIP package The package can contain agent workflow and agent tool plugins that can be used through a workflow/tool plugin manager. This package type in not currently supported by Management API. It is scheduled to be replaced by a new type of Python package (Wheel) that will be fully supported by Management API. .NET NuGet package The package can contain data source, data pipeline stage, content text extraction, and content text partitioning plugins that can be used by the data pipelines infrastructure. This package type is fully supported by Management API. For more details, see Plugin Packages. Plugin The following types of plugins are supported by the FoundationaLLM platform: Platform Category Description Python Agent Workflow A Python agent workflow plugin implements an agent workflow that runs in the LangChain API and is responsible for driving the proccess of tool invocation and response generation. Python Agent Tool A Python agent tool plugin implement an agent tool that can be associated with an agent that uses a workflow that runs the LangChain workflow host (LangChain API). .NET Data Source A .NET data source plugin implements a data source that can be used to retrieve data from an external system and make it available to data pipelines. .NET Data Pipeline Stage A .NET data pipeline stage plugin implements a stage that can be executed as part of a data pipeline. .NET Content Text Extraction A .NET content text extraction plugin implements a content text extraction process that can be used to extract text from binary content (corresponding to various formats like PDF, DOCX, images, etc.). .NET Content Text Partitioning A .NET content text partitioning plugin implements a content text partitioning process that can be used to partition text into segments based on a specific strategy (e.g., token-based or semantic). For more details, see Plugins. Prompt A FoundationaLLM prompt is a text template that is used to create completion requests for agents. FoundationaLLM agents use prompts in two main contexts: Agent Workflow - Prompts are used to create completion requests that identify which tools should be invoked and how the responses from the tools should be processed. Agent Tool - Prompts are used to contribute with additional context to the agent workflow (e.g., helping the workflow to understand the capabilities of the tool), or to provide instructions on the tools internal logic (e.g., to generate code that will be executed by the tool or to finalize the response of the tool). Prompts have support for variables, which allows them to be dynamically generated based on the context of the completion request. This makes prompts flexible and adaptable to different scenarios. For more details, see Prompt variable. Quota A FoundationaLLM quota is a set of rules that define the limits on the usage of resources by a client. Quota definitions are used to enforce limits on the usage of resources by clients and to prevent abuse of the FoundationaLLM platform. Each quota is enforced on a specific metric, such as the number of API completion calls or the number of agent completion calls. The following quota metrics are supported by the FoundationaLLM platform: API requests (quota enforces an API request rate limit). Agent completion requests (quota enforces an agent completion request rate limit). Quota metric limits can be enforced globally or using specific partitioning mechanisms. The following quota metric partitioning mechanisms are supported by the FoundationaLLM platform: None (no partitioning, limit is enforced globally). User identifier (limit is enforced per user unique identifier). User principal name (limit is enforced per user principal name). Examples of FoundationaLLM quotas: 100 requests per minute for API requests sent to the the Completions endpoint of Core API. 50 requests per minute per user principal name for API requests sent to the the Completions endpoint of Core API. 1000 agent completions requests per quarter hour per user identifier for agent completion requests sent to the Core API. Quota Definition FoundationaLLM quotas are specified using quota definitions. A quota definition is a JSON object that defines the limits on the usage of resources by a client. Quota definitions are stored in the main FoundationaLLM storage account, in the quota container in a file named quota-store.json. For more details, see Quota Definition. API Raw Request Rate The API raw request rate is the maximum number of API requests that a client can make to a FoundationaLLM API within a specified time period. For more details, see API Raw Request Rate. Agent Request Rate The API call rate is the maximum number of Core API completion calls that a client can make to the Core API targeting a specific agent within a specified time period. The API call rate is used to limit the rate at which a client can perform completions targeting a specific agent. For more details, see Agent Request Rate. Resource Provider A resource provider is a platform component that is responsible for managing resources in the FoundationaLLM platform. Resource providers are used to create, update, delete, and retrieve resources such as agents, prompts, data sources, data pipelines, plugins and many more. All resource types available in the FoundationaLLM platform are managed by resource providers. The following resource providers are supported by the FoundationaLLM platform: Name Description FoundationaLLM.Agent Manages agents, agent workflows, agent tool, agent access tokens and other agent-related artifacts. FoundationaLLM.AIModel Manages AI models used across the platform. FoundationaLLM.Attachment Manages attachments that are uploaded by users through the User Portal or programmatically using Core API. FoundationaLLM.Authorization Manages the authorization artifacts like roles, policies, and permissions. FoundationaLLM.AzureOpenAI Manages artifacts related to the Azure OpenAI integration (e.g., Assistants files, vector stores, threads, etc.). FoundationaLLM.Configuration Manages the configuration settings of the FoundationaLLM platform. This includes settings from Azure App Configuration and API endpoint configurations. FoundationaLLM.DataPipeline Manages data pipelines, data pipeline stages, data pipeline runs, and other data pipeline-related artifacts. FoundationaLLM.DataSource Manages data sources and other data source-related artifacts. FoundationaLLM.Plugin Manages plugin packages and plugins. FoundationaLLM.Prompt Manages prompts and other prompt-related artifacts. Tool A tool is a well-defined software component that can be used to perform a specific task as part of the execution of an agent workflow. Tools are used by agents to interact with external systems, process data, or perform other tasks. Workflow A workflow is a sequence of steps that an agent follows to perform a specific task. Workflows can be simple or complex, and can involve multiple tools and interactions with external systems. Workflow Host A workflow host is an environment that runs agent workflows. The workflow host is responsible for executing agent workflows, managing the execution of tools, and handling the interactions between agents and external systems. The following workflow hosts are supported by the FoundationaLLM platform: Name Platform Description LangChain LangChain API (Python) The LangChain API is a Python-based workflow host that runs agent workflows using the LangChain orchestrator. SemanticKernel Semantic Kernel API (.NET) The Semantic Kernel API is a .NET-based workflow host that runs agent workflows using the Semantic Kernel orchestrator."
  },
  "docs/management-portal/reference/concepts/plugins-packages.html": {
    "href": "docs/management-portal/reference/concepts/plugins-packages.html",
    "title": "Plugins & Plugin Packages | FoundationaLLM",
    "summary": "Plugins & Plugin Packages This document provides reference information for plugins and plugin packages in FoundationaLLM. Overview Plugins extend the functionality of the FoundationaLLM platform. Plugins can be used to add new features, integrate with external systems, or perform other tasks. Plugins can be managed using the FoundationaLLM Management Portal (interactively) or the FoundationaLLM Management API (programmatically). Plugin Packages A FoundationaLLM plugin package is a unit of versioning and deployment of one or more plugins. Supported Package Types Platform Package Type Description Python ZIP package Contains agent workflow and agent tool plugins. Scheduled to be replaced by Wheel packages. .NET NuGet package Contains data source, data pipeline stage, content text extraction, and content text partitioning plugins. Fully supported by Management API. Package Naming Convention Plugin package names must follow a strict naming convention: {platform}-{name}, where: {platform} can be Dotnet or Python {name} can only contain alphanumerical characters, underlines, or hyphens Example: Dotnet-FoundationaLLMDataPipelinePlugins Managing Plugin Packages via API Create or Update a Plugin Package POST /instances/{instanceId}/providers/FoundationaLLM.Plugin/pluginPackages/{packageName} Request body: Must be of type form-data Must contain a file with the key file (the NuGet package) Must contain a text with key resource: { \"type\": \"plugin-package\", \"name\": \"{packageName}\" } List Plugin Packages GET /instances/{instanceId}/providers/FoundationaLLM.Plugin/pluginPackages Plugins Plugin Types Platform Category Description Python Agent Workflow Implements agent workflows running in LangChain API Python Agent Tool Implements agent tools for LangChain workflow host .NET Data Source Implements data sources for data pipelines .NET Data Pipeline Stage Implements stages for data pipeline execution .NET Content Text Extraction Implements text extraction from binary content .NET Content Text Partitioning Implements text partitioning strategies Plugin Naming Convention Plugin names must follow the format {platform}-{packageName}-{pluginName}, where: {platform} can be Dotnet or Python {packageName} is the plugin package name (alphanumerical, underlines, hyphens only) {pluginName} is the plugin name (alphanumerical, underlines, hyphens only) Example: Dotnet-FoundationaLLMDataPipelinePlugins-AzureAISearchIndexingDataPipelineStage Plugin Definition Structure { \"type\": \"plugin\", \"name\": \"Dotnet-FoundationaLLMDataPipelinePlugins-AzureAISearchIndexingDataPipelineStage\", \"object_id\": \"instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/...\", \"display_name\": \"Azure AI Search Indexing Data Pipeline Stage (FoundationaLLM)\", \"description\": \"Provides the FoundationaLLM standard implementation for indexing data pipeline stages.\", \"category\": \"Data Pipeline Stage\", \"parameters\": [...], \"parameter_selection_hints\": {...}, \"dependencies\": [...] } Plugin Parameters Each plugin has zero or more parameters. Supported parameter types: Type Description string A single string value int A single integer value float A single floating-point value bool A single boolean value datetime A single date and time value array An array of values resource-object-id A FoundationaLLM resource identifier Parameter Selection Hints For resource-object-id parameters, the parameter_selection_hints property provides UI guidance: \"parameter_selection_hints\": { \"APIEndpointConfigurationObjectId\": { \"resourcePath\": \"providers/FoundationaLLM.Configuration/apiEndpointConfigurations\", \"filterActionPayload\": { \"Category\": \"General\", \"Subcategory\": \"Indexing\" } } } Plugin Dependencies Plugins can have dependencies on other plugins. Dependency types: Single: Exactly one dependency plugin must be selected Multiple: One or more dependency plugins must be selected \"dependencies\": [ { \"selection_type\": \"Single\", \"dependency_plugin_names\": [ \"Dotnet-FoundationaLLMDataPipelinePlugins-TokenContentTextPartitioning\", \"Dotnet-FoundationaLLMDataPipelinePlugins-SemanticContentTextPartitioning\" ] } ] Managing Plugins via API List All Plugins GET /instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins Filter Plugins by Category POST /instances/{instanceId}/providers/FoundationaLLM.Plugin/plugins/filter Request body: { \"categories\": [ \"Data Source\", \"Data Pipeline Stage\" ] } Supported categories: Data Source, Data Pipeline Stage, Context Text Extraction, Content Text Partitioning Related Topics Managing Plugins Data Pipelines Agents & Workflows"
  },
  "docs/management-portal/reference/concepts/prompt-variables.html": {
    "href": "docs/management-portal/reference/concepts/prompt-variables.html",
    "title": "Prompt Variable | FoundationaLLM",
    "summary": "Prompt Variable A prompt variable is a placeholder in a prompt that can be replaced with specific values when the prompt is executed. This allows for dynamic and flexible prompts that can adapt to different contexts or inputs. FoundationaLLM supports prompt variables in the following format: {{foundationallm:variable_name[:format]}} where: foundationallm is a reserved keyword that indicates the variable is a FoundationaLLM prompt variable. variable_name is the name of the variable that will be replaced with a specific value. format is an optional parameter that specifies the format of the value to be used when replacing the variable. If not specified, the value will be used as is. Note Prompt variables are extrapolated in a .NET module written in C# which means that format is a .NET format string. For more details on the available format strings, see the Standard Numeric Format Strings and Standard Date and Time Format Strings documentation. The following table provides details about the prompt variables supported by FoundationaLLM: Variable Description Notes {{foundationallm:router_prompt}} Used to insert the tool selection information into the main workflow prompt. This variable should only be used for agents that have tools associated with them. {{foundationallm:tool_list}} Used to insert the list of tools available to the agent into the tool selection information (which in turn is inserted into the main workflow prompt). This variable should only be used for agents that have tools associated with them. {{foundationallm:tool_router_prompts}} Used to insert additional, per tool instructions to help tool selection into the tool selection information (which in turn is inserted into the main workflow prompt). This variable should only be used for agents that have tools associated with them. {{foundationallm:current_datetime_utc[:format]}} Used to insert the current date and time in UTC into the prompt. The format parameter is optional and can be used to specify the format of the date and time. If not specified, the default format is used. The default format is yyyy-MM-ddTHH:mm:ssZ, which represents the date and time in ISO 8601 format. For more details on the available format strings, see the Standard Date and Time Format Strings documentation."
  },
  "docs/management-portal/reference/concepts/prompts-resources.html": {
    "href": "docs/management-portal/reference/concepts/prompts-resources.html",
    "title": "Prompt | FoundationaLLM",
    "summary": "Prompt The FoundationaLLM (FLLM) prompt resource encapsulates the system prompt of an agent. The system prompt describes the persona of the agent and any instructional guardrails used to generate the desired responses to user prompts. The prompt resource is used in the Knowledge Management agent configuration. Prompt configuration The structure of a prompt is the following: { \"type\": \"multipart\", \"name\": \"<name>\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Prompt/prompts/<name>\", \"description\": \"<description>\", \"prefix\": \"<prompt_prefix>\", \"suffix\": \"<prompt_suffix>\" } where: <name> is the name of the agent. <instance_id> is the instance ID of the deployment. <description> is the description of the prompt, describing the persona of the agent. <prompt_prefix> is the beginning of the prompt. <prompt_suffix> (optional) appended to the end of the prompt (after any prefix and context). Parameter Description type The type - will be multipart. multipart prompts have a prefix and suffix. Support for basic prompts, which have no suffix, will be added in a future release. type must be the first parameter in the request body. name The name of the prompt. object_id The object ID of the prompt. Remove this key when creating a prompt, as it is automatically populated by the Management API. description The description of the prompt, ensure this description details the purpose or role of the prompt. prompt_prefix The beginning of the prompt. prompt_suffix Text appended to the ending of the prompt. Managing prompts This section describes how to manage knowledge management prompts using the Management API. {{baseUrl}} is the base URL of the Management API. {{instanceId}} is the unique identifier of the FLLM instance. Retrieve HTTP GET {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts Create or update HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name> Content-Type: application/json BODY <prompt_configuration> where <prompt_configuration> is the prompt configuration structure described above. Delete HTTP DELETE {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name> Note The delete operation is a logical delete. To purge a Prompt, call the /purge endpoint after deleting the Prompt. Purge HTTP POST {{baseUrl}}/instances/{{instanceId}}/providers/FoundationaLLM.Prompt/prompts/<name>/purge Content-Type: application/json BODY {}"
  },
  "docs/management-portal/reference/concepts/quotas.html": {
    "href": "docs/management-portal/reference/concepts/quotas.html",
    "title": "Quotas | FoundationaLLM",
    "summary": "Quotas This document provides reference information for quotas in FoundationaLLM. Overview A FoundationaLLM quota is a set of rules that define the limits on the usage of resources by a client. Quota definitions are used to enforce limits on the usage of resources by clients and to prevent abuse of the FoundationaLLM platform. Each quota is enforced on a specific metric: API requests: Quota enforces an API request rate limit Agent completion requests: Quota enforces an agent completion request rate limit Quota metric limits can be enforced globally or using specific partitioning mechanisms: None: No partitioning, limit is enforced globally User Identifier: Limit is enforced per user unique identifier User Principal Name: Limit is enforced per user principal name Quota Definition Quota definitions are stored in the main FoundationaLLM storage account, in the quota container in a file named quota-store.json. Structure { \"name\": \"CoreAPICompletionsUPNRawRequestRateLimit\", \"description\": \"Defines a per UPN raw request rate limit on the Core API Completions controller.\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 120, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } Properties Property Description name The name of the quota definition description A description of the quota definition context The context (format: <service>:<controller> or <service>:<controller>:<agent>) type RawRequestRateLimit or AgentRequestRateLimit metric_partition None, UserPrincipalName, or UserIdentifier metric_limit Maximum number of requests in the time window metric_window_seconds Time window in seconds lockout_duration_seconds Lockout duration after exceeding quota distributed_enforcement Whether to enforce across multiple API instances Smoothing Time Window FoundationaLLM uses a smoothing time window of 20 seconds for quota enforcement. Recommendations: Set metric_window_seconds to a multiple of 20 seconds (60 seconds is standard) Set metric_limit to a multiple of metric_window_seconds / 20 API Raw Request Rate The API raw request rate quota limits the number of raw requests to API controllers. Supported Controllers Controller Context Completions CoreAPI:Completions CompletionsStatus CoreAPI:CompletionsStatus Branding CoreAPI:Branding Configuration CoreAPI:Configuration Files CoreAPI:Files OneDriveWorkSchool CoreAPI:OneDriveWorkSchool Sessions CoreAPI:Sessions UserProfiles CoreAPI:UserProfiles Status CoreAPI:Status Example: 100 API requests per minute per user { \"name\": \"CoreAPICompletionsRateLimit\", \"description\": \"100 requests per minute per user\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 100, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } CompletionsStatus Controller Note Starting with FoundationaLLM v0.9.7-rc158, the CompletionsStatus controller handles status checks. Client applications (especially the User Portal) poll this endpoint frequently. Consider setting higher limits for CompletionsStatus than for Completions. Agent Request Rate The agent request rate quota limits completion requests to specific agents. Context Format CoreAPI:Completions:<agent_name> Example: 50 completions per minute per user for a specific agent { \"name\": \"MyAgentRateLimit\", \"description\": \"50 completions per minute per user\", \"context\": \"CoreAPI:Completions:my-agent-name\", \"type\": \"AgentRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 50, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } Metric Partition Guidance Scenario Recommended Partition Standard user access UserPrincipalName Service-to-service calls (managed identity) UserIdentifier Global limit for all users None Examples Global limit of 1000 requests per minute { \"name\": \"GlobalAPILimit\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"None\", \"metric_limit\": 1000, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": true } Per-user limit with distributed enforcement { \"name\": \"DistributedUserLimit\", \"context\": \"CoreAPI:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 120, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": true } Related Topics Configuring Quotas Monitoring Token Consumption"
  },
  "docs/management-portal/reference/concepts/resource-management.html": {
    "href": "docs/management-portal/reference/concepts/resource-management.html",
    "title": "Resource Management in FoundationaLLM | FoundationaLLM",
    "summary": "Resource Management in FoundationaLLM With the introduction of the Management API, you can now manage resources in FoundationaLLM programmatically or through the Management API User Interface Portal. This includes creating, updating, and deleting resources in the system. Resource Providers The main concept of the Management API is the resource provider. A resource provider is a service that provides resources to the FoundationaLLM system. For example, the agents, prompts and datasources are provided by a resource provider. The Management API provides a way to manage these resources without the need to manually work with JSON files in storage containers and mimics the same concept and functionality of resources in the Azure Portal. Resource Provider Structure The resource-provider container in the main storage account that was deployed on your behalf in your subscription contains the following structure: Agent References This first folder FoundationaLLM.Agent contains the Agent References. The content of the _agent-references references all the locations of the JSON files that contain the agent information. The _agent-references folder contains the following structure: { \"AgentReferences\": [ { \"Name\": \"sotu-2023\", \"Filename\": \"/FoundationaLLM.Agent/sotu-2023.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu2\", \"Filename\": \"/FoundationaLLM.Agent/sotu2.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu3\", \"Filename\": \"/FoundationaLLM.Agent/sotu3.json\", \"Type\": \"knowledge-management\" }, { \"Name\": \"sotu\", \"Filename\": \"/FoundationaLLM.Agent/sotu.json\", \"Type\": \"knowledge-management\" } ] } From that starting point for the agent references, we get to point to JSON file that describes each agent available to the system. Let's start by taking a look at one odf the agents from above called sotu-2023.json { \"name\": \"sotu-2023\", \"type\": \"knowledge-management\", \"object_id\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Agent/agents/sotu-2023\", \"description\": \"Knowledge Management Agent that queries the State of the Union speech transcript\", \"indexing_profile\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Vectorization/indexingprofiles/sotu-index\", \"embedding_profile\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding\", \"language_model\": { \"type\": \"openai\", \"provider\": \"microsoft\", \"temperature\": 0.0, \"use_chat\": true, \"api_endpoint\": \"FoundationaLLM:AzureOpenAI:API:Endpoint\", \"api_key\": \"FoundationaLLM:AzureOpenAI:API:Key\", \"api_version\": \"FoundationaLLM:AzureOpenAI:API:Version\", \"version\": \"FoundationaLLM:AzureOpenAI:API:Completions:ModelVersion\", \"deployment\": \"FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName\" }, \"sessions_enabled\": true, \"conversation_history\": { \"enabled\": true, \"max_history\": 5 }, \"gatekeeper\": { \"use_system_setting\": false, \"options\": [ \"ContentSafety\", \"Presidio\" ] }, \"orchestrator\": \"LangChain\", \"prompt\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Prompt/prompts/sotu\" } Notice all the different keys and values that are present to identify the agent. This JSON file is usually created or modifed through the Management API UI Portal or via POST or PUT requests to the Management API using a product like POSTMAN. The type could be \"knowledge-management\" or \"analytical\" The language-model section is to identify the provider, its accuracy and endpoints to retrieve from the app configuration resource. sessions_enabled is a boolean to enable or disable the ability to start a session Vs just a one time query using an API tool like Postman. conversation_history is to enable or disable the ability to store the conversation history and the maximum number of conversations to store in case the previous session_enabled is set to true. The gatekeeper section is to enable or disable the use of the system settings for content safety and presidio. If set to false, then the options array will be used to identify the specific gatekeepers to use. The orchestrator is the name of the orchestrator to use for the agent. The orchestrator is the component that is responsible for managing the flow of the conversation and the execution of the agent's logic. It could be LangChain or Semantic Kernel and more options could be used in the future with the growth of the platform and the industry for orchestrators. The prompt is the reference to the prompt that the agent will use to start the conversation. The prompt is a resource that is used to start the conversation with the agent. It is a JSON file that contains the prompt text and the prompt settings. Prompt References The second folder FoundationaLLM.Prompt contains the Prompt References. Within that folder, we have the _prompt-references JSON file that contains the following structure: { \"PromptReferences\": [ { \"Name\": \"sotu5\", \"Filename\": \"/FoundationaLLM.Prompt/sotu5.json\" }, { \"Name\": \"sotu-test\", \"Filename\": \"/FoundationaLLM.Prompt/sotu-test.json\" }, { \"Name\": \"sotu\", \"Filename\": \"/FoundationaLLM.Prompt/sotu.json\" } ] } These references point to the JSON files that contain the prompt information. Let's take a look at one of the prompts from above called sotu5.json for an example: { \"name\": \"sotu5\", \"type\": \"prompt\", \"object_id\": \"/instances/1bc45134-6985-48b9-9466-c5f70ddaaa65/providers/FoundationaLLM.Prompt/prompts/sotu5\", \"description\": \"Prompt for the Knowledge Management Agent that queries the State of the Union speech transcript\", \"prefix\": \"You are a political science professional named Baldwin. You are responsible for answering questions regarding the February 2023 State of the Union Address.\\nAnswer only questions about the February 2023 State of the Union address. Do not make anything up. Check your answers before replying.\\nProvide concise answers that are polite and professional.\", \"suffix\": \"\" } It contains the name, type of prompt, the object_id reference, description and of course most importantly the prefix and suffix of the prompt. The prefix and suffix are the text that will be used to start and end the conversation with the agent. Data Source References A Data Source refers to the location of data that is to be leveraged by an agent. The data source could be a storage account, database, website, etc. The data source references are stored in the FoundationaLLM.DataSource folder. The references are stored in the _datasource-references JSON file that contains the following structure: { \"DataSourceReferences\": [ { \"Name\": \"datalake01\", \"Filename\": \"/FoundationaLLM.DataSource/datalake01.json\", \"Type\": \"azure-data-lake\", \"Deleted\": false }, { \"Name\": \"sharepointsite01\", \"Filename\": \"/FoundationaLLM.DataSource/sharepointsite01.json\", \"Type\": \"sharepoint-online-site\", \"Deleted\": false } ] } These references point to the JSON files that contain the prompt information. Let's take a look at one of the prompts from above called datalake01.json for an example: { \"name\": \"datalake01\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"display_name\": null, \"description\": \"Azure Data Lake data source.\", \"folders\": [ \"/vectorization-input/journals/2024\" ], \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:DataSources:datalake01:AuthenticationType\", \"ConnectionString\": \"FoundationaLLM:DataSources:datalake01:ConnectionString\", \"APIKey\": \"FoundationaLLM:DataSources:datalake01:APIKey\", \"Endpoint\": \"FoundationaLLM:DataSources:datalake01:Endpoint\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } In this example, the data source is an Azure Data Lake data source. The folders array contains the paths to the folders in the data lake that contain the data to be used by the agent. The configuration_references section contains the references to the configuration settings that are used to connect to the data source. The created_on, updated_on, created_by, updated_by are the timestamps and the user that created and updated the data source. The deleted flag is used to mark the data source as deleted. Vectorization Profile References Finally the third folder FoundationaLLM.Vectorization contains the Vectorization References. Where you will find important JSON files: vectorization-indexing-profiles.json { \"DefaultResourceName\": \"AzureAISearch_Default_002\", \"Resources\": [ { \"type\": \"indexing-profile\", \"name\": \"AzureAISearch_Default_002\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/indexingprofiles/AzureAISearch_Default_002\", \"description\": null, \"deleted\": false, \"indexer\": \"AzureAISearchIndexer\", \"settings\": { \"IndexName\": \"fllm-default-002\", \"TopN\": \"3\", \"Filters\": \"\", \"EmbeddingFieldName\": \"Embedding\", \"TextFieldName\": \"Text\" }, \"configuration_references\": { \"AuthenticationType\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:AuthenticationType\", \"Endpoint\": \"FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint\" } } ] } This is where we identify the name and the Indexer to use for the indexing of the content. And within the configuration_references section, we identify the AuthenticationType and Endpoint to use for the indexing. It could be indexing against the Azure AI Search or any other indexer that is available in the system and more will be supported in the future. The DefaultResourceName is the name of the default indexing profile to use in the system if none is specified. vectorization-text-embedding-profiles.json { \"DefaultResourceName\": \"AzureOpenAI_Embedding\", \"Resources\": [ { \"type\": \"text-embedding-profile\", \"name\": \"AzureOpenAI_Embedding\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding\", \"description\": null, \"deleted\": false, \"text_embedding\": \"SemanticKernelTextEmbedding\", \"settings\": {}, \"configuration_references\": { \"APIKey\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIKey\", \"APIVersion\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:APIVersion\", \"AuthenticationType\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:AuthenticationType\", \"DeploymentName\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName\", \"Endpoint\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint\" } } ] } This is where we identify the name and the Text Embedding to use for the vectorization of the text. And within the configuration_references section, we identify the APIKey, APIVersion, AuthenticationType, DeploymentName and Endpoint to use for the text embedding. vectorization-text-partitioning-profiles.json { \"DefaultResourceName\": \"DefaultTokenTextPartition_Small\", \"Resources\": [ { \"type\": \"text-partitioning-profile\", \"name\": \"DefaultTokenTextPartition_Small\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textpartitioningprofiles/DefaultTokenTextPartition_Small\", \"display_name\": null, \"description\": null, \"text_splitter\": \"TokenTextSplitter\", \"settings\": { \"Tokenizer\": \"MicrosoftBPETokenizer\", \"TokenizerEncoder\": \"cl100k_base\", \"ChunkSizeTokens\": \"500\", \"OverlapSizeTokens\": \"50\" }, \"configuration_references\": {}, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } ] } This is where we identify the name and the Text Splitter to use for the chunking and overlapping of the text. In the settings section, we identify the tokenizer and the encoder to use for the text partitioning and the chunk size and overlap size in tokens. vectorization-pipelines.json A vectorization pipeline provides a definition for a reusable and triggerable profile that includes identifying a data source that is the source for vectorization, vectorization profiles, as well as the trigger type. { \"DefaultResourceName\": \"sdzwa\", \"Resources\": [ { \"type\": \"vectorization-pipeline\", \"name\": \"sdzwa\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationPipelines/sdzwa\", \"display_name\": null, \"description\": \"Vectorization data pipeline dedicated to the sdzwa january 2024 pdf.\", \"active\": false, \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"text_partitioning_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textPartitioningProfiles/Streamline\", \"text_embedding_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/textembeddingprofiles/AzureOpenAI_Embedding_v2\", \"indexing_profile_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/indexingprofiles/AzureAISearch_Default_002\", \"trigger_type\": \"Event\", \"trigger_cron_schedule\": null, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } ] } Vectorization Request Resources The storage of vectorization request resources are located in the vectorization-state container following the standard organization of /requests/yyyyMMdd/yyyyMMdd-vectorizationrequestid.json where yyyyMMdd (UTC) is the date of the request and vectorizationrequestid is the unique identifier of the request. When a vectorization request is received, the request gets created and is updated once the request has been processed. An example of a completed vectorization request is: { \"id\": \"f8d940a2-77c0-4b3e-8709-e26445f9743e\", \"object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationRequests/f8d940a2-77c0-4b3e-8709-e26445f9743e\", \"Expired\": false, \"resource_filepath\": \"requests/20240419/20240419-f8d940a2-77c0-4b3e-8709-e26445f9743e.json\", \"content_identifier\": { \"data_source_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.DataSource/dataSources/datalake01\", \"multipart_id\": [ \"fllmaks14sa.dfs.core.windows.net\", \"vectorization-input\", \"sdzwa/journals/2024/SDZWA-Journal-January-2024.pdf\" ], \"canonical_id\": \"sdzwa/journals/2024/SDZWA-Journal-January-2024\", \"metadata\": null }, \"processing_type\": \"Asynchronous\", \"pipeline_object_id\": \"/instances/1e22cd2a-7b81-4160-b79f-f6443e3a6ac2/providers/FoundationaLLM.Vectorization/vectorizationPipelines/sdzwa\", \"pipeline_execution_id\": \"541ae81c-08ea-4ba0-b58c-9d904260a5a2\", \"processing_state\": \"Completed\", \"execution_start\": \"2024-04-19T03:57:33.4571856Z\", \"execution_end\": \"2024-04-19T04:00:41.8572236Z\", \"error_messages\": [], \"steps\": [ { \"id\": \"extract\", \"parameters\": {} }, { \"id\": \"partition\", \"parameters\": { \"text_partitioning_profile_name\": \"Streamline\" } }, { \"id\": \"embed\", \"parameters\": { \"text_embedding_profile_name\": \"AzureOpenAI_Embedding_v2\" } }, { \"id\": \"index\", \"parameters\": { \"indexing_profile_name\": \"AzureAISearch_Default_002\" } } ], \"completed_steps\": [ \"extract\", \"partition\", \"embed\", \"index\" ], \"remaining_steps\": [], \"current_step\": null, \"error_count\": 0, \"running_operations\": {}, \"last_successful_step_time\": \"2024-04-19T04:00:41.8554435Z\" } Valid states for the processing_state property are New, InProgress, Completed, and Failed. Any errors encountered during the processing of a request are stored in the error_messages array. Note: Triggering the vectorization process is done through the Management API by issuing a process action on the resource. See the Triggering Vectorization section for more information. Synchronous Versus Asynchronous Vectorization The vectorization process can be done in a synchronized or asynchronized manner. The synchronized manner is when the vectorization process is done in real time in memory and the results are returned immediately. The asynchronized manner is when the vectorization process is done in the background and the results are returned at a later time. The asynchronized manner is useful when the vectorization process is expected to take a long time to complete and the user does not want to wait for the results. The asynchronized manner is also useful when the vectorization process is expected to be done in batches and the user does not want to wait for the results of each batch. For example, you would use syncronized vactorization when you have one or few files that you want to vectorize and you want the results immediately. You would use asynchronized vectorization when you have hundred or thousands of files that you want to vectorize and you want the results at a later time."
  },
  "docs/management-portal/reference/configuration-reference.html": {
    "href": "docs/management-portal/reference/configuration-reference.html",
    "title": "Configuration Reference | FoundationaLLM",
    "summary": "Configuration Reference Reference documentation for FoundationaLLM configuration settings. Overview FoundationaLLM uses Azure App Configuration as the central store for application settings. This document provides a reference for key configuration categories and their settings. Configuration Sources Source Purpose Azure App Configuration Application settings, feature flags, branding Azure Key Vault Secrets, API keys, connection strings Environment Variables Runtime environment settings, container configuration Configuration Key Structure Keys follow a hierarchical naming convention: FoundationaLLM:{Category}:{SubCategory}:{Setting} Example: FoundationaLLM:APIs:CoreAPI:APIUrl API Configuration Core API Key Description FoundationaLLM:APIs:CoreAPI:APIUrl Core API base URL FoundationaLLM:APIs:CoreAPI:APIScope OAuth scope for Core API FoundationaLLM:APIs:CoreAPI:AppInsightsConnectionString Application Insights connection Management API Key Description FoundationaLLM:APIs:ManagementAPI:APIUrl Management API base URL FoundationaLLM:APIs:ManagementAPI:APIScope OAuth scope for Management API Gateway API Key Description FoundationaLLM:APIs:GatewayAPI:APIUrl Gateway API base URL FoundationaLLM:APIs:GatewayAPI:APIKey (Key Vault reference) API key Authorization API Key Description FoundationaLLM:APIs:AuthorizationAPI:APIUrl Authorization API base URL FoundationaLLM:APIs:AuthorizationAPI:APIScope OAuth scope for Authorization API Branding Configuration See Branding Reference for complete branding settings. Key Description Default FoundationaLLM:Branding:CompanyName Organization name FoundationaLLM FoundationaLLM:Branding:PageTitle Browser tab title FoundationaLLM User Portal FoundationaLLM:Branding:PrimaryColor Primary UI color #131833 FoundationaLLM:Branding:LogoUrl Logo image path foundationallm-logo-white.svg FoundationaLLM:Branding:KioskMode Enable kiosk mode false Authentication Configuration Entra ID (Azure AD) Key Description FoundationaLLM:Instance:Id FoundationaLLM instance identifier FoundationaLLM:Entra:Instance Azure AD instance URL FoundationaLLM:Entra:TenantId Azure AD tenant ID FoundationaLLM:Entra:ClientId Application (client) ID FoundationaLLM:Entra:Scopes Default OAuth scopes FoundationaLLM:Entra:CallbackPath OAuth callback path Portal-Specific Authentication Key Description FoundationaLLM:UserPortal:Entra:ClientId User Portal client ID FoundationaLLM:ManagementPortal:Entra:ClientId Management Portal client ID Storage Configuration Azure Blob Storage Key Description FoundationaLLM:BlobStorageMemorySource:BlobStorageConnection (Key Vault) Storage connection string FoundationaLLM:BlobStorageMemorySource:BlobStorageContainer Container name CosmosDB Key Description FoundationaLLM:CosmosDB:Endpoint CosmosDB endpoint URL FoundationaLLM:CosmosDB:Key (Key Vault) CosmosDB key FoundationaLLM:CosmosDB:Database Database name Orchestration Configuration Agent Settings Key Description FoundationaLLM:Agent:DefaultAgentName Default agent for new conversations FoundationaLLM:Agent:ConversationHistoryMaxMessages Max messages in conversation context LLM Provider Settings Key Description FoundationaLLM:AzureOpenAI:Endpoint Azure OpenAI endpoint FoundationaLLM:AzureOpenAI:DeploymentName Default deployment name FoundationaLLM:AzureOpenAI:ApiKey (Key Vault) API key Feature Flags Feature flags control optional functionality: Key Description Default FoundationaLLM:Features:EnableRating Enable response rating true FoundationaLLM:Features:EnableFileUpload Enable file uploads true FoundationaLLM:Features:EnableAgentSelection Enable agent selection true Quota Configuration Quotas are defined as JSON objects in App Configuration: Key Description FoundationaLLM:Quota:APIRawRequestRate API request rate limits FoundationaLLM:Quota:AgentRequestRate Agent completion request limits See Quotas Reference for quota structure details. Environment Variables Common environment variables used by services: Variable Description ASPNETCORE_ENVIRONMENT Runtime environment (Development, Production) AZURE_CLIENT_ID Managed identity client ID AZURE_TENANT_ID Azure tenant ID APPLICATIONINSIGHTS_CONNECTION_STRING Application Insights connection Key Vault References Sensitive values use Key Vault references in App Configuration: { \"uri\": \"https://{keyvault-name}.vault.azure.net/secrets/{secret-name}\" } The application automatically resolves these references at runtime using managed identity. Accessing Configuration Via Management Portal Navigate to FLLM Platform > Configuration View and edit available settings Via Azure Portal Open your Azure App Configuration resource Use Configuration explorer to browse keys Use Feature manager for feature flags Via API # List all configurations GET /instances/{instanceId}/providers/FoundationaLLM.Configuration/appConfigurations Best Practices Use Key Vault for all secrets and sensitive values Use labels in App Configuration for environment-specific settings Document changes when modifying configuration Test in non-production before applying to production Monitor configuration changes through App Configuration audit logs Related Topics Branding Reference Permissions & Roles Reference Quotas Reference"
  },
  "docs/management-portal/reference/permissions-roles.html": {
    "href": "docs/management-portal/reference/permissions-roles.html",
    "title": "Permissions & Roles Reference | FoundationaLLM",
    "summary": "Permissions & Roles Reference Reference documentation for FoundationaLLM permissions and role definitions. Overview FoundationaLLM implements a Role-Based Access Control (RBAC) system that mirrors Azure RBAC patterns. Access is controlled through: Role Definitions: Named sets of permissions Role Assignments: Bindings between principals, roles, and scopes Authorizable Actions: Granular permission strings Role Definitions Core Roles Owner Property Value ID 1301f8d4-3bea-4880-945f-315dbd2ddb46 Description Full access to manage all resources, including the ability to assign roles in FoundationaLLM RBAC. Permissions * (all actions) Use Cases: Instance administrators Full platform management Managing other users' access Contributor Property Value ID a9f0020f-6e3a-49bf-8d1d-35fd53058edf Description Full access to manage all resources without the ability to assign roles in FoundationaLLM RBAC. Permissions * (all actions) Excluded FoundationaLLM.Authorization/*/write, FoundationaLLM.Authorization/*/delete Use Cases: Creating and managing agents, data sources, pipelines Platform configuration (non-security) Reader Property Value ID 00a53e72-f66e-4c03-8f81-7e885fd2eb35 Description View all resources without the ability to make any changes. Permissions */read Use Cases: Auditors and compliance reviewers Read-only access for reporting User Access Administrator Property Value ID fb8e0fd0-f7e2-4957-89d6-19f44f7d6618 Description Manage access to FoundationaLLM resources. Permissions */read, FoundationaLLM.Authorization/* Use Cases: Delegated access management User onboarding/offboarding Role Based Access Control Administrator Property Value ID 17ca4b59-3aee-497d-b43b-95dd7d916f99 Description Manage access to FoundationaLLM resources by assigning roles using FoundationaLLM RBAC. Permissions Role assignment read/write/delete, role definition read Use Cases: Focused access management (no resource modification) Specialized Contributor Roles Agents Contributor Property Value ID 3f28aa77-a854-4aa7-ae11-ffda238275c9 Description Create new agents. Attachments Contributor Property Value ID 8e77fb6a-7a78-43e1-b628-d9e2285fe25a Description Upload attachments including uploading to Azure OpenAI file store. Permissions Attachment read/write, Azure OpenAI conversation/file mappings, API endpoint configs, AI models Use Cases: Users who need to upload files to agents Conversations Contributor Property Value ID d0d21b90-5317-499a-9208-3a6cb71b84f9 Description Create and update conversations, including Azure OpenAI Assistants threads. Permissions Conversation read/write, Azure OpenAI conversation mappings, API endpoint configs, AI models Use Cases: Chat user portal users Data Pipelines Contributor Property Value ID 2da16a58-ed63-431a-b90e-9df32c2cae4a Description Create new data pipelines. Permissions Read access to data pipelines, vectorization pipelines, data sources, profiles, plugins Use Cases: Data engineers creating pipelines Data Pipelines Execution Manager Property Value ID e959eecb-8edf-4442-b532-4990f9a1df2b Description Manage all aspects related to data pipeline runs. Permissions Data pipeline read/write, data sources, API endpoints, AI models, plugins, vector databases Use Cases: Operators running and monitoring pipelines Prompts Contributor Property Value ID 479e7b36-5965-4a7f-baf7-84e57be854aa Description Create new prompts. Vector Databases Contributor Property Value ID c026f070-abc2-4419-aed9-ec0676f81519 Description Create new vector databases. Permissions Vector database read, API endpoint configuration read Agent Access Tokens Contributor Property Value ID 8c5ea0d3-f5a1-4be5-90a7-a12921c45542 Description Create new agent access tokens. Resource Providers Administrator Property Value ID 63b6cc4d-9e1c-4891-8201-cf58286ebfe6 Description Execute management actions on resource providers. Permissions */management/write Authorizable Actions Actions follow the pattern: {ResourceProvider}/{ResourceType}/{Operation} Authorization Actions Action Description FoundationaLLM.Authorization/roleAssignments/read Read role assignments FoundationaLLM.Authorization/roleAssignments/write Create or update role assignments FoundationaLLM.Authorization/roleAssignments/delete Delete role assignments FoundationaLLM.Authorization/roleDefinitions/read Read role definitions FoundationaLLM.Authorization/securityPrincipals/read Read security principals (users, groups, service principals) FoundationaLLM.Authorization/management/write Execute management actions Agent Actions Action Description FoundationaLLM.Agent/agents/read Read agents FoundationaLLM.Agent/agents/write Create or update agents FoundationaLLM.Agent/agents/delete Delete agents FoundationaLLM.Agent/workflows/read Read workflows FoundationaLLM.Agent/workflows/write Create or update workflows FoundationaLLM.Agent/workflows/delete Delete workflows FoundationaLLM.Agent/tools/read Read tools FoundationaLLM.Agent/tools/write Create or update tools FoundationaLLM.Agent/tools/delete Delete tools FoundationaLLM.Agent/agentTemplates/read Read agent templates FoundationaLLM.Agent/agentTemplates/write Create or update agent templates FoundationaLLM.Agent/agentTemplates/delete Delete agent templates FoundationaLLM.Agent/management/write Execute management actions AI Model Actions Action Description FoundationaLLM.AIModel/aiModels/read Read AI models FoundationaLLM.AIModel/aiModels/write Create or update AI models FoundationaLLM.AIModel/aiModels/delete Delete AI models FoundationaLLM.AIModel/management/write Execute management actions Attachment Actions Action Description FoundationaLLM.Attachment/attachments/read Read attachments FoundationaLLM.Attachment/attachments/write Create or update attachments FoundationaLLM.Attachment/attachments/delete Delete attachments Azure AI Actions Action Description FoundationaLLM.AzureAI/agentConversationMappings/read Read Azure AI Agent Service conversation mappings FoundationaLLM.AzureAI/agentConversationMappings/write Create or update Azure AI Agent Service conversation mappings FoundationaLLM.AzureAI/agentConversationMappings/delete Delete Azure AI Agent Service conversation mappings FoundationaLLM.AzureAI/agentFileMappings/read Read Azure AI Agent Service file mappings FoundationaLLM.AzureAI/agentFileMappings/write Create or update Azure AI Agent Service file mappings FoundationaLLM.AzureAI/agentFileMappings/delete Delete Azure AI Agent Service file mappings FoundationaLLM.AzureAI/projects/read Read Azure AI project resources FoundationaLLM.AzureAI/projects/write Create or update Azure AI project resources FoundationaLLM.AzureAI/projects/delete Delete Azure AI project resources FoundationaLLM.AzureAI/management/write Execute management actions Azure OpenAI Actions Action Description FoundationaLLM.AzureOpenAI/conversationMappings/read Read Azure OpenAI conversation mappings FoundationaLLM.AzureOpenAI/conversationMappings/write Create or update Azure OpenAI conversation mappings FoundationaLLM.AzureOpenAI/conversationMappings/delete Delete Azure OpenAI conversation mappings FoundationaLLM.AzureOpenAI/fileMappings/read Read Azure OpenAI file mappings FoundationaLLM.AzureOpenAI/fileMappings/write Create or update Azure OpenAI file mappings FoundationaLLM.AzureOpenAI/fileMappings/delete Delete Azure OpenAI file mappings FoundationaLLM.AzureOpenAI/management/write Execute management actions Configuration Actions Action Description FoundationaLLM.Configuration/appConfigurations/read Read app configurations FoundationaLLM.Configuration/appConfigurations/write Create or update app configurations FoundationaLLM.Configuration/appConfigurations/delete Delete app configurations FoundationaLLM.Configuration/appConfigurationSets/read Read app configuration sets FoundationaLLM.Configuration/keyVaultSecrets/read Read Key Vault secrets FoundationaLLM.Configuration/keyVaultSecrets/write Create or update Key Vault secrets FoundationaLLM.Configuration/keyVaultSecrets/delete Delete Key Vault secrets FoundationaLLM.Configuration/apiEndpointConfigurations/read Read API endpoint configurations FoundationaLLM.Configuration/apiEndpointConfigurations/write Create or update API endpoint configurations FoundationaLLM.Configuration/apiEndpointConfigurations/delete Delete API endpoint configurations FoundationaLLM.Configuration/management/write Execute management actions Context Actions Action Description FoundationaLLM.Context/knowledgeSources/read Read context knowledge sources FoundationaLLM.Context/knowledgeSources/write Create or update context knowledge sources FoundationaLLM.Context/knowledgeSources/delete Delete context knowledge sources FoundationaLLM.Context/knowledgeUnits/read Read context knowledge units FoundationaLLM.Context/knowledgeUnits/write Create or update context knowledge units FoundationaLLM.Context/knowledgeUnits/delete Delete context knowledge units FoundationaLLM.Context/management/write Execute management actions Conversation Actions Action Description FoundationaLLM.Conversation/conversations/read Read conversations FoundationaLLM.Conversation/conversations/write Create or update conversations FoundationaLLM.Conversation/conversations/delete Delete conversations FoundationaLLM.Conversation/management/write Execute management actions Data Pipeline Actions Action Description FoundationaLLM.DataPipeline/dataPipelines/read Read data pipelines FoundationaLLM.DataPipeline/dataPipelines/write Create or update data pipelines FoundationaLLM.DataPipeline/dataPipelines/delete Delete data pipelines FoundationaLLM.DataPipeline/management/write Execute management actions Data Source Actions Action Description FoundationaLLM.DataSource/dataSources/read Read data sources FoundationaLLM.DataSource/dataSources/write Create or update data sources FoundationaLLM.DataSource/dataSources/delete Delete data sources FoundationaLLM.DataSource/management/write Execute management actions Plugin Actions Action Description FoundationaLLM.Plugin/plugins/read Read plugins FoundationaLLM.Plugin/plugins/write Create or update plugins FoundationaLLM.Plugin/plugins/delete Delete plugins FoundationaLLM.Plugin/pluginPackages/read Read plugin packages FoundationaLLM.Plugin/pluginPackages/write Create or update plugin packages FoundationaLLM.Plugin/pluginPackages/delete Delete plugin packages FoundationaLLM.Plugin/management/write Execute management actions Prompt Actions Action Description FoundationaLLM.Prompt/prompts/read Read prompts FoundationaLLM.Prompt/prompts/write Create or update prompts FoundationaLLM.Prompt/prompts/delete Delete prompts FoundationaLLM.Prompt/management/write Execute management actions Vector Actions Action Description FoundationaLLM.Vector/vectorDatabases/read Read vector databases FoundationaLLM.Vector/vectorDatabases/write Create or update vector databases FoundationaLLM.Vector/vectorDatabases/delete Delete vector databases FoundationaLLM.Vector/management/write Execute management actions Vectorization Actions (Legacy) Action Description FoundationaLLM.Vectorization/vectorizationPipelines/read Read vectorization pipelines FoundationaLLM.Vectorization/vectorizationPipelines/write Create or update vectorization pipelines FoundationaLLM.Vectorization/vectorizationPipelines/delete Delete vectorization pipelines FoundationaLLM.Vectorization/vectorizationRequests/read Read vectorization requests FoundationaLLM.Vectorization/vectorizationRequests/write Create or update vectorization requests FoundationaLLM.Vectorization/vectorizationRequests/delete Delete vectorization requests FoundationaLLM.Vectorization/contentSourceProfiles/read Read content source profiles FoundationaLLM.Vectorization/contentSourceProfiles/write Create or update content source profiles FoundationaLLM.Vectorization/contentSourceProfiles/delete Delete content source profiles FoundationaLLM.Vectorization/textPartitioningProfiles/read Read text partitioning profiles FoundationaLLM.Vectorization/textPartitioningProfiles/write Create or update text partitioning profiles FoundationaLLM.Vectorization/textPartitioningProfiles/delete Delete text partitioning profiles FoundationaLLM.Vectorization/textEmbeddingProfiles/read Read text embedding profiles FoundationaLLM.Vectorization/textEmbeddingProfiles/write Create or update text embedding profiles FoundationaLLM.Vectorization/textEmbeddingProfiles/delete Delete text embedding profiles FoundationaLLM.Vectorization/indexingProfiles/read Read indexing profiles FoundationaLLM.Vectorization/indexingProfiles/write Create or update indexing profiles FoundationaLLM.Vectorization/indexingProfiles/delete Delete indexing profiles Scope Hierarchy Permissions are evaluated against a hierarchical scope structure: /instances/{instanceId} ├── /providers/FoundationaLLM.Agent │ ├── /agents/{agentName} │ ├── /workflows/{workflowName} │ └── /tools/{toolName} ├── /providers/FoundationaLLM.AIModel │ └── /aiModels/{modelName} ├── /providers/FoundationaLLM.Authorization │ ├── /roleAssignments/{assignmentId} │ └── /roleDefinitions/{definitionId} ├── /providers/FoundationaLLM.Configuration │ ├── /appConfigurations/{configName} │ └── /apiEndpointConfigurations/{endpointName} ├── /providers/FoundationaLLM.DataPipeline │ └── /dataPipelines/{pipelineName} ├── /providers/FoundationaLLM.DataSource │ └── /dataSources/{dataSourceName} ├── /providers/FoundationaLLM.Plugin │ ├── /plugins/{pluginName} │ └── /pluginPackages/{packageName} ├── /providers/FoundationaLLM.Prompt │ └── /prompts/{promptName} └── /providers/FoundationaLLM.Vector └── /vectorDatabases/{databaseName} Scope Inheritance Permissions assigned at a parent scope are inherited by child scopes Instance-level assignments (/instances/{instanceId}) apply to all resources Provider-level assignments apply to all resources of that type Resource-level assignments apply only to the specific resource Wildcards Actions support wildcards: Pattern Meaning * All actions on all resources */read All read actions */write All write actions */delete All delete actions */management/write All management write actions FoundationaLLM.Authorization/* All authorization actions Related Topics Instance Access Control Resource Management Concepts"
  },
  "docs/outline.html": {
    "href": "docs/outline.html",
    "title": "FoundationaLLM Documentation Outline | FoundationaLLM",
    "summary": "FoundationaLLM Documentation Outline Overview of FoundationaLLM Overview & Introduction (from index.md) Architecture & Concepts Core Concepts (from concepts/index.md) Authorization & Security Principals Agents, Workflows, Tools Data Pipelines Plugins & Plugin Packages Prompts & Prompt Variables Quotas (API Raw Request Rate, Agent Request Rate) Resource Providers Chargeback Why FoundationaLLM? Where FoundationaLLM Fits Chat User Portal Overview What is the Chat User Portal? Finding your User Portal URL Quick Start Quickstart Guide (from setup-guides/quickstart.md) Creating Your First Agent How to Guides Using Agents Selecting an Agent Managing Available Agents Managing Conversations Configuring Accessibility Uploading Files to a Conversation Downloading Files from a Conversation Using the Code Interpreter Tool Using the Knowledge Tool Using other tools Monitoring Token Consumption Rating Agent Responses Copying Prompts & Formatted Results Dual-Format \"Copy Message\" (Markdown + Rich Text) Behavior Expected Formatting Output Rules and Limitations Printing Conversations Viewing Agent Prompts Management Portal Overview What is the Management Portal? Management Portal Guide (from setup-guides/management-ui/management-ui.md) Finding your User Portal and Management Portal URLs Quick Start Tour of the Portal Creating a First Agent How to Guides Agents Create New Agent Create Model Agnostic Agent with Claude (from how-to-guides/create-model-agnostic-agent-claude.md) Create Model Agnostic Agent with GPT-4o (from how-to-guides/create-model-agnostic-agent-gpt4o.md) All Agents My Agents Prompts Data Data Sources Data Pipelines Creating Data Pipelines Invoking Data Pipelines Monitoring Data Pipelines (Progress/Status in Management Portal) Data Pipeline Runs Knowledge Sources Uploading Files from SharePoint Online Azure Data Lake as a Knowledge Source Private Storage for Custom Agent Owners Knowledge Graph Integration Using a Knowledge Graph as a Knowledge Source Required Schema/Format Expectations Image Description Image-to-Text Description Capability Model Limits Supported Formats Usage Guidelines Models and Endpoints AI Models API Endpoints Security Instance Access Control FLLM Platform Branding Configuration Deployment Information Vectorization [DROP THIS ALL TOGETHER] Vectorization Concepts (from setup-guides/vectorization/vectorization-concepts.md) Configuring Vectorization (from setup-guides/vectorization/vectorization-configuration.md) Managing Vectorization Profiles (from setup-guides/vectorization/vectorization-profiles.md) Triggering Vectorization (from setup-guides/vectorization/vectorization-triggering.md) Vectorization Pipelines Reduced Vectorization Latency Improvements Configuration and Tuning Options Monitoring and Troubleshooting Vectorization (from setup-guides/vectorization/vectorization-monitoring-troubleshooting.md) Managing Plugins Configuring Quotas Reference Concepts Agents & Agent Workflows (from setup-guides/agents/agents_workflows.md) Agent Access Tokens (from setup-guides/agents/Agent_AccessToken.md) Prompts & Prompt Resources (from setup-guides/agents/prompt-resource.md) [OBSOLETE] Knowledge Management Agents (from setup-guides/agents/knowledge-management-agent.md) Resource Management (from setup-guides/exposed-apis/resource-management/resource-management.md) Data Pipelines Plugins & Plugin Packages [OBSOLETE] Vectorization Quotas Branding Customization (from setup-guides/branding/index.md) Using App Configuration Using Management Portal Using REST API Configuration Reference [C:\\Repos\\foundationallm\\src\\dotnet\\Common\\Constants\\Data] Permissions & Roles[C:\\Repos\\foundationallm\\src\\dotnet\\Common\\Constants\\Data\\AuthorizableActions.json] [C:\\Repos\\foundationallm\\src\\dotnet\\Common\\Constants\\Data\\RoleDefinitions.json] APIs & SDKs APIs Core API Core API Overview (from setup-guides/exposed-apis/core-api.md) Finding Core API URL Directly Calling Core API (from development/calling-apis/directly-calling-core-api.md) Standard Deployment Local API Access (from development/calling-apis/standard-deployment-local-api-access.md) Core API Reference (from api/index.md - .NET & Python) Management API Management API Overview (from setup-guides/exposed-apis/management-api.md) Overview of Resource Providers [DOCS can be generated from the code C:\\Repos\\foundationallm\\src\\dotnet\\Common\\Constants\\ResourceProviders metadata files] Directly Calling Management API (from development/calling-apis/directly-calling-management-api.md) Management API Reference Data Pipelines [OBSOLETE] Vectorization API Directly Calling Vectorization API (from development/calling-apis/directly-calling-vectorization-api.md) SDKs .NET SDK (from api/dotnet/index.md) Python SDK (from api/python/index.md) Platform Operations Deployment Deployment Overview (from deployment/index.md) Quick Start Deployment (from deployment/deployment-quick-start.md) Standard Deployment (from deployment/deployment-standard.md) Deployment Configuration (from deployment/deployment-configuration.md) App Configuration Values (from deployment/app-configuration-values.md) Azure Resource Provider Requirements (from deployment/azure-resource-providers-requirements.md) Custom Domains (from deployment/custom-domains.md) Soft Delete (from deployment/soft-delete.md) Standard Deployment Manifest (from deployment/standard/manifest.md) Security & Permissions Platform Security Features & Best Practices (from operations/security.md) Authentication & Authorization Setup (from deployment/authentication-authorization/index.md) Pre-deployment Setup Core Authentication Setup (Entra ID) Management Authentication Setup (Entra ID) Authorization Setup (Entra ID) Post-deployment Setup Core Authentication Post-deployment Management Authentication Post-deployment Authorization Post-deployment Role-Based Access Control (from role-based-access-control/index.md) Role Definitions Role Assignments Scope Role Management Agent Role Assignments Configure Access Control for Services (from deployment/configure-access-control-for-services.md) Graph API Permissions (from operations/graph-api-permissions.md) Network Security Groups (from operations/network-security-groups.md) Vulnerabilities: Identification, Communication, and Remediation (from operations/vulnerabilities.md) Monitoring & Troubleshooting Accessing System Logs & Audit Trails (from operations/logs.md) Troubleshooting & Issue Reporting Guide (from operations/troubleshooting.md) How to Guides Updating Container Versions (from operations/update.md) Backups & Data Resiliency (from operations/backups.md) Purging User Conversations (from operations/purge-conversations.md) Creating Release Notes (from operations/release-notes.md) Additional Topics (to integrate or keep separate) Development & Contributing Development Overview (from development/index.md) Development Approach & DevOps Guidelines (from development/development-approach.md) Configure Local Development Environment (from development/development-local.md) Contributing to FoundationaLLM (from development/contributing/index.md) Git Workflow Style Guide Bug Report Reproduction Guide Reference Materials (Root Level) Agent Schemas (schema.md - at docs root) Documentation Generation (documentation-generation.md - at docs root) Release Notes (Root Level) Release Notes (from release-notes/) Breaking Changes Version-specific Release Notes Note: Release notes are kept at the root level for easy discoverability."
  },
  "docs/overview/architecture-concepts.html": {
    "href": "docs/overview/architecture-concepts.html",
    "title": "Architecture & Core Concepts | FoundationaLLM",
    "summary": "Architecture & Core Concepts This page provides a high-level overview of the core FoundationaLLM architecture and concepts. Core Concepts Overview The following mindmap provides a high-level overview of the core FoundationaLLM concepts. mindmap root((FoundationaLLM)) :::link:https://docs.foundationallm.ai (Authorization) Secret Key Role Based Access Control Policy Based Access Control Security Principal Virtual Security Principal Virtual User Virtual Security Group (Agent) Agent Workflow Agent Tool Agent Access Token Agent Access Token Virtual User Agent Virtual Security Group (Chargeback) (Data Pipeline) (Plugin) Plugin Package Plugin (Prompt) (Quota) Quota Definition API Raw Request Rate Agent Request Rate (Resource Provider) (Tool) (Workflow) Workflow Host Key Concepts Authorization & Security Principals FoundationaLLM uses a comprehensive authorization model that includes: Secret Key: Unique strings for agent authentication Role Based Access Control (RBAC): Access restriction based on user roles Policy Based Access Control (PBAC): Access restriction based on policies Security Principals: Entities that can be authenticated (users, groups, service identities) Virtual Security Principals: Dynamically created principals for agents and access tokens Agents, Workflows, and Tools Agent: The core interaction unit that provides users with customized experiences Agent Workflow: Drives core agent interactions with associated tools Agent Tool: Tools associated with agents for executing code, interacting with APIs, or processing data Agent Access Token: Enables agent authentication without Entra ID credentials Data Pipelines Data pipelines define processes for extracting, transforming, and loading data for LLM-based workloads. Key features include: Scalability for enterprise workloads Flexible and modular structure Local and global processing support Runtime parameterization Extensibility through plugins Plugins & Plugin Packages Plugins extend FoundationaLLM functionality: Plugin Package: Unit of versioning and deployment for plugins Plugin Types: Agent Workflow, Agent Tool, Data Source, Data Pipeline Stage, Content Text Extraction, Content Text Partitioning Prompts & Prompt Variables Prompts are text templates for creating agent completion requests, used in: Agent Workflows (for tool invocation decisions) Agent Tools (for additional context and instructions) Quotas Quotas define resource usage limits: API Raw Request Rate: Maximum API requests per time period Agent Request Rate: Maximum agent completion calls per time period Resource Providers Platform components managing resources like agents, prompts, data sources, and pipelines. Chargeback Mechanism for tracking and billing resource usage by cost center-enabled entities. For detailed information on each concept, see the Concepts Reference."
  },
  "docs/overview/index.html": {
    "href": "docs/overview/index.html",
    "title": "FoundationaLLM: Deploy secure, extensible AI agents inside your own cloud. Designed for scale, built for control, and ready to work. | FoundationaLLM",
    "summary": "FoundationaLLM: Deploy secure, extensible AI agents inside your own cloud. Designed for scale, built for control, and ready to work. FoundationaLLM simplifies and streamlines building knowledge management (e.g., question/answer agents) and analytic (e.g., self-service business intelligence) copilots over the data sources present across your enterprise. FoundationaLLM deploys a secure, comprehensive and highly configurable copilot platform to your Azure cloud environment: Simplifies integration with enterprise data sources used by agent for in-context learning (e.g., enabling RAG, CoT, ReAct and inner monologue patterns). Provides defense in depth with fine-grain security controls over data used by agent and pre/post completion filters that guard against attack. Hardened solution attacked by an LLM red team from inception. Scalable solution load balances across multiple LLM endpoints. Extensible to new data sources, new LLM orchestrators and LLMs. Reality What it really takes to create a secure, well governed, scalable and extensible enterprise copilot solution: Where does FoundationaLLM stack against the other copilot solutions? What do WE mean by \"copilot\" It's a rapidly evolving AI world out there, so let's level set on what we mean when we say copilot as this is concept core to FoundationaLLM. At its most basic, a copilot uses enterprise supplied knowledge and generative AI models to author text, write code or render images, often by reasoning over human supplied prompts. Across these modalities, the AI is used to assist a human directly with a specific task. That's what makes it a copilot. This basic capability emerges in copilots which power these scenarios: Knowledge Management: Help users quickly find the information they seek and deliver at the right level and in the right format. Examples include summarization, rephrasing or retargeting to address a persona (e.g., explain it like I'm five), sentiment analysis and recommendations. Analytics: Help users quickly get to the data driven insights they seek. Examples include recommendations, predictions, anomaly detection, statistical analysis and data querying and reporting. Why is FoundationaLLM Needed? Simply put we saw lot of folks reinventing the wheel just to get a customized copilot that was grounded and bases its responses in their own data as opposed to the trained parametric knowledge of the model. Many of the solutions we saw made for great demos, but were effectively toys wrapping calls to OpenAI endpoints- they were not something intended or ready to take into production. We built FoundationaLLM to provide a continous journey, one that was quick to get started with so folks could experiment quickly with LLM's but not fall off a cliff after that with a solution that would be insecure, unlicensed, inflexible and not fully featured enough to grow from the prototype into a production solution without having to start all over. The core problems to deliver enterprise copilots are: Enterprise grade copilots are complex and have lots of moving parts (not to mention infrastructure). The industry has a skills gap when it comes to filling the roles needed to deliver these complex copilot solutions. The top AI risks (inaccuracy, cybersecurity, compliance, explainability, privacy) are not being mitigated. Delivery of a copilot solution is time consuming, expensive and frustrating. Where can FoundationaLLM fill the need? Documentation Get up to speed with FoundationaLLM by reading the documentation. This includes deployment instructions, quickstarts, architecture, and API references. Getting Started FoundationalLLM provides a simple command line driven approach to getting your first deployment up and running. Basically, it's two commands. After that, you can customize the solution, run it locally on your machine and update the deployment with your customizations. Follow the Quick Start Deployment instructions to get FoundationaLLM deployed in your Azure subscription. If you want to run the solution locally, follow the Local Development instructions. You can browse the documentation using the sidebar or visit the API section for the reference documentation."
  },
  "docs/overview/why-foundationallm.html": {
    "href": "docs/overview/why-foundationallm.html",
    "title": "Why FoundationaLLM? | FoundationaLLM",
    "summary": "Why FoundationaLLM? Why is FoundationaLLM Needed? Simply put we saw lot of folks reinventing the wheel just to get a customized copilot that was grounded and bases its responses in their own data as opposed to the trained parametric knowledge of the model. Many of the solutions we saw made for great demos, but were effectively toys wrapping calls to OpenAI endpoints- they were not something intended or ready to take into production. We built FoundationaLLM to provide a continous journey, one that was quick to get started with so folks could experiment quickly with LLM's but not fall off a cliff after that with a solution that would be insecure, unlicensed, inflexible and not fully featured enough to grow from the prototype into a production solution without having to start all over. The core problems to deliver enterprise copilots are: Enterprise grade copilots are complex and have lots of moving parts (not to mention infrastructure). The industry has a skills gap when it comes to filling the roles needed to deliver these complex copilot solutions. The top AI risks (inaccuracy, cybersecurity, compliance, explainability, privacy) are not being mitigated. Delivery of a copilot solution is time consuming, expensive and frustrating. Where can FoundationaLLM fill the need? What do WE mean by \"copilot\" It's a rapidly evolving AI world out there, so let's level set on what we mean when we say copilot as this is concept core to FoundationaLLM. At its most basic, a copilot uses enterprise supplied knowledge and generative AI models to author text, write code or render images, often by reasoning over human supplied prompts. Across these modalities, the AI is used to assist a human directly with a specific task. That's what makes it a copilot. This basic capability emerges in copilots which power these scenarios: Knowledge Management: Help users quickly find the information they seek and deliver at the right level and in the right format. Examples include summarization, rephrasing or retargeting to address a persona (e.g., explain it like I'm five), sentiment analysis and recommendations. Analytics: Help users quickly get to the data driven insights they seek. Examples include recommendations, predictions, anomaly detection, statistical analysis and data querying and reporting. Where does FoundationaLLM stack against the other copilot solutions? Reality What it really takes to create a secure, well governed, scalable and extensible enterprise copilot solution:"
  },
  "docs/platform-operations/deployment/app-configuration-values.html": {
    "href": "docs/platform-operations/deployment/app-configuration-values.html",
    "title": "App Configuration Values | FoundationaLLM",
    "summary": "App Configuration Values FoundationaLLM uses Azure App Configuration as the central store for configuration values, Key Vault secret references, and feature flags. Overview Benefits of centralized configuration: Single source of truth for all services No redeployment required for configuration changes Key Vault integration for secrets Shared settings across multiple services Configuration Categories Instance Configuration Key Default Description FoundationaLLM:Instance:Id Generated GUID Unique instance identifier FoundationaLLM:Configuration:KeyVaultURI (Required) Key Vault URL for secrets API Endpoints Key Description FoundationaLLM:APIs:CoreAPI:APIUrl Core API base URL FoundationaLLM:APIs:ManagementAPI:APIUrl Management API base URL FoundationaLLM:APIs:OrchestrationAPI:APIUrl Orchestration API URL FoundationaLLM:APIs:GatekeeperAPI:APIUrl Gatekeeper API URL FoundationaLLM:APIs:LangChainAPI:APIUrl LangChain API URL FoundationaLLM:APIs:SemanticKernelAPI:APIUrl Semantic Kernel API URL API Keys (Key Vault References) Key Key Vault Secret FoundationaLLM:APIs:OrchestrationAPI:APIKey foundationallm-apis-orchestrationapi-apikey FoundationaLLM:APIs:GatekeeperAPI:APIKey foundationallm-apis-gatekeeperapi-apikey FoundationaLLM:APIs:LangChainAPI:APIKey foundationallm-apis-langchainapi-apikey FoundationaLLM:APIs:SemanticKernelAPI:APIKey foundationallm-apis-semantickernelapi-apikey Gatekeeper Configuration Key Default Description FoundationaLLM:APIs:CoreAPI:BypassGatekeeper true Skip content filtering for performance FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableAzureContentSafety true Enable Azure Content Safety FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableLakeraGuard true Enable Lakera Guard FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableMicrosoftPresidio true Enable PII detection Azure Content Safety Key Default Description FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:APIUrl (Required) Content Safety endpoint FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:HateSeverity 2 Threshold (0-6) FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SelfHarmSeverity 2 Threshold (0-6) FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SexualSeverity 2 Threshold (0-6) FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:ViolenceSeverity 2 Threshold (0-6) Azure OpenAI Configuration Key Default Description FoundationaLLM:AzureOpenAI:API:Endpoint (Required) Azure OpenAI endpoint FoundationaLLM:AzureOpenAI:API:Version 2023-05-15 API version FoundationaLLM:AzureOpenAI:API:Completions:DeploymentName completions Completion model deployment FoundationaLLM:AzureOpenAI:API:Completions:ModelName gpt-35-turbo Model name FoundationaLLM:AzureOpenAI:API:Completions:MaxTokens 8096 Max tokens FoundationaLLM:AzureOpenAI:API:Completions:Temperature 0 Temperature FoundationaLLM:AzureOpenAI:API:Embeddings:DeploymentName embeddings Embedding deployment FoundationaLLM:AzureOpenAI:API:Embeddings:ModelName text-embedding-ada-002 Embedding model Branding Configuration Key Default Description FoundationaLLM:Branding:CompanyName FoundationaLLM Organization name FoundationaLLM:Branding:PageTitle FoundationaLLM Chat Copilot Browser title FoundationaLLM:Branding:LogoUrl foundationallm-logo-white.svg Logo path FoundationaLLM:Branding:FavIconUrl favicon.ico Favicon path FoundationaLLM:Branding:PrimaryColor #131833 Primary color FoundationaLLM:Branding:PrimaryTextColor #fff Primary text color FoundationaLLM:Branding:SecondaryColor #334581 Secondary color FoundationaLLM:Branding:BackgroundColor #fff Background color FoundationaLLM:Branding:KioskMode false Enable kiosk mode Entra ID Authentication Core API / Chat Portal Key Description FoundationaLLM:CoreAPI:Entra:Instance Entra login URL FoundationaLLM:CoreAPI:Entra:TenantId Tenant ID FoundationaLLM:CoreAPI:Entra:ClientId Client ID FoundationaLLM:CoreAPI:Entra:Scopes API scopes FoundationaLLM:CoreAPI:Entra:CallbackPath /signin-oidc Management API / Portal Key Description FoundationaLLM:ManagementAPI:Entra:Instance Entra login URL FoundationaLLM:ManagementAPI:Entra:TenantId Tenant ID FoundationaLLM:ManagementAPI:Entra:ClientId Client ID FoundationaLLM:ManagementAPI:Entra:Scopes API scopes CosmosDB Configuration Key Default Description FoundationaLLM:CosmosDB:Endpoint (Required) CosmosDB endpoint FoundationaLLM:CosmosDB:Database database Database name FoundationaLLM:CosmosDB:Containers Sessions, UserSessions Container list FoundationaLLM:CosmosDB:ChangeFeedLeaseContainer leases Change feed container Storage Configuration Key Description FoundationaLLM:AgentHub:AgentMetadata:StorageContainer Agent metadata container FoundationaLLM:PromptHub:PromptMetadata:StorageContainer Prompt metadata container FoundationaLLM:DataSourceHub:DataSourceMetadata:StorageContainer Data source container FoundationaLLM:BlobStorageMemorySource:BlobStorageContainer Memory source container Vectorization Configuration Key Description FoundationaLLM:APIs:VectorizationAPI:APIUrl Vectorization API URL FoundationaLLM:Vectorization:AzureAISearchIndexingService:Endpoint AI Search endpoint FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint Embedding endpoint FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName embeddings Event Grid Configuration Key Description FoundationaLLM:Events:AzureEventGridEventService:Endpoint Event Grid endpoint FoundationaLLM:Events:AzureEventGridEventService:NamespaceId Event Grid namespace FoundationaLLM:Events:AzureEventGridEventService:AuthenticationType APIKey Key Vault Secret References Many App Configuration values reference Key Vault secrets. The format is: { \"uri\": \"https://{keyvault-name}.vault.azure.net/secrets/{secret-name}\" } Common secret references: App Config Key Key Vault Secret FoundationaLLM:AzureOpenAI:API:Key foundationallm-azureopenai-api-key FoundationaLLM:CoreAPI:Entra:ClientSecret foundationallm-coreapi-entra-clientsecret FoundationaLLM:ManagementAPI:Entra:ClientSecret foundationallm-managementapi-entra-clientsecret Modifying Configuration Via Azure Portal Navigate to your App Configuration resource Select Configuration explorer Find and edit the key Save changes Via Azure CLI az appconfig kv set \\ --name <app-config-name> \\ --key \"FoundationaLLM:Branding:CompanyName\" \\ --value \"My Company\" Via Management Portal Some settings (like branding) can be modified through the Management Portal UI. Related Topics Deployment Configuration Authentication Setup Branding Configuration"
  },
  "docs/platform-operations/deployment/azure-resource-providers-requirements.html": {
    "href": "docs/platform-operations/deployment/azure-resource-providers-requirements.html",
    "title": "Azure Resource Provider Requirements | FoundationaLLM",
    "summary": "Azure Resource Provider Requirements FoundationaLLM requires specific Azure Resource Providers to be registered in your subscription before deployment. Overview Azure Resource Providers enable specific Azure services. If a required provider is not registered, deployment will fail with a \"resource provider not registered\" error. Quick Start Deployment Requirements The following providers are required for Quick Start (ACA) deployments: Resource Provider Service microsoft.alertsmanagement/smartDetectorAlertRules Smart detection alerts Microsoft.App/containerApps Container Apps Microsoft.App/managedEnvironments Container Apps environments Microsoft.AppConfiguration/configurationStores App Configuration Microsoft.CognitiveServices/accounts Azure OpenAI, Content Safety Microsoft.DocumentDB/databaseAccounts Cosmos DB Microsoft.EventGrid/namespaces Event Grid Microsoft.EventGrid/systemTopics Event Grid system topics Microsoft.Insights/components Application Insights Microsoft.KeyVault/vaults Key Vault Microsoft.ManagedIdentity/userAssignedIdentities Managed identities Microsoft.OperationalInsights/workspaces Log Analytics Microsoft.Portal/dashboards Azure dashboards Microsoft.Search/searchServices Azure AI Search Microsoft.Storage/storageAccounts Storage accounts Standard Deployment Requirements The Standard (AKS) deployment requires additional providers: Resource Provider Service microsoft.alertsmanagement/smartDetectorAlertRules Smart detection alerts Microsoft.AppConfiguration/configurationStores App Configuration Microsoft.CognitiveServices/accounts Azure OpenAI, Content Safety Microsoft.Compute/virtualMachineScaleSets VM scale sets (AKS nodes) Microsoft.ContainerService/managedClusters AKS clusters Microsoft.DocumentDB/databaseAccounts Cosmos DB Microsoft.EventGrid/namespaces Event Grid Microsoft.EventGrid/systemTopics Event Grid system topics Microsoft.Insights/actiongroups Alert action groups Microsoft.Insights/components Application Insights Microsoft.Insights/metricalerts Metric alerts microsoft.insights/privateLinkScopes Private link for monitoring Microsoft.Insights/scheduledqueryrules Log query alerts Microsoft.KeyVault/vaults Key Vault Microsoft.ManagedIdentity/userAssignedIdentities Managed identities Microsoft.Network/loadBalancers Load balancers Microsoft.Network/networkInterfaces Network interfaces Microsoft.Network/networkSecurityGroups NSGs Microsoft.Network/privateEndpoints Private endpoints Microsoft.Network/publicIPAddresses Public IPs Microsoft.Network/virtualNetworks Virtual networks Microsoft.OperationalInsights/workspaces Log Analytics Microsoft.OperationsManagement/solutions Management solutions Microsoft.Search/searchServices Azure AI Search Microsoft.Storage/storageAccounts Storage accounts Checking Provider Registration Azure Portal Navigate to your subscription Select Settings > Resource providers Search for the provider name Check the Status column Azure CLI # List all providers and their status az provider list --output table # Check specific provider az provider show --namespace Microsoft.App --query \"registrationState\" PowerShell # List all providers Get-AzResourceProvider | Select-Object ProviderNamespace, RegistrationState # Check specific provider (Get-AzResourceProvider -ProviderNamespace Microsoft.App).RegistrationState Registering Providers Azure Portal Navigate to Subscription > Resource providers Search for the provider Select the provider Click Register Azure CLI # Register a provider az provider register --namespace Microsoft.App # Wait for registration (may take several minutes) az provider show --namespace Microsoft.App --query \"registrationState\" --output tsv PowerShell # Register a provider Register-AzResourceProvider -ProviderNamespace Microsoft.App # Check status (Get-AzResourceProvider -ProviderNamespace Microsoft.App).RegistrationState Bulk Registration Script Register all required providers for Standard deployment: $providers = @( \"Microsoft.App\", \"Microsoft.AppConfiguration\", \"Microsoft.CognitiveServices\", \"Microsoft.Compute\", \"Microsoft.ContainerService\", \"Microsoft.DocumentDB\", \"Microsoft.EventGrid\", \"Microsoft.Insights\", \"Microsoft.KeyVault\", \"Microsoft.ManagedIdentity\", \"Microsoft.Network\", \"Microsoft.OperationalInsights\", \"Microsoft.OperationsManagement\", \"Microsoft.Search\", \"Microsoft.Storage\" ) foreach ($provider in $providers) { Write-Host \"Registering $provider...\" Register-AzResourceProvider -ProviderNamespace $provider } # Wait and verify Start-Sleep -Seconds 30 foreach ($provider in $providers) { $status = (Get-AzResourceProvider -ProviderNamespace $provider).RegistrationState Write-Host \"$provider : $status\" } Troubleshooting Issue Solution \"Resource provider not registered\" Register the provider shown in error Registration stuck in \"Registering\" Wait up to 10 minutes, then retry \"Authorization failed\" Ensure you have Contributor role on subscription Provider not available Check if service is available in your region Required Permissions To register resource providers, you need: Contributor or Owner role on the subscription, OR A custom role with Microsoft.*/register/action permission Related Topics Quick Start Deployment Standard Deployment Troubleshooting"
  },
  "docs/platform-operations/deployment/custom-domains.html": {
    "href": "docs/platform-operations/deployment/custom-domains.html",
    "title": "Custom Domains | FoundationaLLM",
    "summary": "Custom Domains Configure custom domain names for FoundationaLLM portals and APIs to match your organization's branding. Overview Both Azure Container Apps (ACA) and Azure Kubernetes Service (AKS) deployments support custom domains with SSL certificates. Deployment Type Certificate Management Quick Start (ACA) Azure managed or custom Standard (AKS) Custom certificates required Azure Container Apps (Quick Start) Adding a Custom Domain Open Azure Portal Navigate to your subscription and resource group Select Container App Choose the ACA instance (e.g., chat or management) Configure Domain Select Settings > Custom domains Click Add custom domain Enter your domain name Configure DNS Note the DNS records displayed in the dialog: CNAME record pointing to your Container App TXT record for domain verification Validate Domain Add the required DNS records to your DNS provider Return to Azure Portal and click Validate Add Certificate Choose Managed certificate (free, auto-renewed) OR Upload a custom certificate Click Add Certificate Options Option Description Use Case Managed Certificate Free Azure-managed SSL Most deployments Custom Certificate Upload your own PFX Enterprise requirements Note: Managed certificates may take a few minutes to provision. For detailed instructions, see Custom domain names and certificates in Azure Container Apps. Azure Kubernetes Service (Standard) Pre-Deployment SSL Setup For Standard deployments, SSL certificates must be provisioned before deployment. Obtain Certificates Acquire SSL certificates for each domain Export to PFX format with private key Place Certificate Files deploy/standard/certs/ ├── api.example.com.pfx ├── management-api.example.com.pfx ├── chat.example.com.pfx └── management.example.com.pfx Configure Deployment Manifest Update Deployment-Manifest.json with hostnames Deploy Certificates are automatically configured during deployment Certificate Requirements Service Hostname Example File Name Core API api.example.com api.example.com.pfx Management API management-api.example.com management-api.example.com.pfx Chat Portal chat.example.com chat.example.com.pfx Management Portal management.example.com management.example.com.pfx For detailed instructions, see Set up a custom domain name and SSL certificate with the application routing add-on. Update Entra ID Redirect URIs After configuring custom domains, update the App Registration redirect URIs. Chat Portal (User Portal) Navigate to Azure Portal > Microsoft Entra ID Select App registrations Search for and select the Chat UI app registration Select Manage > Authentication In Single-page application Redirect URIs, add: https://your-custom-domain.com/signin-oidc Click Save Management Portal Navigate to App registrations Search for and select the Management UI app registration Select Manage > Authentication In Single-page application Redirect URIs, add: https://your-custom-domain.com/management/signin-oidc Click Save Using the Update Script For Quick Start deployments, run: cd deploy/quick-start ../common/scripts/Update-OAuthCallbackUris.ps1 DNS Configuration Required DNS Records Record Type Name Value CNAME chat <aca-fqdn> or <aks-ingress-ip> CNAME management <aca-fqdn> or <aks-ingress-ip> CNAME api <aca-fqdn> or <aks-ingress-ip> A (AKS only) Various Ingress IP address Private DNS (Standard Deployment) For Standard deployments with private networking: Deployment generates hosts file in deploy/standard/config/ Add entries to: Local hosts file for testing, OR Organization's private DNS server Troubleshooting Issue Solution Certificate not trusted Ensure certificate chain is complete DNS validation failed Check DNS propagation (allow 24-48 hours) Redirect loop Verify redirect URIs in App Registration Mixed content warnings Ensure all resources use HTTPS Best Practices Use Managed Certificates (ACA) when possible for automatic renewal Plan DNS Changes in advance - propagation takes time Test in Staging before updating production Monitor Certificate Expiry for custom certificates Document Custom Domains for team reference Related Topics Quick Start Deployment Standard Deployment Authentication Setup Deployment Manifest"
  },
  "docs/platform-operations/deployment/deployment-configuration.html": {
    "href": "docs/platform-operations/deployment/deployment-configuration.html",
    "title": "Deployment Configuration Reference | FoundationaLLM",
    "summary": "Deployment Configuration Reference This document provides a reference for environment variables and configuration settings used during FoundationaLLM deployment. Environment Variable Reference Note: These settings are primarily used during initial deployment. For runtime configuration, see App Configuration Values. Core API Configuration Variable Type Description foundationallm-core-api-url URL Core API endpoint foundationallm-core-api-keyvault-name String Key Vault name foundationallm-core-api-entra-instance URL Entra login URL (default: https://login.microsoftonline.com/) foundationallm-core-api-entra-tenant-id GUID Azure AD tenant ID foundationallm-core-api-entra-client-id GUID App registration client ID foundationallm-core-api-entra-callback-path Path OAuth callback (default: /signin-oidc) foundationallm-core-api-entra-scopes String Required scopes foundationallm-core-api-gatekeeper-api-url URL Gatekeeper API endpoint Gatekeeper API Configuration Variable Type Description foundationallm-gatekeeper-api-keyvault-name String Key Vault name foundationallm-gatekeeper-api-key Secret API key (Key Vault) foundationallm-gatekeeper-api-orchestration-api-url URL Orchestration API endpoint Orchestration API Configuration Variable Type Description foundationallm-orchestration-api-keyvault-name String Key Vault name foundationallm-orchestration-api-key Secret API key (Key Vault) foundationallm-orchestration-api-agenthub-api-url URL Agent Hub API endpoint foundationallm-orchestration-api-prompthub-api-url URL Prompt Hub API endpoint foundationallm-orchestration-api-datasourcehub-api-url URL Data Source Hub API endpoint foundationallm-orchestration-api-langchain-api-url URL LangChain API endpoint foundationallm-orchestration-api-semantickernel-api-url URL Semantic Kernel API endpoint Hub API Keys Variable Type Description foundationallm-agenthub-api-key Secret Agent Hub API key foundationallm-prompthub-api-key Secret Prompt Hub API key foundationallm-datasourcehub-api-key Secret Data Source Hub API key foundationallm-langchain-api-key Secret LangChain API key Azure OpenAI Configuration Variable Default Description foundationallm-azure-openai-api-url (Required) Azure OpenAI endpoint foundationallm-azure-openai-api-key (Secret) API key foundationallm-azure-openai-api-completions-deployment (Required) Completion deployment name foundationallm-azure-openai-api-completions-model-version (Required) Model version foundationallm-azure-openai-api-version (Required) API version LangChain Configuration Variable Default Description foundationallm-langchain-summary-model-name gpt-35-turbo Summary model foundationallm-langchain-summary-max-tokens 4097 Max input tokens SQL Database Configuration (Optional) Variable Description foundationallm-langchain-sqldb-testdb-server-name SQL Server name foundationallm-langchain-sqldb-testdb-database-name Database name foundationallm-langchain-sqldb-testdb-username Username foundationallm-langchain-sqldb-testdb-database-password Password (Key Vault) Storage Configuration Variable Description foundationallm-keyvault-name Key Vault name for SDK foundationallm-prompt-metadata-storage-container Prompt storage container foundationallm-datasource-metadata-storage-container Data source container foundationallm-agent-metadata-storage-container Agent metadata container Python SDK Configuration Variable Description foundationallm-configuration-allow-environment-variables When True, checks environment before Key Vault Temporary Configuration Note: The following settings are temporary and will be removed in future versions. Variable Type Description foundationallm-langchain-csv-file-url URL CSV file URL with SAS token Configuration Sources FoundationaLLM uses multiple configuration sources in this priority order: Environment Variables (highest priority) Azure App Configuration Key Vault Secrets Default Values (lowest priority) Best Practices Security Store sensitive values in Key Vault Use managed identities where possible Rotate API keys regularly Avoid storing secrets in environment variables Organization Use consistent naming conventions Document custom configurations Use separate App Configuration instances for dev/test/prod Enable versioning for audit trails Troubleshooting If configuration values aren't being read: Verify App Configuration connection string Check managed identity permissions Verify Key Vault access policies Review service logs for configuration errors Related Topics App Configuration Values Authentication Setup Troubleshooting"
  },
  "docs/platform-operations/deployment/deployment-quick-start.html": {
    "href": "docs/platform-operations/deployment/deployment-quick-start.html",
    "title": "Quick Start Deployment | FoundationaLLM",
    "summary": "Quick Start Deployment The Quick Start deployment uses Azure Container Apps (ACA) for rapid deployment and streamlined development. This option is ideal for proof-of-concept, development, and smaller production workloads. Overview Aspect Details Infrastructure Azure Container Apps (ACA) Deployment Time ~30 minutes Scalability Auto-scaling built-in Networking Public endpoints SSL Managed certificates Prerequisites Azure Requirements Requirement Details Azure Subscription Active subscription with billing enabled Azure OpenAI Access Request access here VM Quota Minimum 65 vCPUs across all VM families Role Assignment Owner on target subscription Entra ID Requirements Requirement Details App Registrations 6 registrations required (see Authentication Setup) Role Assignment Owner on app registrations Required Tools Tool Version Installation Azure Developer CLI v1.6.1+ Install azd Azure CLI v2.51.0+ Install Azure CLI PowerShell 7.4.1+ Install PowerShell Git Latest Install Git Optional Tools Tool Purpose .NET 8 SDK Local debugging Visual Studio 2022 Development Docker Desktop Container testing Deployment Steps Step 1: Pre-Deployment Setup Complete the Authentication Setup to create the required Entra ID app registrations. Important: App registrations must be created before deployment. You will update some values after deployment. Step 2: Clone Repository git clone https://github.com/foundationallm/foundationallm.git cd foundationallm git checkout release/0.8.3 Note: Use the latest release branch for production deployments. The main branch is for development. Step 3: Install Deployment Utilities cd deploy/common/scripts ./Get-AzCopy.ps1 Step 4: Authenticate cd deploy/quick-start # Azure CLI az login # Azure Developer CLI azd auth login # AzCopy ../common/tools/azcopy/azcopy login Step 5: Configure Environment azd env new --location <azure-region> --subscription <subscription-id> Supported regions: Choose a region that supports Azure OpenAI and required models. See Azure OpenAI regional availability. Step 6: Configure Authentication Settings ../common/scripts/Set-AzdEnvEntra.ps1 This script prompts for your Entra ID app registration values. Step 7: (Optional) Use Existing Azure OpenAI If you have an existing Azure OpenAI instance: azd env set OPENAI_NAME <openai-name> azd env set OPENAI_RESOURCE_GROUP <resource-group> azd env set OPENAI_SUBSCRIPTION_ID <subscription-id> Important: Ensure relevant Managed Identities (LangChain API, Semantic Kernel API, Gateway API) have OpenAI Reader role on the Azure OpenAI resource. Step 8: Deploy azd up This command: Provisions Azure infrastructure Updates App Configuration entries Deploys API and web services Imports default files to storage Post-Deployment Configuration Configure MS Graph Permissions Run this script to grant MS Graph access: cd deploy/quick-start ../common/scripts/Set-FllmGraphRoles.ps1 -resourceGroupName rg-<azd-env-name> Requirement: User must be Global Administrator or have Privileged Role Administrator role. Update OAuth Callback URIs ../common/scripts/Update-OAuthCallbackUris.ps1 Verify Deployment Navigate to the Chat Portal URL (from deployment output) Log in with an authorized user Send a test message to verify agent response Teardown To remove all deployed resources: azd down --purge Important: Use --purge to permanently delete soft-deleted resources (Azure OpenAI, Key Vault, AI Search, Content Safety). Troubleshooting Issue Solution Quota exceeded Request quota increase Authentication errors Verify app registration settings Deployment fails Check azd logs for specific errors Container crashes Review container logs in Azure Portal Related Topics Standard Deployment Authentication Setup App Configuration Values Troubleshooting"
  },
  "docs/platform-operations/deployment/deployment-standard.html": {
    "href": "docs/platform-operations/deployment/deployment-standard.html",
    "title": "Standard Deployment with AKS | FoundationaLLM",
    "summary": "Standard Deployment with AKS The Standard deployment uses Azure Kubernetes Service (AKS) for production environments requiring advanced orchestration, scalability, and enterprise-grade security features. Overview Aspect Details Infrastructure Azure Kubernetes Service (AKS) Deployment Time ~2 hours Scalability Advanced Kubernetes orchestration Networking Private endpoints, VNet peering SSL Custom certificates required Architecture The Standard deployment creates: Two AKS clusters: Frontend (portals) and Backend (APIs) Private networking: VNet with subnets, private endpoints Multiple resource groups: Organized by workload type Full Azure monitoring: Log Analytics, Application Insights Prerequisites Azure Requirements Requirement Details Azure Subscription Dedicated subscription recommended Azure OpenAI Access Request access VM Quota ~64 vCPUs minimum (varies by node SKU) Service Principal Requirements Role Scope Directory Reader Azure Tenant Owner Target resource groups (or subscription if RGs don't exist) DNS Zone Contributor DNS resource group Network Contributor Hub VNet resource Entra ID Requirements Requirement Details AD Groups FLLM-Admins, FLLM-Users (pre-created) App Registrations 6 registrations (see Authentication Setup) Required Tools Tool Purpose Azure CLI v2.51.0+ Azure management PowerShell 7.4.1+ Deployment scripts Git Repository access Helm Kubernetes deployments kubectl Cluster management kubelogin AKS authentication Pre-Deployment Information Gather the following before starting: General Settings Setting Description Example Project ID 3-8 character identifier fllm Location Azure region eastus2 Instance ID Unique GUID 5d40d2ee-... Admin Group AD Group Object ID 995a549b-... Network Configuration Setting Default Description VNet CIDR 10.220.128.0/20 Must be /20 netmask AKS Service CIDR 10.100.0.0/16 Kubernetes services Allowed External CIDRs 192.168.100.0/24 Access whitelist AKS Node Pools Pool Default SKU Nodes Backend User Standard_D8_v5 1-3 Backend System Standard_D2_v5 1-3 Frontend User Standard_D2_v5 1-3 Frontend System Standard_D2_v5 1-3 SSL Certificates Provision certificates in PFX format for: Service Hostname Example Core API api.example.com Management API management-api.example.com Chat UI chat.example.com Management UI management.example.com Deployment Steps Step 1: Clone Repository git clone https://github.com/foundationallm/foundationallm.git cd foundationallm git checkout release/0.9.6 Step 2: Configure Environment azd env new --location <azure-region> --subscription <subscription-id> Step 3: Prepare SSL Certificates Place PFX files in deploy/standard/certs/: deploy/standard/certs/ ├── api.example.com.pfx ├── management-api.example.com.pfx ├── chat.example.com.pfx └── management.example.com.pfx Step 4: Create Deployment Manifest Copy and configure the manifest: cp deploy/standard/Deployment-Manifest.template.json deploy/standard/Deployment-Manifest.json Edit Deployment-Manifest.json with your settings. See Deployment Manifest Setup for details. Step 5: Provision Infrastructure cd deploy/standard azd provision The script generates hosts file in config/ folder with private endpoint IPs. Step 6: Configure Network Access Ensure network connectivity to deployed resources: Update local hosts file, OR Configure organization DNS Step 7: Deploy Applications azd deploy This deploys to both clusters and generates hosts.ingress with service endpoints. Step 8: Post-Deployment Configuration Configure MS Graph Permissions: ../common/scripts/Set-FllmGraphRoles.ps1 -resourceGroupName <app-resource-group> Update OAuth Callback URIs: ../common/scripts/Update-OAuthCallbackUris.ps1 Step 9: Verify Deployment Access the Chat UI URL Log in with an authorized user Send a test message: \"Who are you?\" Verify the default agent responds Resource Groups Name Pattern Purpose rg-{project}-{env}-{region}-app AKS clusters rg-{project}-{env}-{region}-auth Authorization storage rg-{project}-{env}-{region}-data Customer data rg-{project}-{env}-{region}-dns Private DNS rg-{project}-{env}-{region}-net Networking rg-{project}-{env}-{region}-oai Azure OpenAI rg-{project}-{env}-{region}-ops Operations rg-{project}-{env}-{region}-storage FLLM storage rg-{project}-{env}-{region}-vec Vector databases Updating Deployments To update container images: ./Deploy-Images-Aks.ps1 ` -aksName \"<aks-name>\" ` -resourceGroup \"<resource-group>\" ` -tag \"<version>\" See Updating Container Versions for details. Troubleshooting Issue Solution Quota exceeded Request quota increase for selected SKUs Network access denied Verify hosts file or DNS configuration Certificate errors Verify PFX format and placement Pod crashes Check pod logs with kubectl logs Related Topics Quick Start Deployment Deployment Manifest Setup Authentication Setup Network Security Groups Updating Container Versions"
  },
  "docs/platform-operations/deployment/index.html": {
    "href": "docs/platform-operations/deployment/index.html",
    "title": "Deployment | FoundationaLLM",
    "summary": "Deployment FoundationaLLM provides multiple deployment options to suit different environments and requirements. Deployment Options Deployment Type Infrastructure Best For Quick Start Azure Container Apps (ACA) Development, POC, small deployments Standard Azure Kubernetes Service (AKS) Production, enterprise scale Quick Comparison Feature Quick Start (ACA) Standard (AKS) Complexity Low High Setup Time ~30 minutes ~2 hours Scalability Auto-scaling Advanced orchestration Networking Public endpoints Private networking, VPN SSL Certificates Managed Custom required Cost Lower Higher Production Ready Dev/Test Yes Prerequisites Both deployment types require: Requirement Description Azure Subscription With appropriate quotas Azure OpenAI Access Request access Entra ID App Registrations 6 app registrations required Owner Role On subscription and app registrations Post-Deployment Configuration After deployment completes, you must: Configure Authentication - Complete Entra ID app registration setup Set MS Graph Permissions - Run the Graph roles script Update Redirect URIs - Configure OAuth callbacks Verify Access - Test portal and API access See Authentication Setup for detailed instructions. Deployment Tools Tool Version Purpose Azure CLI v2.51.0+ Azure resource management Azure Developer CLI (azd) v1.6.1+ Infrastructure provisioning PowerShell 7.4.1+ Deployment scripts Git Latest Repository cloning AzCopy Latest File synchronization Additional Tools for Standard Deployment Tool Purpose kubectl Kubernetes management kubelogin AKS authentication Helm Kubernetes package management Related Topics Quick Start Deployment Standard Deployment App Configuration Values Authentication Setup Azure Resource Providers"
  },
  "docs/platform-operations/deployment/soft-delete.html": {
    "href": "docs/platform-operations/deployment/soft-delete.html",
    "title": "Soft Delete in Azure Resources | FoundationaLLM",
    "summary": "Soft Delete in Azure Resources Several Azure resources used by FoundationaLLM implement soft delete, which retains deleted resources for a period before permanent deletion. Overview Soft delete protects against accidental deletion but requires consideration during: Redeployment with same resource names Subscription cleanup Quota management Resources with Soft Delete Resource Type Default Retention Impact Azure OpenAI 48 hours Quota remains allocated Azure Key Vault 7-90 days Name unavailable Azure AI Search 7 days Name unavailable Azure Content Safety 48 hours Quota remains allocated Implications Resource Naming When redeploying after deletion: Soft-deleted resources block reuse of the same name Either purge the resource OR use a different name Quota Management Soft-deleted Azure OpenAI resources: Continue consuming quota allocation May prevent new deployments in the same region Must be purged to release quota Purging Resources Using azd down The recommended method for complete cleanup: azd down --purge The --purge flag: Deletes all deployed resources Purges soft-deleted resources Releases all quotas and name reservations Manual Purging Azure OpenAI # List deleted resources az cognitiveservices account list-deleted # Purge specific resource az cognitiveservices account purge \\ --location <region> \\ --resource-group <resource-group> \\ --name <account-name> Key Vault Via Portal: Navigate to Key Vaults Select Manage deleted vaults Select the vault Click Purge Via CLI: # List deleted vaults az keyvault list-deleted # Purge specific vault az keyvault purge --name <vault-name> Azure AI Search Via Portal: Navigate to Azure AI Search The deleted service will show in soft-deleted state Select and purge Via CLI: # Currently requires portal or REST API Azure Content Safety # Similar to Azure OpenAI az cognitiveservices account purge \\ --location <region> \\ --resource-group <resource-group> \\ --name <account-name> Best Practices Before Redeployment Check for Soft-Deleted Resources az keyvault list-deleted az cognitiveservices account list-deleted Purge if Reusing Names azd down --purge Or Use Different Names Update resource names in deployment manifest/parameters Regular Cleanup Audit Soft-Deleted Resources Periodically review deleted resources Purge unnecessary ones to release quota Document Naming Conventions Track resource names used Plan for naming collisions Quota Planning Monitor Quota Usage Track Azure OpenAI TPM/RPM limits Account for soft-deleted resources Plan Regional Distribution Spread deployments across regions if hitting limits Troubleshooting Issue Cause Solution \"Name already exists\" Soft-deleted resource Purge or use different name Quota exceeded Soft-deleted resources consuming quota Purge resources Cannot recreate Key Vault Soft-delete retention Wait for retention period or purge Deployment fails Previous soft-deleted resources Run azd down --purge first Disabling Soft Delete Warning: Disabling soft delete removes protection against accidental deletion. Key Vault Soft delete is mandatory for Key Vault and cannot be disabled. Other Resources Some resources allow disabling soft delete during creation, but this is not recommended for production environments. Related Topics Quick Start Deployment Standard Deployment Troubleshooting"
  },
  "docs/platform-operations/deployment/standard-manifest.html": {
    "href": "docs/platform-operations/deployment/standard-manifest.html",
    "title": "Deployment Manifest Setup | FoundationaLLM",
    "summary": "Deployment Manifest Setup The Deployment Manifest is a JSON file that configures all aspects of a Standard (AKS) deployment. Overview The manifest template is located at: deploy/standard/Deployment-Manifest.template.json Copy to create your deployment configuration: cp Deployment-Manifest.template.json Deployment-Manifest.json Tip: Create separate manifests for different environments (dev, test, prod). General Settings Root-level properties defining the deployment: Property Type Description Example adminObjectId GUID AD Group Object ID for admins 995a549b-067e-... baseDomain String Base domain for services example.com createVpnGateway Boolean Create VPN Gateway true environment String Environment identifier dev, test, prod instanceId GUID Unique deployment instance ID 5d40d2ee-aeb5-... k8sNamespace String Kubernetes namespace fllm letsEncryptEmail Email Let's Encrypt notifications admin@example.com location String Azure region eastus2 networkName String Pre-provisioned network name fllm-network project String Project identifier (3-8 chars) fllm subscription GUID Azure subscription ID ad82622e-458a-... Notes createVpnGateway: Set to false if using existing VPN or ExpressRoute instanceId: Generate a unique GUID; used by authorization system letsEncryptEmail: Only needed if using Let's Encrypt certificates location: Choose a region supporting required Azure OpenAI models Supported Models by Region The template auto-configures available models: gpt-35-turbo (0613, 1106) gpt-4 (1106-Preview) gpt-4o (2024-05-13) text-embedding-ada-002 (2) text-embedding-3-large/small See Azure OpenAI model availability. Entra Client IDs Application registration client IDs: Property Description Reference authorization Authorization API Authorization Setup chat Chat Portal Core Auth Setup core Core API Core Auth Setup managementapi Management API Management Auth Setup managementui Management Portal Management Auth Setup vectorizationapi Vectorization API Authentication Setup Entra Client Secrets Secrets for authorization: Property Description authorization Client secret for Authorization app registration Security: Store secrets securely. Consider using Azure Key Vault references. Entra Instances Cloud endpoints for authentication: Property Value authorization https://login.microsoftonline.com/ For sovereign clouds, use the appropriate login URL. Entra Scopes API scopes for each service: Property Example Description authorization api://FoundationaLLM-Authorization-Auth Authorization API scope chat api://FoundationaLLM-Auth/Data.Read Chat Portal scope core Data.Read Core API scope managementapi Data.Manage Management API scope managementui api://FoundationaLLM-Management-Auth/Data.Manage Management Portal scope vectorizationapi Data.Manage Vectorization API scope Ingress Configuration API Ingress (Backend Cluster) { \"apiIngress\": { \"coreapi\": { \"host\": \"api.fllm.example.com\", \"path\": \"/core/\", \"pathType\": \"ImplementationSpecific\", \"serviceName\": \"core-api\", \"sslCert\": \"coreapi\" }, \"managementapi\": { \"host\": \"management-api.fllm.example.com\", \"path\": \"/management/\", \"pathType\": \"ImplementationSpecific\", \"serviceName\": \"management-api\", \"sslCert\": \"managementapi\" }, \"vectorizationapi\": { \"host\": \"vectorization-api.fllm.example.com\", \"path\": \"/vectorization/\", \"pathType\": \"ImplementationSpecific\", \"serviceName\": \"vectorization-api\", \"sslCert\": \"vectorizationapi\" } } } Frontend Ingress (Frontend Cluster) { \"frontendIngress\": { \"chatui\": { \"host\": \"chat.fllm.example.com\", \"path\": \"/\", \"pathType\": \"ImplementationSpecific\", \"serviceName\": \"chat-ui\", \"sslCert\": \"chatui\" }, \"managementui\": { \"host\": \"management.fllm.example.com\", \"path\": \"/\", \"pathType\": \"ImplementationSpecific\", \"serviceName\": \"management-ui\", \"sslCert\": \"managementui\" } } } Resource Groups Define resource group names for each workload: Property Purpose Default Pattern app AKS clusters rg-{env}-{region}-app-{project} auth Authorization storage rg-{env}-{region}-auth-{project} data Customer data rg-{env}-{region}-data-{project} dns Private DNS rg-{env}-{region}-dns-{project} jbx Jumpbox rg-{env}-{region}-jbx-{project} net Networking rg-{env}-{region}-net-{project} oai Azure OpenAI rg-{env}-{region}-oai-{project} ops Operations rg-{env}-{region}-ops-{project} storage FLLM storage rg-{env}-{region}-storage-{project} vec Vector databases rg-{env}-{region}-vec-{project} External Resource Groups Pre-existing resource groups: Property Purpose dns Pre-provisioned Private DNS zones Note: Remove corresponding entry from resourceGroups if using external resources. Example Manifest { \"adminObjectId\": \"995a549b-067e-4fe3-9f90-98d78b9ed086\", \"baseDomain\": \"example.com\", \"createVpnGateway\": false, \"environment\": \"dev\", \"instanceId\": \"5d40d2ee-aeb5-4391-95a0-1fd9045d7720\", \"k8sNamespace\": \"fllm\", \"letsEncryptEmail\": \"admin@example.com\", \"location\": \"eastus2\", \"networkName\": \"fllm-network\", \"project\": \"ai\", \"subscription\": \"ad82622e-458a-4a48-8023-6b18eed1cf79\", \"entraClientIds\": { \"authorization\": \"...\", \"chat\": \"...\", \"core\": \"...\", \"managementapi\": \"...\", \"managementui\": \"...\", \"vectorizationapi\": \"...\" }, \"entraClientSecrets\": { \"authorization\": \"...\" }, \"entraInstances\": { \"authorization\": \"https://login.microsoftonline.com/\" }, \"entraScopes\": { \"authorization\": \"api://FoundationaLLM-Authorization-Auth\", \"chat\": \"api://FoundationaLLM-Auth/Data.Read\", \"core\": \"Data.Read\", \"managementapi\": \"Data.Manage\", \"managementui\": \"api://FoundationaLLM-Management-Auth/Data.Manage\", \"vectorizationapi\": \"Data.Manage\" }, \"resourceGroups\": { \"app\": \"rg-ai-dev-eastus2-app\", \"auth\": \"rg-ai-dev-eastus2-auth\", \"data\": \"rg-ai-dev-eastus2-data\", \"dns\": \"rg-ai-dev-eastus2-dns\", \"jbx\": \"rg-ai-dev-eastus2-jbx\", \"net\": \"rg-ai-dev-eastus2-net\", \"oai\": \"rg-ai-dev-eastus2-oai\", \"ops\": \"rg-ai-dev-eastus2-ops\", \"storage\": \"rg-ai-dev-eastus2-storage\", \"vec\": \"rg-ai-dev-eastus2-vec\" } } Validation Before deployment, verify: All GUIDs are valid Domain names are correct SSL certificates are in place Entra app registrations exist Resource group names follow conventions Related Topics Standard Deployment Authentication Setup Custom Domains"
  },
  "docs/platform-operations/how-to-guides/backups.html": {
    "href": "docs/platform-operations/how-to-guides/backups.html",
    "title": "Backups & Data Resiliency | FoundationaLLM",
    "summary": "Backups & Data Resiliency This guide covers backup strategies and data resiliency options for FoundationaLLM platform components. Overview Before implementing backup strategies, consider: Factor Description RTO Recovery Time Objective - acceptable downtime RPO Recovery Point Objective - acceptable data loss Compliance Regulatory requirements for data retention Cost Storage and compute costs for backups Cosmos DB Cosmos DB stores conversations, user sessions, and other application data. Automated Backups Feature Standard Deployment Default Backup Mode Continuous or Periodic Retention Period 30 days Backup Frequency Automatic Restore Granularity Point-in-time Configuring Backup Policy Via Azure Portal: Navigate to your Cosmos DB account Select Settings > Backup & Restore Configure backup mode and retention Via Azure CLI: az cosmosdb update \\ --name <account-name> \\ --resource-group <resource-group> \\ --backup-policy-type Continuous Restoring from Backup Via Azure Portal: Navigate to Cosmos DB account Select Settings > Backup & Restore Click Restore Select restore point and destination Via Azure CLI: az cosmosdb restore \\ --account-name <source-account> \\ --resource-group <resource-group> \\ --target-database-account-name <target-account> \\ --restore-timestamp \"2024-01-01T00:00:00Z\" \\ --location <region> Data Resiliency Options Feature Description Default Global Distribution Multi-region replication Not enabled Consistency Levels Session, Strong, Eventual, etc. Session Automatic Failover Regional failover Not configured TODO: Document procedure for enabling multi-region replication in Standard deployment. Storage Accounts Storage accounts contain prompts, agents, data sources, and vectorized content. Built-in Protection Feature Standard Deployment Default Replication LRS (Locally Redundant) Blob Versioning Enabled Soft Delete (Blobs) 30 days Soft Delete (Containers) 30 days Upgrading Replication Option Description Use Case LRS 3 copies in single datacenter Development ZRS 3 copies across availability zones Production GRS 6 copies across two regions Disaster recovery RA-GRS GRS with read access to secondary High availability Change Replication: az storage account update \\ --name <account-name> \\ --resource-group <resource-group> \\ --sku Standard_GRS Azure Backup for Storage Configure Azure Backup for blob data: Navigate to Recovery Services vault Select + Backup Choose Azure Blob Storage Select storage account Configure backup policy Manual Export Export data for archival or migration: # Export using AzCopy azcopy copy \"https://<account>.blob.core.windows.net/<container>/*\" \\ \"/local/backup/\" \\ --recursive Key Vault Key Vault stores secrets, certificates, and encryption keys. Built-in Protection Feature Default Soft Delete Enabled (7-90 days) Purge Protection Enabled (7 days) Secret Versioning Automatic Backing Up Secrets Via Azure Portal: Navigate to Key Vault Select Secrets Select a secret Click Download Backup Via Azure CLI: # Backup single secret az keyvault secret backup \\ --vault-name <vault-name> \\ --name <secret-name> \\ --file <backup-file> # Restore secret az keyvault secret restore \\ --vault-name <vault-name> \\ --file <backup-file> Backup All Secrets Script $vaultName = \"<vault-name>\" $backupPath = \"./keyvault-backup\" # Create backup directory New-Item -ItemType Directory -Force -Path $backupPath # Get all secrets $secrets = az keyvault secret list --vault-name $vaultName --query \"[].name\" -o tsv # Backup each secret foreach ($secret in $secrets) { az keyvault secret backup ` --vault-name $vaultName ` --name $secret ` --file \"$backupPath/$secret.backup\" } Note: Key Vault does not support full vault backup. Back up individual items. App Configuration App Configuration stores application settings and feature flags. Versioning Changes are automatically versioned Access history via History view in portal Rollback by restoring previous version Export Configuration Via Azure Portal: Navigate to App Configuration Select Import/export Select Export Choose destination (file, App Service, another App Config) Via Azure CLI: # Export to file az appconfig kv export \\ --name <app-config-name> \\ --destination file \\ --path ./config-backup.json \\ --format json Import Configuration # Import from file az appconfig kv import \\ --name <app-config-name> \\ --source file \\ --path ./config-backup.json \\ --format json Backup Schedule Recommendations Component Frequency Retention Cosmos DB Continuous 30 days Storage (critical) Daily 30 days Key Vault secrets Before changes As needed App Configuration Before changes As needed Disaster Recovery Standard Deployment Considerations Component DR Strategy AKS Clusters Redeploy from templates Cosmos DB Restore from backup Storage Restore from GRS secondary Key Vault Restore secrets from backup App Config Import from export Recovery Procedure Assess - Identify affected components Provision - Deploy new infrastructure Restore - Restore data from backups Configure - Apply App Configuration Validate - Test functionality Cutover - Update DNS/routing TODO: Add detailed disaster recovery runbook for Standard deployment. Related Topics Logs & Monitoring Troubleshooting Standard Deployment"
  },
  "docs/platform-operations/how-to-guides/creating-release-notes.html": {
    "href": "docs/platform-operations/how-to-guides/creating-release-notes.html",
    "title": "Creating Release Notes | FoundationaLLM",
    "summary": "Creating Release Notes This guide provides the process for creating release notes for FoundationaLLM updates. Overview Release notes communicate changes to end-users, stakeholders, and operations teams. Well-structured notes help with: Understanding what's new or changed Planning upgrade timing Identifying breaking changes Tracking project evolution Release Notes Structure Standard Format # Release X.Y.Z **Release Date:** YYYY-MM-DD ## Highlights - Major feature or change summary ## New Features - Feature 1 description (#issue-number) - Feature 2 description (#issue-number) ## Enhancements - Enhancement 1 description (#issue-number) ## Bug Fixes - Fix 1 description (#issue-number) ## Breaking Changes - Description of breaking change - Migration steps ## Deprecations - Feature being deprecated - Removal timeline ## Security Updates - Security-related fixes ## Known Issues - Outstanding issues ## Upgrade Instructions - Steps to upgrade Process 1. Define Release Scope Review the milestone or sprint scope Identify all changes included Confirm release version number 2. Gather Changes from Version Control Using GitHub: # List commits since last release git log v0.8.3..HEAD --oneline # List merged PRs gh pr list --state merged --base main --json number,title Using GitHub Web: Navigate to repository Go to Pull requests > Closed Filter by milestone or date range 3. Categorize Changes Group changes into standard categories: Category Description New Features New functionality Enhancements Improvements to existing features Bug Fixes Resolved issues Breaking Changes Changes requiring user action Deprecations Features being phased out Security Updates Security-related changes 4. Write Clear Descriptions Good Example: - Add support for GPT-4o model deployment in agents (#1234) - Fix conversation history not persisting after browser refresh (#1235) Poor Example: - Updated stuff - Fixed bug 5. Document Breaking Changes For each breaking change: Describe what changed Explain the impact Provide migration steps Note any automated migration ## Breaking Changes ### Agent Configuration Format Change The agent configuration schema has changed: **Before (v0.8.x):** ```json { \"workflow\": \"langchain\" } After (v0.9.x): { \"workflow_object_id\": \"/instances/{id}/providers/FoundationaLLM.Agent/workflows/langchain\" } Migration: Run the upgrade script Migrate-AgentConfig.ps1 before deploying v0.9.x. ### 6. Include Upgrade Instructions Provide clear upgrade steps: ```markdown ## Upgrade Instructions ### From v0.8.x to v0.9.x 1. Backup your deployment 2. Run pre-migration script 3. Update container images 4. Run post-migration script 5. Verify functionality See [Upgrade Guide](./upgrade-guide.md) for detailed steps. 7. Review Process Self-review - Check for accuracy and completeness Technical review - Have developers verify changes Stakeholder review - Get product owner approval Edit - Incorporate feedback 8. Publish GitHub Release: Navigate to repository > Releases Click Draft a new release Select or create tag (e.g., v0.9.0) Enter release title Paste release notes Attach any artifacts Click Publish release Templates Minor Release Template # Release X.Y.0 **Release Date:** YYYY-MM-DD ## Highlights - [Main feature or theme of this release] ## New Features - ## Enhancements - ## Bug Fixes - ## Upgrade Instructions 1. Pull latest container images 2. Update App Configuration (if needed) 3. Verify deployment See [Upgrade Guide](link) for details. Patch Release Template # Release X.Y.Z **Release Date:** YYYY-MM-DD This is a patch release containing bug fixes. ## Bug Fixes - ## Upgrade Instructions Update container images to tag `X.Y.Z`. Best Practices Practice Description Be Specific Include issue numbers and specific descriptions User Focus Write from the user's perspective Consistent Format Use the same structure for all releases Link Documentation Reference detailed docs where appropriate Test Instructions Verify upgrade steps work Automation Consider automating release note generation: # Example GitHub Action name: Generate Release Notes on: release: types: [created] jobs: generate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Generate notes run: | gh api repos/${{ github.repository }}/releases/generate-notes \\ -f tag_name=${{ github.ref_name }} \\ -f previous_tag_name=$(git describe --tags --abbrev=0 HEAD^) Related Topics Updating Container Versions GitHub Releases"
  },
  "docs/platform-operations/how-to-guides/purge-conversations.html": {
    "href": "docs/platform-operations/how-to-guides/purge-conversations.html",
    "title": "Purging Deleted Conversations | FoundationaLLM",
    "summary": "Purging Deleted Conversations This guide explains how to anonymize deleted conversations in Cosmos DB to remove sensitive information while preserving statistical data. Overview When users delete conversations in FoundationaLLM: The session is soft-deleted (marked as deleted) Original content remains in the database Statistical data (tokens, feedback) is preserved For compliance or security requirements, you may need to anonymize this content. Anonymization Approach The anonymization process: Identifies soft-deleted sessions Replaces sensitive content with \"Deleted\" Preserves metadata for analytics What Gets Anonymized Document Type Anonymized Fields CompletionPrompt prompt Message text, content.value, analysisResults.toolInput, analysisResults.toolOutput What's Preserved Data Purpose Document IDs Reference integrity Timestamps Analytics Token counts Usage metrics User feedback Quality metrics Session structure Statistical analysis Creating the Stored Procedure Step 1: Open Azure Portal Navigate to your Cosmos DB account Select Data Explorer Step 2: Create Stored Procedure Expand your database Right-click the Sessions container Select New Stored Procedure Step 3: Add Stored Procedure Code Name: anonymizeDeletedSession function anonymizeDeletedSession() { var collection = getContext().getCollection(); var response = getContext().getResponse(); // Retrieve Session document var sessionQuery = `SELECT * FROM c WHERE c.type = 'Session'`; var isSessionAccepted = collection.queryDocuments( collection.getSelfLink(), sessionQuery, {}, function (err, sessionDocuments, responseOptions) { if (err) throw err; if (sessionDocuments.length === 0 || !sessionDocuments[0].deleted) { response.setBody(\"Session cannot be anonymized because it was not deleted.\"); return; } // Session marked as deleted, proceed var query = `SELECT * FROM c WHERE c.type = 'CompletionPrompt' OR c.type = 'Message'`; var isAccepted = collection.queryDocuments( collection.getSelfLink(), query, {}, function (err, documents, responseOptions) { if (err) throw err; if (documents.length === 0) { response.setBody(\"No related documents found.\"); return; } var updatedCount = 0; var processedCount = 0; documents.forEach(function (doc) { if (doc.type === \"CompletionPrompt\") { doc.prompt = \"Deleted\"; } if (doc.type === \"Message\") { doc.text = \"Deleted\"; if (Array.isArray(doc.content)) { doc.content.forEach(function (contentItem) { contentItem.value = \"Deleted\"; }); } if (Array.isArray(doc.analysisResults)) { doc.analysisResults.forEach(function (analysisItem) { analysisItem.toolInput = \"Deleted\"; analysisItem.toolOutput = \"Deleted\"; }); } } // Update the document var acceptUpdate = collection.replaceDocument(doc._self, doc, function (err) { if (err) throw err; updatedCount++; processedCount++; // Check if all documents processed if (processedCount === documents.length) { response.setBody(\"Updated \" + updatedCount + \" documents.\"); } }); if (!acceptUpdate) throw new Error(\"Update not accepted, aborting\"); }); } ); if (!isAccepted) throw new Error(\"Query was not accepted by server.\"); } ); if (!isSessionAccepted) throw new Error(\"Session query was not accepted by server.\"); } Step 4: Save Click Save to create the stored procedure. Executing the Stored Procedure Via Azure Portal Navigate to Data Explorer Expand Sessions container > Stored Procedures Select anonymizeDeletedSession Click Execute In the input panel: Partition key value: Enter the sessionId of the deleted session Click Execute Expected Output Success: Updated X documents. Session Not Deleted: Session cannot be anonymized because it was not deleted. No Related Documents: No related documents found. Via Azure CLI # Execute stored procedure az cosmosdb sql stored-procedure execute \\ --account-name <cosmos-account> \\ --database-name <database> \\ --container-name Sessions \\ --name anonymizeDeletedSession \\ --partition-key-value <session-id> \\ --resource-group <resource-group> Bulk Anonymization For bulk anonymization of multiple sessions: Query Deleted Sessions SELECT c.id, c.sessionId FROM c WHERE c.type = 'Session' AND c.deleted = true Automation Script # Get all deleted sessions $deletedSessions = az cosmosdb sql query \\ --account-name $cosmosAccount \\ --database-name $database \\ --container-name Sessions \\ --query \"SELECT c.sessionId FROM c WHERE c.type = 'Session' AND c.deleted = true\" \\ | ConvertFrom-Json # Anonymize each session foreach ($session in $deletedSessions) { Write-Host \"Anonymizing session: $($session.sessionId)\" az cosmosdb sql stored-procedure execute ` --account-name $cosmosAccount ` --database-name $database ` --container-name Sessions ` --name anonymizeDeletedSession ` --partition-key-value $session.sessionId ` --resource-group $resourceGroup } Scheduling Automatic Purges Using Azure Functions Create a timer-triggered function to run periodically: [FunctionName(\"AnonymizeDeletedSessions\")] public static async Task Run( [TimerTrigger(\"0 0 2 * * *\")] TimerInfo timer, // Daily at 2 AM [CosmosDB( databaseName: \"%CosmosDbDatabase%\", containerName: \"Sessions\", Connection = \"CosmosDbConnection\")] CosmosClient client, ILogger log) { // Implementation to find and anonymize deleted sessions } TODO: Provide complete Azure Function implementation for scheduled anonymization. Compliance Considerations Requirement Implementation GDPR Right to Erasure Anonymize user data on request Data Retention Preserve aggregate statistics Audit Trail Log anonymization operations Verification Query to confirm anonymization Verification Query Confirm anonymization: SELECT c.type, c.text, c.prompt FROM c WHERE c.sessionId = '<session-id>' AND (c.type = 'Message' OR c.type = 'CompletionPrompt') Expected result: All text and prompt fields show \"Deleted\". Related Topics Backups & Data Resiliency Platform Security Logs & Monitoring"
  },
  "docs/platform-operations/how-to-guides/updating-container-versions.html": {
    "href": "docs/platform-operations/how-to-guides/updating-container-versions.html",
    "title": "Updating Container Versions | FoundationaLLM",
    "summary": "Updating Container Versions This guide covers updating FoundationaLLM container images in Standard (AKS) deployments. Overview Container updates are required to: Apply bug fixes Access new features Address security vulnerabilities Maintain platform compatibility Prerequisites Requirement Description Azure CLI Configured for correct tenant/subscription AKS Credentials In .kube/config directory Helm CLI Installed and configured kubectl Installed and configured Getting AKS Credentials az aks get-credentials ` --name \"<aks-cluster-name>\" ` --resource-group \"<resource-group>\" Update Process Step 1: Check Current Versions # List all Helm releases helm list -A # Check specific deployment kubectl get deployment -n fllm -o wide Step 2: Get Current Values Export the current Helm values: # For backend services helm get values \"foundationallm\" --all > gvalues.yaml # For frontend services helm get values \"foundationallm-web\" --all > gvalues-web.yaml Step 3: Identify Target Version Check available versions: GitHub Releases Container registry tags Step 4: Update Images Use the deployment script: cd deploy/scripts .\\Deploy-Images-Aks.ps1 ` -aksName \"<aks-name>\" ` -resourceGroup \"<resource-group>\" ` -tag \"<version-tag>\" ` -charts \"*\" ` -namespace \"fllm\" Script Parameters Parameter Default Description -name foundationallm Helm release name -aksName (Required) AKS cluster name -resourceGroup (Required) Azure resource group -tag latest Image version tag -charts * Charts to deploy (* for all) -valuesFile gvalues.yaml Helm values file path -namespace (Current) Kubernetes namespace -tlsEnv prod TLS environment (prod, staging, none, custom) -tlsHost - Custom TLS hostname -tlsSecretName - Custom TLS secret name -autoscale $false Enable autoscaling Selective Updates Update Specific Charts # Update only API services .\\Deploy-Images-Aks.ps1 ` -aksName \"<aks-name>\" ` -resourceGroup \"<resource-group>\" ` -tag \"0.9.0\" ` -charts \"core-api,management-api\" Available Charts Chart Description core-api Core API service management-api Management API service orchestration-api Orchestration service gatekeeper-api Content safety service chat-ui Chat portal management-ui Management portal Update Frontend Only .\\Deploy-Images-Aks.ps1 ` -aksName \"<frontend-aks-name>\" ` -resourceGroup \"<resource-group>\" ` -tag \"0.9.0\" ` -charts \"chat-ui,management-ui\" Rolling Updates Kubernetes performs rolling updates by default: Old pods remain until new pods are healthy No downtime during updates Automatic rollback on failure Monitor Update Progress # Watch deployment status kubectl rollout status deployment/core-api -n fllm # View pod status kubectl get pods -n fllm -w Rollback if Needed # Rollback to previous version kubectl rollout undo deployment/core-api -n fllm # Rollback Helm release helm rollback foundationallm 1 -n fllm Custom Values Modifying Configuration Edit gvalues.yaml to customize: # Example: Increase replica count coreApi: replicaCount: 3 resources: requests: cpu: \"500m\" memory: \"512Mi\" limits: cpu: \"1000m\" memory: \"1Gi\" Apply Custom Values .\\Deploy-Images-Aks.ps1 ` -aksName \"<aks-name>\" ` -resourceGroup \"<resource-group>\" ` -tag \"0.9.0\" ` -valuesFile \"custom-values.yaml\" Verifying Updates Check Image Versions kubectl get pods -n fllm -o jsonpath=\"{.items[*].spec.containers[*].image}\" Check Service Health # Get pod status kubectl get pods -n fllm # Check logs kubectl logs -n fllm deployment/core-api --tail=100 Test Functionality Access Chat Portal Send a test message Verify agent response Check Management Portal Troubleshooting Issue Solution Pods not starting Check kubectl describe pod <pod-name> -n fllm Image pull errors Verify registry access and image tag Service unavailable Check kubectl get svc -n fllm Configuration issues Compare gvalues.yaml with defaults Common Errors ImagePullBackOff: # Check image details kubectl describe pod <pod-name> -n fllm | grep -A 5 \"Image:\" # Verify registry credentials kubectl get secrets -n fllm CrashLoopBackOff: # Check pod logs kubectl logs <pod-name> -n fllm --previous # Check events kubectl get events -n fllm --sort-by='.lastTimestamp' Maintenance Windows Best Practices Schedule Updates during low-traffic periods Test in Staging before production Review Release Notes for breaking changes Backup before major updates Monitor closely after updates Update Checklist [ ] Review release notes [ ] Backup current configuration [ ] Update staging environment [ ] Test functionality in staging [ ] Schedule production maintenance window [ ] Update production [ ] Verify all services [ ] Monitor for issues Related Topics Standard Deployment Creating Release Notes Troubleshooting GitHub Releases"
  },
  "docs/platform-operations/monitoring-troubleshooting/logs.html": {
    "href": "docs/platform-operations/monitoring-troubleshooting/logs.html",
    "title": "System Logs & Audit Trails | FoundationaLLM",
    "summary": "System Logs & Audit Trails This guide covers accessing and analyzing logs in FoundationaLLM deployments. Overview FoundationaLLM centralizes logs in Azure Log Analytics Workspace, providing: Unified view of all platform components Correlation across services Advanced query capabilities Integration with Azure Monitor Log Types Log Type Source Purpose Application Logs Container apps/pods Application errors, info, debug Security Logs Entra ID, Key Vault Authentication, authorization System Logs AKS, ACA infrastructure Platform health, scaling Audit Logs Key Vault, Cosmos DB Resource access tracking Log Location Standard Deployment All logs flow to the Log Analytics Workspace created during deployment: Located in the Operations (ops) resource group Named: log-{project}-{env}-{region} Quick Start Deployment Logs are available in: Container Apps environment logs Log Analytics Workspace Accessing Logs Azure Portal Navigate to Log Analytics workspace Select Logs in the left menu Use Kusto Query Language (KQL) to query Azure CLI # Query logs az monitor log-analytics query \\ --workspace <workspace-id> \\ --analytics-query \"ContainerAppConsoleLogs | take 100\" Common Queries Application Errors // Last 24 hours of errors across all services ContainerAppConsoleLogs | where TimeGenerated > ago(24h) | where Log contains \"error\" or Log contains \"Error\" or Log contains \"ERROR\" | project TimeGenerated, ContainerAppName, Log | order by TimeGenerated desc Specific Service Logs // Core API logs ContainerAppConsoleLogs | where ContainerAppName contains \"core-api\" | where TimeGenerated > ago(1h) | project TimeGenerated, Log | order by TimeGenerated desc Authentication Failures // Failed authentication attempts AADSignInLogs | where TimeGenerated > ago(7d) | where ResultType != 0 | project TimeGenerated, UserPrincipalName, ResultType, ResultDescription | order by TimeGenerated desc Key Vault Access // Key Vault operations AzureDiagnostics | where ResourceProvider == \"MICROSOFT.KEYVAULT\" | where TimeGenerated > ago(24h) | project TimeGenerated, OperationName, ResultType, CallerIPAddress | order by TimeGenerated desc Request Performance // API request duration AppRequests | where TimeGenerated > ago(1h) | summarize avg(DurationMs), percentile(DurationMs, 95), count() by Name | order by avg_DurationMs desc Container Restarts // Container restart events ContainerAppSystemLogs | where TimeGenerated > ago(24h) | where Reason == \"Restarted\" or Reason == \"BackOff\" | project TimeGenerated, ContainerAppName, Reason, Log | order by TimeGenerated desc Setting Up Alerts Create Alert Rule Navigate to Monitor > Alerts Click + Create > Alert rule Select your Log Analytics workspace Configure condition (e.g., error count > threshold) Configure action group (email, webhook, etc.) Create rule Example: Error Spike Alert // Alert condition query ContainerAppConsoleLogs | where TimeGenerated > ago(5m) | where Log contains \"error\" or Log contains \"ERROR\" | summarize ErrorCount = count() by bin(TimeGenerated, 5m) | where ErrorCount > 10 Log Retention Default Settings Log Type Default Retention Application Logs 30 days Security Logs 90 days System Logs 30 days Changing Retention Via Azure Portal: Navigate to Log Analytics workspace Select Usage and estimated costs Select Data Retention Adjust retention period Via Azure CLI: az monitor log-analytics workspace update \\ --resource-group <resource-group> \\ --workspace-name <workspace-name> \\ --retention-time 90 Long-Term Archival For retention beyond 730 days: Export to Storage Account Navigate to Log Analytics workspace Select Export under Settings Configure export to Storage Account Select tables and destination Archive to Data Lake Configure continuous export: az monitor log-analytics workspace data-export create \\ --resource-group <resource-group> \\ --workspace-name <workspace-name> \\ --name \"archive-export\" \\ --destination <storage-account-resource-id> \\ --enable true \\ --tables ContainerAppConsoleLogs Access Control Required Permissions Role Access Log Analytics Reader Read logs, run queries Log Analytics Contributor Read/write, manage queries Monitoring Contributor Full access to monitoring Restrict Access Use Azure RBAC to limit log access: az role assignment create \\ --assignee <user-or-group-id> \\ --role \"Log Analytics Reader\" \\ --scope <workspace-resource-id> Integration with Azure Monitor Application Insights FoundationaLLM APIs integrate with Application Insights for: Request tracing Dependency tracking Performance metrics Custom telemetry Dashboards Create custom dashboards: Navigate to Dashboard in Azure Portal Click + Add tile Select Logs and add your query Configure visualization Workbooks Use Azure Monitor Workbooks for interactive reports: Navigate to Monitor > Workbooks Create or use existing templates Add queries and visualizations Azure Sentinel Integration For advanced security monitoring: Enable Azure Sentinel on the Log Analytics workspace Configure data connectors for Entra ID, Key Vault Create analytics rules for threat detection Set up playbooks for automated response Note: Azure Sentinel is not configured by default in Standard deployment. Best Practices Practice Description Centralize Send all logs to single workspace Retain Set appropriate retention policies Alert Configure alerts for critical issues Review Regularly review security logs Archive Export for long-term compliance Related Topics Troubleshooting Platform Security Backups & Data Resiliency"
  },
  "docs/platform-operations/monitoring-troubleshooting/troubleshooting.html": {
    "href": "docs/platform-operations/monitoring-troubleshooting/troubleshooting.html",
    "title": "Troubleshooting Guide | FoundationaLLM",
    "summary": "Troubleshooting Guide This guide provides structured approaches to diagnosing and resolving common issues in FoundationaLLM deployments. Quick Diagnostics Health Check Commands # AKS deployment - check all pods kubectl get pods -n fllm # AKS deployment - check services kubectl get svc -n fllm # Quick Start - check container apps az containerapp list -g <resource-group> -o table Expected Healthy State Component Status All pods Running All services Active All containers Running Common Issues 1. Authentication Failures Symptoms Unable to log in to portals \"Invalid token\" errors Redirect loops after login Diagnosis // Check Entra ID sign-in logs AADSignInLogs | where TimeGenerated > ago(1h) | where ResultType != 0 | project TimeGenerated, UserPrincipalName, AppDisplayName, ResultType, ResultDescription Solutions Issue Solution Invalid redirect URI Update redirect URIs in App Registration Missing scopes Configure API permissions Client secret expired Generate new secret, update Key Vault Wrong tenant Verify tenant ID in App Configuration Verify App Registration: Open Azure Portal > Microsoft Entra ID > App registrations Select the application Check Authentication > Redirect URIs Verify the URI matches your deployment URL + /signin-oidc See Authentication Setup for detailed configuration. 2. Missing App Configuration Values Symptoms Services fail to start Configuration-related errors in logs \"Key not found\" errors Diagnosis # Check App Configuration az appconfig kv list --name <app-config-name> -o table # Check specific key az appconfig kv show --name <app-config-name> --key \"FoundationaLLM:Instance:Id\" Solutions Verify Key Vault References # Check Key Vault secret exists az keyvault secret show --vault-name <vault-name> --name <secret-name> Check Managed Identity Permissions App Configuration Reader on App Config Key Vault Secrets User on Key Vault Re-run Configuration Script # Quick Start cd deploy/quick-start ../common/scripts/Set-AzdEnvEntra.ps1 3. Container Crashes Symptoms Pods in CrashLoopBackOff state Services intermittently unavailable Container restarts Diagnosis # Get pod status kubectl get pods -n fllm # Describe failing pod kubectl describe pod <pod-name> -n fllm # Get logs from crashed container kubectl logs <pod-name> -n fllm --previous // Query for container crashes ContainerAppConsoleLogs | where TimeGenerated > ago(24h) | where Log contains \"exception\" or Log contains \"fatal\" or Log contains \"crash\" | project TimeGenerated, ContainerAppName, Log | order by TimeGenerated desc Solutions Issue Solution Out of memory Increase memory limits in Helm values Missing config Check environment variables and App Config Dependency unavailable Verify dependent services are running Image pull error Check registry access and image tag Check Resource Usage: kubectl top pods -n fllm Update Resource Limits: # In Helm values resources: limits: memory: \"2Gi\" cpu: \"1000m\" requests: memory: \"1Gi\" cpu: \"500m\" 4. API Errors Symptoms 500 errors from APIs Timeout errors Incomplete responses Diagnosis // API error analysis AppRequests | where TimeGenerated > ago(1h) | where Success == false | summarize count() by Name, ResultCode | order by count_ desc # Check API pod logs kubectl logs -n fllm deployment/core-api --tail=200 Solutions Error Code Likely Cause Solution 401 Authentication Check token and permissions 403 Authorization Verify RBAC roles 500 Server error Check logs for details 502 Bad gateway Check pod health 503 Service unavailable Check service endpoints 504 Timeout Check dependencies, increase timeout 5. Azure OpenAI Errors Symptoms \"Model deployment not found\" Quota exceeded errors Timeout on completions Diagnosis # Check OpenAI deployment az cognitiveservices account deployment list \\ --name <openai-account> \\ --resource-group <resource-group> # Check quota az cognitiveservices account show \\ --name <openai-account> \\ --resource-group <resource-group> Solutions Issue Solution Deployment not found Create model deployment in Azure Portal Quota exceeded Request quota increase or use different region Wrong deployment name Update App Configuration with correct name Region unavailable Check model availability by region 6. Vector Search Issues Symptoms No results from knowledge queries \"Index not found\" errors Slow search responses Diagnosis # Check AI Search service az search service show --name <search-service> --resource-group <resource-group> # List indexes az search index list --service-name <search-service> --resource-group <resource-group> Solutions Verify Index Exists - Check in Azure Portal Check Permissions - Managed Identity needs Search Index Data Reader Re-run Indexing - Trigger data pipeline 7. Network Connectivity Symptoms Services can't communicate DNS resolution failures Timeout between services Diagnosis (AKS) # Check service endpoints kubectl get endpoints -n fllm # Test DNS resolution kubectl run dns-test --image=busybox --rm -it --restart=Never -- nslookup core-api.fllm.svc.cluster.local # Check network policies kubectl get networkpolicies -n fllm Solutions Issue Solution No endpoints Check pod selector labels DNS failure Restart CoreDNS pods Network policy blocking Review network policy rules Private endpoint issues Check NSG and private DNS zones Diagnostic Tools Log Analytics Queries Save these queries for quick access: // Error summary by service ContainerAppConsoleLogs | where TimeGenerated > ago(24h) | where Log contains \"error\" or Log contains \"Error\" | summarize ErrorCount = count() by ContainerAppName | order by ErrorCount desc // Request latency percentiles AppRequests | where TimeGenerated > ago(1h) | summarize p50 = percentile(DurationMs, 50), p95 = percentile(DurationMs, 95), p99 = percentile(DurationMs, 99) by Name Health Check Script # Quick health check script $namespace = \"fllm\" Write-Host \"=== Pod Status ===\" -ForegroundColor Cyan kubectl get pods -n $namespace Write-Host \"`n=== Service Status ===\" -ForegroundColor Cyan kubectl get svc -n $namespace Write-Host \"`n=== Recent Events ===\" -ForegroundColor Cyan kubectl get events -n $namespace --sort-by='.lastTimestamp' | Select-Object -Last 10 Write-Host \"`n=== Resource Usage ===\" -ForegroundColor Cyan kubectl top pods -n $namespace Reporting Issues If issues persist after troubleshooting: 1. Check Existing Issues Search GitHub Issues for similar problems. 2. Gather Information Collect before opening an issue: Deployment type (Quick Start/Standard) Version/release tag Error messages and logs Steps to reproduce Configuration (sanitized) 3. Open GitHub Issue Navigate to GitHub Issues Click New Issue Select appropriate template Provide detailed information Add relevant labels Issue Template ## Description Brief description of the issue ## Environment - Deployment Type: [Quick Start / Standard] - Version: [e.g., 0.9.0] - Azure Region: [e.g., eastus2] ## Steps to Reproduce 1. Step 1 2. Step 2 3. Step 3 ## Expected Behavior What should happen ## Actual Behavior What actually happens ## Logs/Screenshots [Attach relevant logs or screenshots] ## Additional Context Any other relevant information Related Topics System Logs Authentication Setup App Configuration Values Platform Security"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/index.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/index.html",
    "title": "Authentication & Authorization Setup | FoundationaLLM",
    "summary": "Authentication & Authorization Setup FoundationaLLM uses Microsoft Entra ID for user authentication and authorization across all platform components. Overview Component Authentication App Authorization Chat Portal FoundationaLLM-User-Portal Core API access Core API FoundationaLLM-Core-API User operations Management Portal FoundationaLLM-Management-Portal Management API access Management API FoundationaLLM-Management-API Admin operations Authorization API FoundationaLLM-Authorization-API RBAC management Required App Registrations You must create 6 app registrations in Microsoft Entra ID: App Registration Purpose FoundationaLLM-User-Portal Chat portal authentication FoundationaLLM-Core-API Core API authentication FoundationaLLM-Management-Portal Management portal authentication FoundationaLLM-Management-API Management API authentication FoundationaLLM-Authorization-API Authorization service FoundationaLLM-Reader Read-only access (optional) Setup Options Option 1: Automated Script (Recommended) Run the automated script to create all app registrations: cd deploy/common/scripts ./Create-FllmEntraIdApps.ps1 The script: Creates all 6 app registrations Configures scopes and permissions Sets up proper token configurations After script completion, verify in Azure Portal > Microsoft Entra ID > App registrations. Option 2: Manual Setup Complete the following guides in order: Pre-Deployment (Before running azd up) Core API & User Portal Setup Management API & Portal Setup Authorization API Setup Post-Deployment (After running azd up) Prerequisites Core API Post-Deployment Management API Post-Deployment Authorization Post-Deployment Configuration Summary App Configuration Keys After setup, verify these App Configuration values: Key Value FoundationaLLM:Chat:Entra:ClientId User Portal client ID FoundationaLLM:Chat:Entra:TenantId Your tenant ID FoundationaLLM:Chat:Entra:Scopes api://FoundationaLLM-Core/Data.Read FoundationaLLM:CoreAPI:Entra:ClientId Core API client ID FoundationaLLM:CoreAPI:Entra:TenantId Your tenant ID FoundationaLLM:Management:Entra:ClientId Management Portal client ID FoundationaLLM:Management:Entra:Scopes api://FoundationaLLM-Management/Data.Manage FoundationaLLM:ManagementAPI:Entra:ClientId Management API client ID Required Permissions Role Scope Purpose Cloud Application Administrator Entra ID Create app registrations Global Administrator OR Privileged Role Administrator Entra ID Assign MS Graph permissions Contributor Azure Subscription Access App Configuration Post-Deployment Scripts Configure MS Graph Permissions After deployment, run: cd deploy/quick-start # or deploy/standard ../common/scripts/Set-FllmGraphRoles.ps1 -resourceGroupName <resource-group> This grants managed identities the required MS Graph permissions. Update OAuth Callback URIs Update redirect URIs with deployment URLs: ../common/scripts/Update-OAuthCallbackUris.ps1 Verifying Setup Test Authentication Navigate to Chat Portal URL Sign in with Entra ID account Verify successful login Troubleshoot Authentication Issues Issue Solution Redirect loop Check redirect URIs in app registration Invalid token Verify client IDs in App Configuration Access denied Check API permissions and scopes 401 Unauthorized Verify tenant ID configuration See Troubleshooting for detailed diagnostics. Security Considerations Practice Recommendation Token Lifetime Use default settings Conditional Access Configure based on security requirements MFA Enable for all users Secret Rotation Rotate client secrets before expiration Related Topics Role-Based Access Control Platform Security App Configuration Values"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/authorization-post.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/authorization-post.html",
    "title": "Authorization API Post-Deployment Configuration | FoundationaLLM",
    "summary": "Authorization API Post-Deployment Configuration Complete these steps after running azd up to finalize Authorization API configuration. Prerequisites Deployment completed successfully App Configuration access configured (see Prerequisites) Authorization API app registration created (see Pre-Deployment Setup) Update App Configuration Settings Step 1: Access App Configuration Sign in to Azure Portal Navigate to your deployment resource group Select the App Configuration resource Select Configuration explorer Step 2: Update Authorization Scope Search for authorization in the filter Find FoundationaLLM:APIs:AuthorizationAPI:APIScope Click Edit Set value to: api://FoundationaLLM-Authorization Click Apply Verify Configuration Check App Configuration Values Verify these authorization-related settings: Key Expected Value FoundationaLLM:APIs:AuthorizationAPI:APIScope api://FoundationaLLM-Authorization FoundationaLLM:APIs:AuthorizationAPI:APIUrl Your Authorization API URL Configure Initial Role Assignments After authorization is configured, set up initial RBAC: Assign Admin Role The deployment administrator needs the Owner role: Navigate to Management Portal Go to Security > Role Assignments Create assignment: Principal: Your admin user/group Role: Owner Scope: /instances/{instanceId} Or via Management API: POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments Content-Type: application/json Authorization: Bearer <token> { \"name\": \"admin-assignment\", \"principal_id\": \"<admin-object-id>\", \"principal_type\": \"User\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/1301f8d4-3bea-4880-945f-315dbd2ddb46\", \"scope\": \"/instances/{instanceId}\" } Verify Authorization Test that authorization is working: Sign in to Management Portal Navigate to any resource Verify you can perform expected operations Run MS Graph Roles Script The Authorization API requires MS Graph permissions for user lookups: cd deploy/quick-start # or deploy/standard ../common/scripts/Set-FllmGraphRoles.ps1 -resourceGroupName <resource-group> Requirement: User running script must be Global Administrator or have Privileged Role Administrator role. Troubleshooting Issue Solution Authorization denied Verify role assignments exist User lookup fails Run MS Graph roles script Invalid scope Check APIScope value in App Configuration 403 Forbidden Verify user has appropriate role for action Check Authorization API Logs AKS: kubectl logs deployment/authorization-api -n fllm --tail=100 Verify Role Assignments Query existing assignments: GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments Authorization: Bearer <token> Next Steps Review Role Definitions Configure Role Assignments Set up Agent-Level Permissions Related Topics Authentication Setup Overview Role-Based Access Control Permissions Reference"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/core-authentication-post.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/core-authentication-post.html",
    "title": "Core API & User Portal Post-Deployment Configuration | FoundationaLLM",
    "summary": "Core API & User Portal Post-Deployment Configuration Complete these steps after running azd up to finalize Core API and Chat Portal authentication. Prerequisites Deployment completed successfully App Configuration access configured (see Prerequisites) App registrations created (see Pre-Deployment Setup) Update App Configuration Settings Step 1: Access App Configuration Sign in to Azure Portal Navigate to your deployment resource group Select the App Configuration resource Select Configuration explorer Step 2: Filter Settings Enter entra in the search box Check the box next to Key to select all Entra-related settings Click Edit to open the JSON editor Step 3: Update Values Update the following settings with values from your app registrations: Key Value FoundationaLLM:Chat:Entra:ClientId Application (client) ID of FoundationaLLM-User-Portal FoundationaLLM:Chat:Entra:TenantId Directory (tenant) ID FoundationaLLM:Chat:Entra:Scopes api://FoundationaLLM-Core/Data.Read FoundationaLLM:CoreAPI:Entra:ClientId Application (client) ID of FoundationaLLM-Core-API FoundationaLLM:CoreAPI:Entra:TenantId Directory (tenant) ID Step 4: Verify Default Values Confirm these values are correct: Key Expected Value FoundationaLLM:Chat:Entra:CallbackPath /signin-oidc FoundationaLLM:Chat:Entra:Instance https://login.microsoftonline.com/ FoundationaLLM:CoreAPI:Entra:CallbackPath /signin-oidc FoundationaLLM:CoreAPI:Entra:Instance https://login.microsoftonline.com/ FoundationaLLM:CoreAPI:Entra:Scopes Data.Read Step 5: Save Changes Click Apply to save all configuration changes. Restart Services After updating configuration, restart the services to apply changes. Azure Container Apps (Quick Start) Navigate to your resource group Select the Core API Container App (ends with coreapica) Select Revisions in left menu Select the active revision Click Restart in the Revision details panel Repeat for Chat UI Container App (ends with chatuica) Azure Kubernetes Service (Standard) Via Azure Portal: Navigate to the AKS resource Select Workloads > Pods tab Filter by namespace: fllm Select core-api and chat-ui pods Click Delete (new pods auto-create) Via kubectl: kubectl rollout restart deployment/core-api -n fllm kubectl rollout restart deployment/chat-ui -n fllm Verify Authentication Test Sign-In Navigate to your Chat Portal URL You should be redirected to Microsoft sign-in Enter your Entra ID credentials Verify successful authentication Troubleshoot Issues Issue Solution Redirect loop Verify redirect URI in app registration matches deployment URL Invalid client Check FoundationaLLM:Chat:Entra:ClientId value Invalid scope Verify scope format: api://FoundationaLLM-Core/Data.Read AADSTS50011 Add correct redirect URI to app registration AADSTS700016 Verify tenant ID is correct Check Logs ACA: az containerapp logs show -n <app-name> -g <resource-group> AKS: kubectl logs deployment/core-api -n fllm --tail=100 kubectl logs deployment/chat-ui -n fllm --tail=100 Next Steps Complete Management API Post-Deployment Complete Authorization Post-Deployment Configure Role-Based Access Control Related Topics Authentication Setup Overview App Configuration Values Troubleshooting"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/management-authentication-post.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/post-deployment/management-authentication-post.html",
    "title": "Management API & Portal Post-Deployment Configuration | FoundationaLLM",
    "summary": "Management API & Portal Post-Deployment Configuration Complete these steps after running azd up to finalize Management API and Portal authentication. Prerequisites Deployment completed successfully App Configuration access configured (see Prerequisites) App registrations created (see Pre-Deployment Setup) Core API configuration completed (see Core API Post-Deployment) Update App Configuration Settings Step 1: Access App Configuration Sign in to Azure Portal Navigate to your deployment resource group Select the App Configuration resource Select Configuration explorer Step 2: Filter Settings Enter entra in the search box Select all management-related settings Step 3: Update Values Update the following settings with values from your app registrations: Key Value FoundationaLLM:Management:Entra:ClientId Application (client) ID of FoundationaLLM-Management-Portal FoundationaLLM:Management:Entra:TenantId Directory (tenant) ID FoundationaLLM:Management:Entra:Scopes api://FoundationaLLM-Management/Data.Manage FoundationaLLM:ManagementAPI:Entra:ClientId Application (client) ID of FoundationaLLM-Management-API FoundationaLLM:ManagementAPI:Entra:TenantId Directory (tenant) ID Step 4: Verify Default Values Confirm these values are correct: Key Expected Value FoundationaLLM:Management:Entra:CallbackPath /signin-oidc FoundationaLLM:Management:Entra:Instance https://login.microsoftonline.com/ FoundationaLLM:ManagementAPI:Entra:Instance https://login.microsoftonline.com/ FoundationaLLM:ManagementAPI:Entra:Scopes Data.Manage Step 5: Save Changes Click Apply to save all configuration changes. Restart Services After updating configuration, restart the services to apply changes. Azure Container Apps (Quick Start) Navigate to your resource group Select the Management API Container App (ends with managementapica) Select Revisions in left menu Select the active revision Click Restart in the Revision details panel Repeat for Management UI Container App (ends with managementuica) Azure Kubernetes Service (Standard) Via kubectl: kubectl rollout restart deployment/management-api -n fllm kubectl rollout restart deployment/management-ui -n fllm Verify Authentication Test Sign-In Navigate to your Management Portal URL You should be redirected to Microsoft sign-in Enter your Entra ID credentials Verify successful authentication Confirm you can access the Management Portal dashboard Troubleshoot Issues Issue Solution Redirect loop Verify redirect URI matches deployment URL Invalid client Check FoundationaLLM:Management:Entra:ClientId value Access denied Verify user has appropriate RBAC roles Invalid scope Verify scope: api://FoundationaLLM-Management/Data.Manage Check Logs AKS: kubectl logs deployment/management-api -n fllm --tail=100 kubectl logs deployment/management-ui -n fllm --tail=100 Next Steps Complete Authorization Post-Deployment Configure Role-Based Access Control Test Management Portal Features Related Topics Authentication Setup Overview Core API Post-Deployment App Configuration Values"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/authorization-setup.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/authorization-setup.html",
    "title": "Authorization API Setup | FoundationaLLM",
    "summary": "Authorization API Setup This guide covers creating the Microsoft Entra ID app registration for the Authorization API service. Overview The Authorization API handles role-based access control (RBAC) for FoundationaLLM resources. App Registration Purpose Type FoundationaLLM-Authorization-API Authorization service authentication API Create the API Application Step 1: Register the Application Sign in to Microsoft Entra admin center Navigate to Identity > Applications > App registrations Click + New registration Configure: Name: FoundationaLLM-Authorization-API Supported account types: Accounts in this organizational directory only Click Register Record the Application (client) ID and Directory (tenant) ID Step 2: Expose an API Under Manage, select Expose an API Click Add a scope Set Application ID URI: api://FoundationaLLM-Authorization Click Save and continue Configure scope: Field Value Scope name Authorization.Manage Who can consent? Admins and users Admin consent display name Manage Authorization Admin consent description Allows the app to manage data on behalf of the signed-in user User consent display name Manage data on behalf of the user User consent description Allows the app to manage data on behalf of the signed-in user State Enabled Click Add scope Record the scope name: api://FoundationaLLM-Authorization/Authorization.Manage Step 3: Update Manifest Under Manage, select Manifest Find accessTokenAcceptedVersion Change value to 2 Click Save Step 4: Configure Authentication Under Manage, select Authentication Click Add a platform > Web Enter Redirect URI: http://localhost Under Implicit grant and hybrid flows: Check Access tokens Check ID tokens Click Configure Client Secret (For Standard Deployment) Standard deployments require a client secret: Step 1: Create Secret Under Manage, select Certificates & secrets Click + New client secret Configure: Description: FoundationaLLM-Authorization Expires: Select appropriate duration Click Add Record the secret Value immediately (it won't be shown again) Step 2: Store in Deployment Manifest For Standard deployments, add to Deployment-Manifest.json: { \"entraClientSecrets\": { \"authorization\": \"<secret-value>\" } } Values to Record Save these values for configuration: Value Configuration Location Application (client) ID Deployment Manifest: entraClientIds.authorization Directory (tenant) ID Used across all configurations Scope api://FoundationaLLM-Authorization/Authorization.Manage Client Secret Deployment Manifest: entraClientSecrets.authorization App Configuration Keys After deployment, verify these values: Key Expected Value FoundationaLLM:APIs:AuthorizationAPI:APIScope api://FoundationaLLM-Authorization Next Steps Run deployment (azd up) Complete Post-Deployment Configuration Related Topics Authentication Setup Overview Core API & Portal Setup Management API & Portal Setup Role-Based Access Control"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/core-authentication-setup.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/core-authentication-setup.html",
    "title": "Core API & User Portal Authentication Setup | FoundationaLLM",
    "summary": "Core API & User Portal Authentication Setup This guide covers creating the Microsoft Entra ID app registrations for the Core API and Chat (User) Portal. Overview You will create two app registrations: App Registration Purpose Type FoundationaLLM-User-Portal Chat portal authentication Client (SPA) FoundationaLLM-Core-API Core API authentication API Create the Client Application (User Portal) Step 1: Register the Application Sign in to Microsoft Entra admin center Navigate to Identity > Applications > App registrations Click + New registration Configure: Name: FoundationaLLM-User-Portal Supported account types: Accounts in this organizational directory only Click Register Record the Application (client) ID and Directory (tenant) ID Step 2: Configure Authentication Under Manage, select Authentication Click Add a platform > Single-page application Add Redirect URIs: Environment URI Production <CHAT_PORTAL_URL>/signin-oidc Local Dev http://localhost:3000/signin-oidc Click Configure Step 3: Enable Implicit Grant Under Implicit grant and hybrid flows: Check Access tokens Check ID tokens Click Save Step 4: (Optional) Add Postman Redirect For API testing with Postman: Click Add a platform > Mobile and desktop applications Add: https://oauth.pstmn.io/v1/callback Click Configure Step 5: Update Manifest Under Manage, select Manifest Find accessTokenAcceptedVersion Change value to 2 Click Save Create the API Application (Core API) Step 1: Register the Application Navigate to App registrations > + New registration Configure: Name: FoundationaLLM-Core-API Supported account types: Accounts in this organizational directory only Click Register Record the Application (client) ID and Directory (tenant) ID Step 2: Configure Authentication Under Manage, select Authentication Click Add a platform > Web Enter Redirect URI: http://localhost Click Configure Note: The localhost URI is required to enable token options but isn't used in production. Step 3: Enable Implicit Grant Check Access tokens Check ID tokens Click Save Step 4: Expose an API Under Manage, select Expose an API Click Add a scope Set Application ID URI: api://FoundationaLLM-Core Click Save and continue Configure scope: Field Value Scope name Data.Read Who can consent? Admins and users Admin consent display name Read data on behalf of users Admin consent description Allows the app to read data on behalf of the signed-in user User consent display name Read data on behalf of the user User consent description Allows the app to read data on behalf of the signed-in user State Enabled Click Add scope Record the scope name: api://FoundationaLLM-Core/Data.Read Step 5: Add Authorized Client Application Still in Expose an API, click + Add a client application Paste the Application (client) ID of FoundationaLLM-User-Portal Check the Data.Read scope Click Add application Step 6: Update Manifest Under Manage, select Manifest Find accessTokenAcceptedVersion Change value to 2 Click Save Configure API Permissions (Client App) Add Permissions to User Portal Navigate to App registrations Select FoundationaLLM-User-Portal Under Manage, select API permissions Click + Add a permission Select My APIs tab Select FoundationaLLM-Core-API Check Data.Read Click Add permissions Grant Admin Consent (Optional) If required by your organization: Click Grant admin consent for [tenant] Confirm Values to Record Save these values for App Configuration: Value App Configuration Key User Portal Client ID FoundationaLLM:Chat:Entra:ClientId Core API Client ID FoundationaLLM:CoreAPI:Entra:ClientId Tenant ID FoundationaLLM:Chat:Entra:TenantId, FoundationaLLM:CoreAPI:Entra:TenantId Scope FoundationaLLM:Chat:Entra:Scopes = api://FoundationaLLM-Core/Data.Read Next Steps Complete Management API & Portal Setup Complete Authorization API Setup Run deployment (azd up) Complete Post-Deployment Configuration Related Topics Authentication Setup Overview App Configuration Values"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/management-authentication-setup.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/pre-deployment/management-authentication-setup.html",
    "title": "Management API & Portal Authentication Setup | FoundationaLLM",
    "summary": "Management API & Portal Authentication Setup This guide covers creating the Microsoft Entra ID app registrations for the Management API and Management Portal. Overview You will create two app registrations: App Registration Purpose Type FoundationaLLM-Management-Portal Management portal authentication Client (SPA) FoundationaLLM-Management-API Management API authentication API Create the Client Application (Management Portal) Step 1: Register the Application Sign in to Microsoft Entra admin center Navigate to Identity > Applications > App registrations Click + New registration Configure: Name: FoundationaLLM-Management-Portal Supported account types: Accounts in this organizational directory only Click Register Record the Application (client) ID and Directory (tenant) ID Step 2: Configure Authentication Under Manage, select Authentication Click Add a platform > Single-page application Add Redirect URIs: Environment URI Production <MANAGEMENT_PORTAL_URL>/signin-oidc Local Dev http://localhost:3001/signin-oidc Click Configure Step 3: Enable Implicit Grant Under Implicit grant and hybrid flows: Check Access tokens Check ID tokens Click Save Step 4: (Optional) Add Postman Redirect For API testing with Postman: Click Add a platform > Mobile and desktop applications Add: https://oauth.pstmn.io/v1/callback Click Configure Step 5: Update Manifest Under Manage, select Manifest Find accessTokenAcceptedVersion Change value to 2 Click Save Create the API Application (Management API) Step 1: Register the Application Navigate to App registrations > + New registration Configure: Name: FoundationaLLM-Management-API Supported account types: Accounts in this organizational directory only Click Register Record the Application (client) ID and Directory (tenant) ID Step 2: Configure Authentication Under Manage, select Authentication Click Add a platform > Web Enter Redirect URI: http://localhost Click Configure Note: The localhost URI is required to enable token options but isn't used in production. Step 3: Enable Implicit Grant Check Access tokens Check ID tokens Click Save Step 4: Expose an API Under Manage, select Expose an API Click Add a scope Set Application ID URI: api://FoundationaLLM-Management (or accept default) Click Save and continue Configure scope: Field Value Scope name Data.Manage Who can consent? Admins and users Admin consent display name Manage data on behalf of users Admin consent description Allows the app to manage data on behalf of the signed-in user User consent display name Manage data on behalf of the user User consent description Allows the app to manage data on behalf of the signed-in user State Enabled Click Add scope Record the scope name: api://FoundationaLLM-Management/Data.Manage Step 5: Add Authorized Client Application Still in Expose an API, click + Add a client application Paste the Application (client) ID of FoundationaLLM-Management-Portal Check the Data.Manage scope Click Add application Step 6: Update Manifest Under Manage, select Manifest Find accessTokenAcceptedVersion Change value to 2 Click Save Configure API Permissions (Client App) Add Permissions to Management Portal Navigate to App registrations Select FoundationaLLM-Management-Portal Under Manage, select API permissions Click + Add a permission Select My APIs tab Select FoundationaLLM-Management-API Check Data.Manage Click Add permissions Grant Admin Consent (Optional) If required by your organization: Click Grant admin consent for [tenant] Confirm Values to Record Save these values for App Configuration: Value App Configuration Key Management Portal Client ID FoundationaLLM:Management:Entra:ClientId Management API Client ID FoundationaLLM:ManagementAPI:Entra:ClientId Tenant ID FoundationaLLM:Management:Entra:TenantId, FoundationaLLM:ManagementAPI:Entra:TenantId Scope FoundationaLLM:Management:Entra:Scopes = api://FoundationaLLM-Management/Data.Manage Next Steps Complete Authorization API Setup Run deployment (azd up) Complete Post-Deployment Configuration Related Topics Authentication Setup Overview Core API & Portal Setup App Configuration Values"
  },
  "docs/platform-operations/security-permissions/authentication-authorization/pre-requisites.html": {
    "href": "docs/platform-operations/security-permissions/authentication-authorization/pre-requisites.html",
    "title": "Post-Deployment Prerequisites | FoundationaLLM",
    "summary": "Post-Deployment Prerequisites Complete these prerequisites before configuring authentication settings after deployment. Overview After running azd up or azd deploy, you need to: Access App Configuration Obtain application URLs Update app registrations with deployment-specific values Setup App Configuration Access Step 1: Navigate to Resources Sign in to Azure Portal Navigate to your deployment resource group Note: For ACA deployments, you'll see an additional ME_* resource group. For AKS, you'll see MC_*. Access the main resource group (without prefix) for App Configuration. Step 2: Verify Access Select the App Configuration resource Select Configuration explorer Attempt to view values Step 3: Add Permissions (if needed) If you cannot access configurations: Select Access control (IAM) in App Configuration Click + Add > Add role assignment Select role: App Configuration Data Owner Assign to your user account Click Review + assign Obtain Application URLs Azure Container Apps (Quick Start) Chat Portal URL: Navigate to resource group Select Container App ending with chatuica Copy Application Url from Overview Management Portal URL: Select Container App ending with managementuica Copy Application Url from Overview Azure Kubernetes Service (Standard) Chat Portal URL: Navigate to resource group Select the Kubernetes service resource Select Properties Note the HTTP application routing domain Chat URL: https://<domain>/ Management Portal URL: Management URL: https://<domain>/management/ Or from hosts file: Check deploy/standard/config/hosts.ingress generated during deployment. URLs for Redirect URIs Record these URLs for app registration updates: Application URL Pattern (ACA) URL Pattern (AKS) Chat Portal https://<name>chatuica.<region>.azurecontainerapps.io https://chat.<domain> Management Portal https://<name>managementuica.<region>.azurecontainerapps.io https://management.<domain> Update Redirect URIs After obtaining URLs, update app registrations: Chat Portal (FoundationaLLM-User-Portal) Navigate to Microsoft Entra ID > App registrations Select FoundationaLLM-User-Portal Select Authentication Under Single-page application, add: <CHAT_PORTAL_URL>/signin-oidc Click Save Management Portal (FoundationaLLM-Management-Portal) Select FoundationaLLM-Management-Portal Select Authentication Under Single-page application, add: <MANAGEMENT_PORTAL_URL>/signin-oidc Click Save Automation Script For Quick Start deployments, use the provided script: cd deploy/quick-start ../common/scripts/Update-OAuthCallbackUris.ps1 This automatically updates redirect URIs based on deployed resources. Next Steps After completing prerequisites: Complete Core API Post-Deployment Complete Management API Post-Deployment Complete Authorization Post-Deployment Related Topics Authentication Setup Overview App Configuration Values Troubleshooting"
  },
  "docs/platform-operations/security-permissions/configure-access-control-services.html": {
    "href": "docs/platform-operations/security-permissions/configure-access-control-services.html",
    "title": "Configure Access Control for Azure Services | FoundationaLLM",
    "summary": "Configure Access Control for Azure Services FoundationaLLM uses Azure RBAC to control access to underlying Azure services. This guide covers granting access to App Configuration and Key Vault. Overview FoundationaLLM follows least-privilege principles: Default: No user access Access must be explicitly granted Service accounts pre-configured by deployment Prerequisites FoundationaLLM deployed and running Contributor role on the resource group or subscription Azure Portal access Azure App Configuration Azure App Configuration stores application settings and feature flags. Available Roles Role Permissions Use Case App Configuration Data Reader Read settings and feature flags Services, local development App Configuration Data Owner Read and write settings Administrators Grant Access Via Azure Portal Navigate to Azure Portal Go to your deployment resource group Note: For ACA/AKS deployments, use the resource group without ME_ or MC_ prefix. Select the App Configuration resource (name ends with -appconfig) Select Access control (IAM) in left menu Click + Add > Add role assignment Select role: App Configuration Data Reader (for read access) App Configuration Data Owner (for full access) Click Next Click + Select members Search for and select the user/group Click Select Click Review + assign Via Azure CLI # Get resource ID APP_CONFIG_ID=$(az appconfig show \\ --name <app-config-name> \\ --resource-group <resource-group> \\ --query id -o tsv) # Assign Reader role az role assignment create \\ --assignee <user-or-group-id> \\ --role \"App Configuration Data Reader\" \\ --scope $APP_CONFIG_ID # Or assign Owner role az role assignment create \\ --assignee <user-or-group-id> \\ --role \"App Configuration Data Owner\" \\ --scope $APP_CONFIG_ID Azure Key Vault Azure Key Vault stores secrets, certificates, and keys. Available Roles Role Permissions Use Case Key Vault Secrets User Read secrets Services, local development Key Vault Secrets Officer Read and write secrets Administrators Key Vault Administrator Full management Key Vault admins Grant Access Via Azure Portal Navigate to your deployment resource group Select the Key Vault resource (name ends with -kv) Select Access control (IAM) Click + Add > Add role assignment Select role: Key Vault Secrets User (for read access) Key Vault Secrets Officer (for management) Click Next Click + Select members Search for and select the user/group Click Select Click Review + assign Via Azure CLI # Get resource ID KEYVAULT_ID=$(az keyvault show \\ --name <keyvault-name> \\ --resource-group <resource-group> \\ --query id -o tsv) # Assign Secrets User role az role assignment create \\ --assignee <user-or-group-id> \\ --role \"Key Vault Secrets User\" \\ --scope $KEYVAULT_ID # Or assign Secrets Officer role az role assignment create \\ --assignee <user-or-group-id> \\ --role \"Key Vault Secrets Officer\" \\ --scope $KEYVAULT_ID Other Azure Resources Storage Accounts Role Permissions Storage Blob Data Reader Read blobs Storage Blob Data Contributor Read/write blobs Storage Blob Data Owner Full blob access Cosmos DB Role Permissions Cosmos DB Account Reader Read metadata Cosmos DB Operator Manage accounts DocumentDB Account Contributor Full access Azure AI Search Role Permissions Search Index Data Reader Read index data Search Index Data Contributor Read/write index data Search Service Contributor Manage service Best Practices Practice Description Least Privilege Grant minimum required permissions Use Groups Assign roles to groups, not individuals Regular Review Audit role assignments periodically Just-in-Time Consider PIM for sensitive roles Document Track who has access and why Troubleshooting Access Denied Errors Verify role assignment exists Check correct resource scope Allow up to 30 minutes for propagation Verify user/group object ID Check Existing Assignments # List role assignments az role assignment list \\ --resource-group <resource-group> \\ --output table # Check specific resource az role assignment list \\ --scope <resource-id> \\ --output table Related Topics Platform Security Role-Based Access Control App Configuration Values"
  },
  "docs/platform-operations/security-permissions/graph-api-permissions.html": {
    "href": "docs/platform-operations/security-permissions/graph-api-permissions.html",
    "title": "Microsoft Graph API Permissions | FoundationaLLM",
    "summary": "Microsoft Graph API Permissions FoundationaLLM requires Microsoft Graph API permissions for its role-based access control (RBAC) implementation. Overview Graph API permissions enable: User and group enumeration Security principal lookups Group membership resolution Display name mapping Why Graph API Permissions Are Required RBAC Implementation A fully functional RBAC system requires: Capability Graph API Requirement Resolve group membership Group.Read.All Display user names User.Read.All Display service principals Application.Read.All Assign roles to groups Group.Read.All Enterprise Scenarios Standard approaches (like group claims in tokens) have limitations: Token size limits restrict group count Large organizations exceed claim limits Graph API reference is needed anyway FoundationaLLM directly calls Graph API for robust handling of enterprise scenarios. Required Permissions Permission Type Purpose Group.Read.All Application Read all groups, resolve memberships User.Read.All Application Read all users, display names Application.Read.All Application Read service principals Permission Justification Group.Read.All: Get group membership for authenticated users Evaluate role membership List groups for role assignment User.Read.All: Retrieve user display names List users for role assignment Resolve user object IDs Application.Read.All: Read service principal information Support for non-user principals in groups Required when groups contain service principals Assigned Services Permissions are granted to managed identities for: Service Purpose Core API User authentication, RBAC evaluation Management API RBAC management, user/group lookup Granting Permissions Using the Deployment Script After deployment, run: cd deploy/quick-start # or deploy/standard ../common/scripts/Set-FllmGraphRoles.ps1 -resourceGroupName <resource-group> Requirements Requirement Description Role Global Administrator OR Privileged Role Administrator Scope Entra ID tenant Manual Assignment If the script cannot be used: Navigate to Azure Portal > Microsoft Entra ID Select Enterprise applications Find the Core API managed identity Select Permissions Click Grant admin consent Repeat for Management API Transparency & Auditing Source Code Review All Graph API interactions are implemented in: src/dotnet/Common/Services/Security/MicrosoftGraphIdentityManagementService.cs The code: Performs only the operations described above Uses minimal property sets Is fully auditable in the public GitHub repository Audit Graph API Calls Monitor Graph API usage: AuditLogs | where OperationName contains \"Microsoft Graph\" | project TimeGenerated, OperationName, InitiatedBy, TargetResources | order by TimeGenerated desc Security Considerations Principle of Least Privilege Only Read permissions are requested No write or modify capabilities Application permissions (not delegated) Managed Identity Benefits Benefit Description No credentials No secrets to manage Auto-rotation Azure handles key management Audit trail All access logged Scoped access Limited to assigned permissions Alternative Approaches Approach Limitation Why Not Used Token claims 200 group limit Enterprise scenarios exceed limits Delegated permissions User context only Service-to-service needs app permissions Client secrets Credential management Managed identities are more secure Troubleshooting Permission Errors Error Solution \"Insufficient privileges\" Run Set-FllmGraphRoles.ps1 \"Access denied\" Verify admin consent granted \"Graph API error\" Check managed identity permissions Verify Permissions # List permissions for managed identity $servicePrincipal = Get-AzADServicePrincipal -DisplayName \"<managed-identity-name>\" Get-AzADAppPermission -ObjectId $servicePrincipal.Id Related Topics Platform Security Role-Based Access Control Authentication Setup"
  },
  "docs/platform-operations/security-permissions/network-security-groups.html": {
    "href": "docs/platform-operations/security-permissions/network-security-groups.html",
    "title": "Network Security Groups | FoundationaLLM",
    "summary": "Network Security Groups FoundationaLLM Standard deployment uses Network Security Groups (NSGs) to control network traffic in Azure Virtual Networks. Overview NSGs provide stateful packet filtering based on: Source/destination IP addresses Source/destination ports Protocol (TCP, UDP, etc.) Direction (inbound/outbound) Application Gateway NSG The Application Gateway subnet requires specific NSG rules for Azure services. Inbound Rules Rule Name Access Source Destination Port Protocol Priority Notes allow-internet-http-inbound Allow Internet VirtualNetwork 80 TCP 128 Customizable allow-internet-https-inbound Allow Internet VirtualNetwork 443 TCP 132 Customizable allow-gatewaymanager-inbound Allow GatewayManager * 65200-65535 TCP 148 Required allow-loadbalancer-inbound Allow AzureLoadBalancer * * * 164 Required deny-all-inbound Deny * * * * 4096 Default deny Important Notes GatewayManager Rule - Required by Azure. Do not modify or delete. LoadBalancer Rule - Required by Azure. Do not modify or delete. Internet Rules - Can be restricted to specific IP ranges. For more information, see Azure Application Gateway NSG requirements. AKS Subnet NSGs Backend Cluster Direction Source Destination Ports Purpose Inbound Application Gateway AKS Subnet 443 API traffic Inbound AzureLoadBalancer AKS Subnet * Health probes Outbound AKS Subnet Internet 443 Azure services Outbound AKS Subnet AzureCloud 443 Azure APIs Frontend Cluster Direction Source Destination Ports Purpose Inbound Application Gateway AKS Subnet 443 Portal traffic Inbound AzureLoadBalancer AKS Subnet * Health probes Outbound AKS Subnet Backend 443 API calls Outbound AKS Subnet Internet 443 Azure services Private Endpoint Subnet Direction Source Destination Ports Purpose Inbound AKS Subnets Private Endpoints 443 Service access Inbound Jumpbox Private Endpoints 443 Admin access Outbound * * * Deny all Customization Guidelines Restricting Internet Access Replace Internet source with specific CIDRs: { \"name\": \"allow-corporate-https-inbound\", \"properties\": { \"access\": \"Allow\", \"direction\": \"Inbound\", \"protocol\": \"Tcp\", \"sourceAddressPrefix\": \"203.0.113.0/24\", \"sourcePortRange\": \"*\", \"destinationAddressPrefix\": \"VirtualNetwork\", \"destinationPortRange\": \"443\", \"priority\": 132 } } Adding Custom Rules Example: Allow specific IP range: az network nsg rule create \\ --resource-group <resource-group> \\ --nsg-name <nsg-name> \\ --name allow-partner-https \\ --priority 140 \\ --access Allow \\ --direction Inbound \\ --protocol Tcp \\ --source-address-prefixes 198.51.100.0/24 \\ --source-port-ranges '*' \\ --destination-address-prefixes VirtualNetwork \\ --destination-port-ranges 443 Service Tags Use Azure service tags for easier rule management: Tag Description VirtualNetwork All VNet addresses AzureLoadBalancer Azure Load Balancer Internet Public internet GatewayManager Azure Gateway Manager AzureCloud All Azure public IPs AzureCosmosDB Cosmos DB service Storage Azure Storage AzureKeyVault Key Vault service Monitoring NSG Traffic Enable Flow Logs az network watcher flow-log create \\ --resource-group <resource-group> \\ --nsg <nsg-name> \\ --storage-account <storage-account> \\ --enabled true View NSG Effective Rules az network nic list-effective-nsg \\ --resource-group <resource-group> \\ --name <nic-name> Troubleshooting Common Issues Issue Solution Connection timeout Check NSG rules allow traffic Service unavailable Verify Application Gateway rules Inter-service failures Check AKS subnet rules NSG Diagnostic Commands # List NSG rules az network nsg rule list \\ --resource-group <resource-group> \\ --nsg-name <nsg-name> \\ --output table # Check flow logs az network watcher flow-log show \\ --resource-group <resource-group> \\ --nsg <nsg-name> Best Practices Practice Description Least Privilege Only allow necessary traffic Use Service Tags Easier maintenance than IP ranges Enable Flow Logs Audit and troubleshooting Document Changes Track rule modifications Test Changes Verify in staging first Related Topics Platform Security Standard Deployment Troubleshooting"
  },
  "docs/platform-operations/security-permissions/platform-security.html": {
    "href": "docs/platform-operations/security-permissions/platform-security.html",
    "title": "Platform Security Features & Best Practices | FoundationaLLM",
    "summary": "Platform Security Features & Best Practices This guide covers security features and best practices for FoundationaLLM deployments. Security Overview FoundationaLLM implements security at multiple layers: Layer Implementation Identity Microsoft Entra ID Network VNets, NSGs, Private Endpoints Data Encryption at rest/transit Application RBAC, Content Safety Operations Monitoring, Audit Logs Identity & Access Management Microsoft Entra ID Integration FoundationaLLM uses Entra ID for: User authentication Service authentication (Managed Identities) API authorization Group-based access control Best Practices Practice Recommendation MFA Enable for all users Conditional Access Configure based on risk profile Privileged Identity Management Use for admin accounts App Registration Security Limit secret expiration, use certificates Managed Identities All FoundationaLLM services use managed identities for: Azure resource access (Key Vault, Storage, Cosmos DB) Inter-service communication MS Graph API access Benefits: No credential management Automatic rotation Auditable access Network Security Standard Deployment Architecture Component Network Configuration VNets Isolated network segments Subnets Service-specific isolation NSGs Traffic filtering Private Endpoints Private access to PaaS services VNet Peering Hub connectivity See Network Security Groups for detailed NSG rules. Quick Start Deployment Uses Azure Container Apps managed networking Public endpoints with Azure-managed security Suitable for development/POC Private Connectivity Options Option Use Case VPN Gateway Site-to-site connectivity ExpressRoute Dedicated private connection Private Endpoints Private PaaS access Data Encryption Encryption at Rest Service Encryption Azure Storage AES-256, Microsoft-managed keys Cosmos DB AES-256, Microsoft-managed keys Key Vault HSM-backed encryption Azure OpenAI Microsoft-managed keys Customer-Managed Keys (CMK) Enable CMK for enhanced control: # Enable CMK for Storage Account az storage account update \\ --name <storage-account> \\ --resource-group <resource-group> \\ --encryption-key-source Microsoft.Keyvault \\ --encryption-key-vault <keyvault-uri> \\ --encryption-key-name <key-name> Note: CMK requires additional Key Vault configuration and is not enabled by default. Encryption in Transit All services use TLS 1.2+ Internal communication uses mTLS (AKS) API endpoints enforce HTTPS Threat Detection & Monitoring Default Monitoring Standard deployment enables: Azure Diagnostics on all resources Log Analytics workspace Application Insights Recommended Enhancements Feature Benefit Microsoft Defender for Cloud Threat detection, security posture Azure Sentinel SIEM, threat hunting Azure Monitor Alerts Real-time alerting Enable Defender for Cloud az security pricing create \\ --name VirtualMachines \\ --tier Standard az security pricing create \\ --name StorageAccounts \\ --tier Standard Security Alerts Configure alerts for: Failed authentication attempts Unusual API access patterns Resource configuration changes Key Vault access anomalies Gatekeeper & Content Safety Content Filtering FoundationaLLM includes content safety integrations: Feature Purpose Azure Content Safety Harmful content detection Microsoft Presidio PII detection/redaction Lakera Guard Prompt injection protection Enkrypt Guardrails Additional safety controls Configuration Enable/disable in App Configuration: Key Default FoundationaLLM:APIs:CoreAPI:BypassGatekeeper true FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableAzureContentSafety true FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableMicrosoftPresidio true Patch Management Container Updates Monitor GitHub Releases Apply security updates promptly Test in staging before production See Updating Container Versions. Azure Resource Updates Enable Azure auto-updates where applicable Review Azure Security Center recommendations Apply Azure platform updates Compliance Considerations Data Residency Deploy in regions meeting data residency requirements Configure Cosmos DB and Storage geo-replication appropriately Review Azure OpenAI data processing locations Audit Logging Enabled by default: Key Vault audit logs Cosmos DB diagnostic logs API request logs Authentication events Data Retention Configure retention based on compliance needs: Default: 30 days in Log Analytics Extend or archive for compliance Export to storage for long-term retention Security Checklist Pre-Deployment [ ] Review and accept Azure OpenAI terms [ ] Plan network architecture [ ] Identify admin users/groups [ ] Prepare SSL certificates (Standard) [ ] Review compliance requirements Post-Deployment [ ] Verify Entra ID configuration [ ] Configure RBAC assignments [ ] Enable MFA for all users [ ] Configure monitoring alerts [ ] Review NSG rules [ ] Test authentication flow Ongoing [ ] Monitor security alerts [ ] Review access logs [ ] Update container images [ ] Rotate secrets/certificates [ ] Conduct security reviews Related Topics Network Security Groups Role-Based Access Control Authentication Setup Vulnerability Management Graph API Permissions"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/agent-role-assignments.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/agent-role-assignments.html",
    "title": "Automating Agent Role Assignments | FoundationaLLM",
    "summary": "Automating Agent Role Assignments This guide covers programmatic management of role assignments for agents using the Management API. Overview Automate agent access control for: Bulk user provisioning Integration with identity management systems Scheduled access reviews Temporary access grants Prerequisites Authentication az login TOKEN=$(az account get-access-token \\ --scope api://FoundationaLLM-Management/Data.Manage \\ --query accessToken -o tsv) Required Role The caller must have at the agent scope: Owner (1301f8d4-3bea-4880-945f-315dbd2ddb46), OR User Access Administrator (fb8e0fd0-f7e2-4957-89d6-19f44f7d6618) Get Management API URL Find the URL in Management Portal under Deployment Information, or from your deployment configuration. Listing Agents Get all agents in your instance: GET {managementApiUrl}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents Authorization: Bearer {token} Response [ { \"resource\": { \"name\": \"sales-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/sales-agent\", \"display_name\": \"Sales Assistant\", \"description\": \"Helps sales team with customer inquiries\" } }, { \"resource\": { \"name\": \"support-agent\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/support-agent\", \"display_name\": \"Support Bot\", \"description\": \"Customer support automation\" } } ] Note the object_id for use in role assignments. Managing External Role Assignments Use the external role assignments endpoint for bulk operations: POST {managementApiUrl}/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/{agentName}/externalRoleAssignments Authorization: Bearer {token} Content-Type: application/json Request Body { \"roleAssignmentsToAdd\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [ \"user1@contoso.com\", \"user2@contoso.com\" ], \"expirationDate\": \"2024-12-31T23:59:59Z\" } ], \"roleAssignmentsToRemove\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [ \"former.employee@contoso.com\" ] } ] } Parameters Parameter Description roleDefinitionId Role to assign/remove identities Array of user principal names (UPNs) expirationDate Optional expiration for added assignments Common Role Definition IDs Role ID Owner 1301f8d4-3bea-4880-945f-315dbd2ddb46 Contributor e459c3a6-6b93-4062-85b3-fffc9fb253df Reader 00a53e72-f66e-4c03-8f81-7e885fd2eb35 Examples Grant Reader Access to Multiple Users curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/sales-agent/externalRoleAssignments\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"roleAssignmentsToAdd\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [ \"alice@contoso.com\", \"bob@contoso.com\", \"charlie@contoso.com\" ] } ], \"roleAssignmentsToRemove\": [] }' Grant Temporary Access curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/demo-agent/externalRoleAssignments\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"roleAssignmentsToAdd\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [\"demo.user@contoso.com\"], \"expirationDate\": \"2024-03-15T17:00:00Z\" } ], \"roleAssignmentsToRemove\": [] }' Remove Access curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/sales-agent/externalRoleAssignments\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"roleAssignmentsToAdd\": [], \"roleAssignmentsToRemove\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [ \"departed.employee@contoso.com\" ] } ] }' Add and Remove in Single Call curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/support-agent/externalRoleAssignments\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"roleAssignmentsToAdd\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [\"new.hire@contoso.com\"] } ], \"roleAssignmentsToRemove\": [ { \"roleDefinitionId\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"identities\": [\"contractor@external.com\"] } ] }' PowerShell Script Example $managementApiUrl = \"https://<management-api>\" $instanceId = \"your-instance-id\" $agentName = \"sales-agent\" # Get token $token = az account get-access-token ` --scope api://FoundationaLLM-Management/Data.Manage ` --query accessToken -o tsv # Define assignments $body = @{ roleAssignmentsToAdd = @( @{ roleDefinitionId = \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\" identities = @(\"user1@contoso.com\", \"user2@contoso.com\") } ) roleAssignmentsToRemove = @() } | ConvertTo-Json -Depth 3 # Make request $headers = @{ \"Authorization\" = \"Bearer $token\" \"Content-Type\" = \"application/json\" } $uri = \"$managementApiUrl/instances/$instanceId/providers/FoundationaLLM.Agent/agents/$agentName/externalRoleAssignments\" Invoke-RestMethod -Uri $uri -Method Post -Headers $headers -Body $body Integration Scenarios HR System Integration Sync agent access with HR onboarding/offboarding: HR system triggers webhook on employee status change Azure Function processes webhook Function calls Management API to add/remove assignments Access is immediately updated Access Review Automation Schedule periodic access reviews: Azure Logic App runs on schedule Queries current assignments Compares against approved access list Removes unauthorized assignments Sends report to administrators Related Topics Role Definitions Role Assignments Role Management Management API Reference"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/index.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/index.html",
    "title": "Role-Based Access Control | FoundationaLLM",
    "summary": "Role-Based Access Control FoundationaLLM RBAC provides fine-grained access control to platform resources. Overview FoundationaLLM RBAC enables: Controlling who can access resources Defining what actions users can perform Scoping access to specific resources or the entire instance Auditing access changes Key Concepts Concept Description Role Definition A collection of permissions (actions) Role Assignment Grants a role to a principal at a scope Principal User, group, service principal, or managed identity Scope Where the access applies (instance or resource) Built-in Roles Role Description Owner Full access including role assignment management Contributor Full access except role assignment management Reader Read-only access to resources User Access Administrator Manage role assignments only See Role Definitions for complete details. How RBAC Works graph LR A[Principal] -->|assigned| B[Role] B -->|defines| C[Permissions] C -->|applied at| D[Scope] D -->|grants access to| E[Resources] Principal requests access System checks role assignments Permissions are evaluated at the scope Access is granted or denied Management Options Management Portal The Management Portal provides a UI for: Viewing role assignments Creating new assignments Deleting assignments Managing access at instance and resource levels Management API Programmatic access via REST API: List role definitions Create/delete role assignments Query assignments by scope Retrieve identity information See Role Management for API details. Quick Start Grant Instance Access Navigate to Management Portal Select Security > Instance Access Control Click Add Role Assignment Select principal (user/group) Select role Click Save Grant Resource Access Navigate to the resource (e.g., Agent) Click Access Control button Click Add Role Assignment Select principal and role Click Save Inheritance Scope Inherits From Resource Instance Instance None (top level) Roles assigned at instance level apply to all resources Resource-level assignments add specific permissions Cannot remove inherited permissions at lower levels Documentation Topic Description Role Definitions Understanding role structure and permissions Role Assignments How assignments work Scope Understanding scope levels Role Management Managing assignments via Portal and API Agent Role Assignments Automating agent access control Related Topics Authentication Setup Platform Security Permissions Reference"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/role-assignments.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/role-assignments.html",
    "title": "Role Assignments | FoundationaLLM",
    "summary": "Role Assignments Role assignments grant access by associating a principal with a role at a specific scope. Role Assignment Components Component Description Principal Who receives access (user, group, service principal) Role Definition What access is granted Scope Where access applies Name Unique identifier for the assignment Description Optional explanation Assignment Structure { \"RoleAssignmentName\": \"00000000-0000-0000-0000-000000000000\", \"RoleAssignmentId\": \"/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/00000000-0000-0000-0000-000000000000\", \"Scope\": \"/instances/{instanceId}\", \"RoleDefinitionName\": \"Contributor\", \"RoleDefinitionId\": \"e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"ObjectId\": \"22222222-2222-2222-2222-222222222222\", \"ObjectType\": \"User\", \"DisplayName\": \"John Doe\", \"SignInName\": \"john.doe@contoso.com\", \"Description\": \"Platform administrator access\" } Properties Property Description RoleAssignmentName Unique GUID for the assignment RoleAssignmentId Full resource path including name Scope Resource identifier where assignment applies RoleDefinitionName Human-readable role name RoleDefinitionId Role definition GUID ObjectId Principal's unique identifier ObjectType User, Group, or ServicePrincipal DisplayName Principal's display name SignInName Principal's UPN Description Optional description Principal Types Type Description Use Case User Individual Entra ID user Named user access Group Entra ID security group Team/department access ServicePrincipal App registration or managed identity Automation/integration Assignment Examples Instance-Level Assignment Grant Contributor access to entire instance: { \"name\": \"unique-guid\", \"principal_id\": \"user-object-id\", \"principal_type\": \"User\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"scope\": \"/instances/{instanceId}\", \"description\": \"Platform contributor\" } Resource-Level Assignment Grant Reader access to specific agent: { \"name\": \"unique-guid\", \"principal_id\": \"group-object-id\", \"principal_type\": \"Group\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"scope\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/sales-agent\", \"description\": \"Sales team read access\" } Creating Assignments Via Management Portal Navigate to Security > Instance Access Control (or resource's Access Control) Click Add Role Assignment Click Browse to select principal Select role from dropdown Add description (optional) Click Save Via Management API POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{assignmentName} Content-Type: application/json Authorization: Bearer <token> { \"name\": \"55555555-4444-3333-2222-111111111111\", \"description\": \"Sales team agent access\", \"principal_id\": \"11111111-2222-3333-4444-555555555555\", \"principal_type\": \"Group\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"scope\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/sales-agent\" } Querying Assignments Filter by Scope POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/filter Content-Type: application/json Authorization: Bearer <token> { \"scope\": \"/instances/{instanceId}\" } Filter by Resource POST /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/filter Content-Type: application/json { \"scope\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/agents/my-agent\" } Deleting Assignments Via Management Portal Navigate to role assignments list Find the assignment Click Delete icon Via Management API DELETE /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{assignmentName} Authorization: Bearer <token> Inheritance Assignment Level Applies To Instance All resources in instance Resource Only that specific resource When viewing resource-level access control: This resource = Direct assignment Instance (inherited) = Inherited from instance Best Practices Practice Description Use Groups Assign to groups, not individuals Least Privilege Grant minimum required access Document Add descriptions to assignments Review Regularly Audit assignments periodically Scope Appropriately Use resource scope when possible Related Topics Role Definitions Scope Role Management"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/role-definitions.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/role-definitions.html",
    "title": "Role Definitions | FoundationaLLM",
    "summary": "Role Definitions A role definition is a collection of permissions that defines what actions can be performed. Role Definition Structure Property Description Name Display name of the role Id Unique identifier (GUID) Description Purpose of the role Actions Control plane actions allowed NotActions Actions excluded from Actions DataActions Data plane actions allowed NotDataActions Actions excluded from DataActions AssignableScopes Where the role can be assigned Built-in Roles Owner Full control over all resources, including role assignment management. { \"Name\": \"Owner\", \"Id\": \"1301f8d4-3bea-4880-945f-315dbd2ddb46\", \"Description\": \"Full access to manage all resources, including the ability to assign roles.\", \"Actions\": [\"*\"], \"NotActions\": [], \"AssignableScopes\": [\"/\"] } Contributor Manage all resources except role assignments. { \"Name\": \"Contributor\", \"Id\": \"e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"Description\": \"Manage everything except access to resources.\", \"Actions\": [\"*\"], \"NotActions\": [ \"FoundationaLLM.Authorization/*/delete\", \"FoundationaLLM.Authorization/*/write\" ], \"AssignableScopes\": [\"/\"] } Reader Read-only access to all resources. { \"Name\": \"Reader\", \"Id\": \"00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"Description\": \"Read-only access to all resources.\", \"Actions\": [\"*/read\"], \"NotActions\": [], \"AssignableScopes\": [\"/\"] } User Access Administrator Manage role assignments only. { \"Name\": \"User Access Administrator\", \"Id\": \"fb8e0fd0-f7e2-4957-89d6-19f44f7d6618\", \"Description\": \"Manage user access to resources.\", \"Actions\": [ \"FoundationaLLM.Authorization/roleAssignments/read\", \"FoundationaLLM.Authorization/roleAssignments/write\", \"FoundationaLLM.Authorization/roleAssignments/delete\" ], \"NotActions\": [], \"AssignableScopes\": [\"/\"] } Action Format Actions follow this pattern: FoundationaLLM.{ProviderName}/{resourceType}/{action} Examples Action Description FoundationaLLM.Agent/agents/read Read agents FoundationaLLM.Agent/agents/write Create/update agents FoundationaLLM.Agent/agents/delete Delete agents FoundationaLLM.Prompt/prompts/* All prompt actions */read Read all resources * All actions Wildcards Wildcard Meaning * All actions on all resources */read Read all resource types FoundationaLLM.Agent/* All Agent provider actions FoundationaLLM.Agent/agents/* All actions on agents Control vs Data Plane Control Plane Actions Specified in Actions and NotActions. Examples: Create/update/delete resources Manage configurations Manage role assignments Data Plane Actions Specified in DataActions and NotDataActions. Examples: Read resource content Execute operations Access data Important: Control plane access is NOT inherited to data plane. Having FoundationaLLM.Agent/agents/write does not grant FoundationaLLM.Agent/agents/read. Listing Role Definitions Via Management API GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions Authorization: Bearer <token> Via Azure CLI token=$(az account get-access-token \\ --scope api://FoundationaLLM-Management/Data.Manage \\ --query accessToken -o tsv) curl -H \"Authorization: Bearer $token\" \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions\" Role Selection Guide Use Case Recommended Role Full administration Owner Resource management only Contributor View resources only Reader Manage access only User Access Administrator Agent users Reader (on specific agents) Related Topics Role Assignments Scope Role Management"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/role-management.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/role-management.html",
    "title": "Managing Role Assignments | FoundationaLLM",
    "summary": "Managing Role Assignments This guide covers managing role assignments via the Management Portal and API. Prerequisites To manage role assignments, you must have: User Access Administrator role at the appropriate scope, OR Owner role Role ID: /providers/FoundationaLLM.Authorization/roleDefinitions/fb8e0fd0-f7e2-4957-89d6-19f44f7d6618 Management Portal Accessing Role Management Instance-Level: Navigate to Management Portal Select Security > Instance Access Control Resource-Level: Navigate to the resource (e.g., Agent) Click Access Control button Viewing Assignments The role assignment list shows: Column Description Name Role assignment name (expandable for details) Type User or Group Scope Instance or resource-level Delete Delete action Click the expand arrow to view full details. Creating Assignments Click Add Role Assignment Click Browse to search for identities Select user, group, or service principal The form auto-populates: Principal Type Principal Name Principal Email Principal ID Select Role from dropdown Add Description (optional) Click Save Deleting Assignments Find the assignment in the list Click the Delete icon Confirm deletion Note: You cannot edit assignments. Delete and recreate to make changes. Management API Authentication All API calls require: HTTPS Bearer token with Data.Manage scope User Access Administrator or Owner role Get token: az login az account get-access-token --scope api://FoundationaLLM-Management/Data.Manage Role Definition Endpoints Method Endpoint Description GET /instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions List all role definitions Role Assignment Endpoints Method Endpoint Description POST /.../roleAssignments/filter Query assignments by scope POST /.../roleAssignments/{name} Create assignment DELETE /.../roleAssignments/{name} Delete assignment Identity Endpoints Method Endpoint Description POST /instances/{instanceId}/identity/users/retrieve Search users POST /instances/{instanceId}/identity/groups/retrieve Search groups POST /instances/{instanceId}/identity/objects/retrievebyids Get objects by ID API Examples List Role Definitions curl -X GET \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleDefinitions\" \\ -H \"Authorization: Bearer $TOKEN\" Query Assignments curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/filter\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{\"scope\":\"/instances/{instanceId}\"}' Create Assignment curl -X POST \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{assignmentName}\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"unique-assignment-guid\", \"description\": \"Platform contributor access\", \"principal_id\": \"user-or-group-object-id\", \"principal_type\": \"User\", \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/e459c3a6-6b93-4062-85b3-fffc9fb253df\", \"scope\": \"/instances/{instanceId}\" }' Delete Assignment curl -X DELETE \\ \"https://<management-api>/instances/{instanceId}/providers/FoundationaLLM.Authorization/roleAssignments/{assignmentName}\" \\ -H \"Authorization: Bearer $TOKEN\" Search Users curl -X POST \\ \"https://<management-api>/instances/{instanceId}/identity/users/retrieve\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"john\", \"ids\": [], \"page_number\": 1, \"page_size\": 20 }' Search Groups curl -X POST \\ \"https://<management-api>/instances/{instanceId}/identity/groups/retrieve\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"sales\", \"ids\": [], \"page_number\": 1, \"page_size\": 20 }' Data Store Role assignments are stored in a dedicated data store: Isolated from main platform data Uses Azure Cosmos DB or Data Lake Storage Gen2 Enables independent scaling Supports compliance auditing Auditing All role assignment changes are audited: Creation timestamp Modification history User who made changes Previous values View audit history in Management Portal or query via API. Related Topics Role Definitions Role Assignments Scope Agent Role Assignments"
  },
  "docs/platform-operations/security-permissions/role-based-access-control/scope.html": {
    "href": "docs/platform-operations/security-permissions/role-based-access-control/scope.html",
    "title": "Understanding Scope | FoundationaLLM",
    "summary": "Understanding Scope Scope defines where a role assignment applies, controlling which resources a principal can access. Scope Levels Level Description Example Instance Entire FoundationaLLM deployment All agents, prompts, data sources Resource Specific resource Single agent Scope Hierarchy /instances/{instanceId} <- Instance scope /providers/{providerName} /{resourceType} /{resourceName} <- Resource scope /{subResourceType} /{subResourceName} <- Sub-resource scope Scope Format Scopes are resource identifiers following this pattern: /instances/{instanceId}/providers/{providerName}/{resourceType}/{resourceName} Components Component Description instanceId Unique GUID of your FoundationaLLM deployment providerName Resource provider (e.g., FoundationaLLM.Agent) resourceType Type of resource (e.g., agents) resourceName Name of specific resource Scope Examples Instance Scope /instances/11111111-1111-1111-1111-111111111111 Applies to all resources in the instance. Agent Scope /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.Agent/agents/sales-agent Applies only to the sales-agent agent. Data Source Scope /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.DataSource/dataSources/customer-data Applies only to the customer-data data source. Prompt Scope /instances/11111111-1111-1111-1111-111111111111/providers/FoundationaLLM.Prompt/prompts/support-prompt Applies only to the support-prompt prompt. Inheritance Rules Rule Description Hierarchical Child scopes inherit parent permissions Additive Lower scopes can add permissions No Reduction Cannot remove inherited permissions at lower levels Example Assignment Scope Effect Contributor @ Instance /instances/{id} Can manage all resources Reader @ Agent /instances/{id}/providers/.../agents/x Can read agent x A user with Contributor at instance level can manage agent x without needing the Reader assignment. Choosing Scope Scenario Recommended Scope Platform administrators Instance Department access to specific agents Resource (agent) Data team managing data sources Resource (data sources) Read-only access to everything Instance Scope in the Portal Instance Access Control Navigate to Security > Instance Access Control Assignments here apply to all resources Resource Access Control Navigate to specific resource (e.g., Agent) Click Access Control button Assignments here apply only to that resource Viewing Scope In role assignment lists, the Scope column shows: This resource - Direct assignment on current resource Instance (inherited) - Inherited from instance level Scope Best Practices Practice Description Start Narrow Grant at resource level when possible Use Instance Sparingly Only for true platform administrators Group Resources Consider organizational structure Document Scope Decisions Add descriptions explaining scope choice Related Topics Role Definitions Role Assignments Role Management"
  },
  "docs/platform-operations/security-permissions/vulnerabilities.html": {
    "href": "docs/platform-operations/security-permissions/vulnerabilities.html",
    "title": "Vulnerability Management | FoundationaLLM",
    "summary": "Vulnerability Management FoundationaLLM maintains a proactive approach to identifying, categorizing, and remediating security vulnerabilities. Vulnerability Identification Regular Security Testing Activity Frequency Purpose Red Team Exercises Periodic Identify attack vectors Container Scans Every build Detect CVEs Dependency Analysis Continuous Track vulnerable packages Penetration Testing Periodic External assessment Automated Scanning Every container image build includes: Base image vulnerability scanning Dependency analysis CVE detection Security best practice checks Severity Classification Severity Levels Level Description Response Time Critical Severe, imminent threat Immediate (hours) High Significant risk 24-48 hours Major Moderate risk 1 week Minor Low/negligible risk Next release cycle Classification Criteria Critical: Remote code execution Authentication bypass Data exfiltration potential Active exploitation in the wild High: Privilege escalation Significant data exposure Denial of service Requires immediate patching Major: Limited impact vulnerabilities Complex exploitation requirements Defense-in-depth failures Scheduled remediation Minor: Information disclosure (low impact) Configuration issues Best practice deviations Addressed in routine maintenance Remediation Process Critical/High Severity graph LR A[Detect] --> B[Assess] B --> C[Develop Fix] C --> D[Test] D --> E[Release Patch] E --> F[Notify Users] Detection - Automated or reported Assessment - Verify and classify Development - Create fix Testing - Validate fix Release - Deploy patch Notification - Inform users Standard Remediation Included in regular release cycles Documented in release notes Users upgrade at their schedule Communication GitHub Releases Primary channel for security updates: Information Location Security patches Release notes Vulnerability details Security advisories Upgrade instructions Release documentation Breaking changes Migration guides Release Notes Include Summary of vulnerabilities addressed Severity classification Affected versions Upgrade instructions Credit to reporters Responsible Disclosure We practice responsible disclosure: Credit researchers who report vulnerabilities Coordinate disclosure timing Provide details after patch availability Document in security advisories Reporting Vulnerabilities How to Report Email: security@foundationallm.ai GitHub: Security advisories (private) Do not disclose publicly before coordinated response Report Contents Please include: Description of the vulnerability Steps to reproduce Potential impact Suggested remediation (if any) Your contact information Response Timeline Action Timeline Acknowledge receipt 24-48 hours Initial assessment 1 week Status update Weekly Patch development Based on severity Coordinated disclosure After patch available Staying Updated Subscribe to Updates Watch GitHub repository Enable release notifications Monitor security advisories Version Management Track deployed versions Plan regular update cycles Prioritize security patches Customer Responsibilities Recommended Practices Practice Description Stay Current Apply updates promptly Monitor Releases Watch for security patches Test Updates Verify in staging first Report Issues Help improve security Update Checklist [ ] Review release notes [ ] Identify security fixes [ ] Plan update window [ ] Test in staging [ ] Deploy to production [ ] Verify functionality Related Topics Platform Security Updating Container Versions Creating Release Notes GitHub Releases"
  },
  "docs/release-notes/breaking-changes.html": {
    "href": "docs/release-notes/breaking-changes.html",
    "title": "History of breaking changes | FoundationaLLM",
    "summary": "History of breaking changes Note This section is for changes that are not yet released but will affect future releases. Starting from 0.9.7 Configuration changes TheFoundationaLLM:APIEndpoints:CoreAPI:Configuration:AllowedUploadFileExtensions configuration entry has been removed and was replaced by the FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:AllowedFileExtensions configuration entry. The following configuration entries have changed their default values: Name New value FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:AllowedFileExtensions c, cpp, cs, css, html, java, js, json, jsonl, md, php, py, rb, sh, tex, ts, txt, xml, yaml, yml, gif, jpeg, jpg, png, wav, pdf, docx, pptx, xlsx, csv, zip, tar, doc, ppt, xls FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchContextFileExtensions c, cpp, cs, css, html, java, js, json, jsonl, md, php, py, rb, sh, tex, ts, txt, xml, yaml, yml, gif, jpeg, jpg, png, wav FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchContextFileMaxSizeBytes {\"c, cpp, cs, css, html, java, js, json, jsonl, md, php, py, rb, sh, tex, ts, txt, xml, yaml, yml\": 1048576, \"gif, jpeg, jpg, png, wav\":20971520} FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchFileExtensions c, cpp, cs, css, html, java, js, json, jsonl, md, php, py, rb, sh, tex, ts, txt, xml, yaml, yml, gif, jpeg, jpg, png, wav, pdf, docx, pptx Starting from 0.9.7-rc487 Artifact changes The new data pipeline ShieldedFileContent is available. Role assignment changes The following role assignments must be added to the Context API's Managed Identity: Name Type Target Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. Starting from 0.9.7-rc477 Artifact changes A new workflow resource must be added to support the LangChainAgentWorkflow. Ensure the reference is added to _resource-references.json as well. { \"type\": \"langchain-agent-workflow\", \"name\": \"LangChainAgentWorkflow\", \"object_id\": \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.Agent/workflows/LangChainAgentWorkflow\", \"display_name\": \"LangChainAgentWorkflow\", \"description\": \"LangChain Agent workflow\", \"cost_center\": null, \"properties\": null, \"created_on\": \"2025-11-16T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"SYSTEM\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting from 0.9.7-rc472 Configuration changes A new feature flag named FoundationaLLM.Agent.SelfService has been added. This flag enables or disables the self-service capabilities for agents in the FoundationaLLM User Portal and the Core API. The default value is true. Starting from 0.9.7-rc470 Schema changes The FoundationaLLM.Agent resource provider schema is updated to version 2. The schema changes are as follows: The agent type knowledge-management is replaced with generic-agent. The agent workflow type external-agent-workflow is replaced with generic-agent-workflow. Artifact changes A new workflow resource must be added to support the GenericAgentWorkflow. Ensure the reference is added to _resource-references.json as well. { \"type\": \"generic-agent-workflow\", \"name\": \"GenericAgentWorkflow\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/GenericAgentWorkflow\", \"display_name\": \"Generic Agent Workflow\", \"description\": \"Generic Agent Workflow\", \"cost_center\": null, \"properties\": null, \"created_on\": \"2025-11-01T12:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"SYSTEM\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting from 0.9.7-rc422 The managed identity of Core API must have the Data Pipelines Execution Manager role assigned on the FoundationaLLM instance. Starting from 0.9.7-rc388 The external-modules-python container from the main storage account is obsolete and must be deleted. Starting from 0.9.7-rc382 Python packages must be built using the build version. Python packages must be deployed via the FoundationaLLM.Core PowerShell module. Starting from 0.9.7-rc376 Configuration changes The following App Configuration value have been added: Name Default value Description FoundationaLLM:UserPortal:Configuration:AgentManagementPermissionRequestUrl N/A The URL to request agent management permissions. FoundationaLLM:UserPortal:Configuration:FeaturedAgentNames N/A The comma-separated list of featured agent names. The following configuration values are obsolete and must be removed: FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:Modules FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:RootStorageContainer Permissions changes All User Portal users must have read permission on the /instances/<instance_id_>/providers/FoundationaLLM.Configuration/appConfigurationSets/UserPortal scope. AllAgentsVirtualSecurityGroup (5bb493a2-5909-4771-93ba-d83b7b5a1de9) must have read permission on the /instances/<instance_id_>/providers/FoundationaLLM.Configuration/appConfigurationSets/UserPortal scope. All Management Portal users must have read permissions on the /instances/<instance_id_>/providers/FoundationaLLM.Configuration/appConfigurationSets/ManagementPortal scope. Starting from 0.9.7-rc365 Configuration changes The following App Configuration value have been added: Name Default value Description FoundationaLLM:IdentityManagement:MicrosoftGraph:RetrieveOnPremisesAccountName false Indicates whether on-premises account names are retrieved by the Microsoft Graph API. Starting from 0.9.7 (up to rc364) Configuration changes The value of the FoundationaLLM:Events:Profiles:DataPipelineAPI configuration entry is changed to: { \"EventProcessingCycleSeconds\": 5, \"Topics\": [ { \"Name\": \"resource-providers\", \"SubscriptionPrefix\": \"rp-dp\" } ] } The FoundationaLLM:APIEndpoints:CoreAPI:Configuration:CompletionResponsePollingIntervalSeconds configuration entry is replaced by the FoundationaLLM:APIEndpoints:CoreAPI:Configuration:CompletionResponsePollingIntervalMilliseconds configuration entry. The default value is now 100 milliseconds instead of 1 second. This change is made to improve the responsiveness of the completion response polling mechanism. The values of the FoundationaLLM:APIEndpoints:ContextAPI:Configuration:KnowledgeGraphService:Embedding configuration value (listed below as a newly added value) must be configured with a content type of application/json and must have the following structure: { \"EmbeddingAPIEndpointConfigurationObjectId\": \"/instances/<instance_id>/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureOpenAI\", \"ModelDeployments\": { \"text-embedding-3-large\": \"embeddings-3-large\" } } where: <instance_id> is the identifier of the FoundationaLLM instance. It is assumed that the AzureOpenAI API endpoint configuration is already created. If not, AzureOpenAI must be replaced with the name of the API endpoint configuration that is used for embeddings. It is assumed that a model deployment named embeddings-3-large is already created in the Azure OpenAI service for the text-embedding-3-large embedding model. If not, it must be replaced with the name of the model deployment that is used for embeddings with the text-embedding-3-large embedding model. The following App Configuration value have been added: Name Default value Description FoundationaLLM:ResourceProviders:Context:Storage:AccountName <storage_account_name> The name of the storage account used by the FoundationaLLM Context resource provider. FoundationaLLM:ResourceProviders:Context:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Context resource provider to connect to the storage account. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:KnowledgeService:Embedding {\"EmbeddingAPIEndpointConfigurationObjectId\": null, \"ModelDeployments\": {}} The embedding configuration used by the FoundationaLLM Context API Knowledge service. This configuration value must have a content type of application/json. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:KnowledgeService:Storage:AccountName <storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Context API Knowledge service. This must be the same storage account as the one used by the FoundationaLLM Context API. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:KnowledgeService:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Context API Knowledge service to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:GatewayAPI:Configuration:TokenRateLimitMultiplier 0.8 The multiplier used to calculate the token rate limit for the Gateway API. This value is used to ensure that the rate limit is not exceeded and accounts for the possible differences in tokenization between the Gateway API and the deployed model(s). FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchFileExtensions c, cpp, cs, css, doc, docx, html, java, js, json, md, pdf, php, pptx, py, rb, sh, tex, ts, txt, gif, jpeg, jpg, png The comma-separated list file extensions that are processed as sources for knowledge search. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchContextFileExtensions c, cs, cpp, css, html, java, js, json, jsonl, md, php, py, rb, sh, ts, tex, txt, gif, jpeg, jpg, png The comma-separated list of file extensions that indicate files that can be directly used in the context of a completion request. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:KnowledgeSearchContextFileMaxSizeBytes {\"c,cs,cpp,html,java,js,json,jsonl,md,php,py,rb,sh,ts,tex,txt\": 25000, \"gif,jpeg,jpg,png\":500000} The dictionary of lists of extensions of files that can be directly used in the context of a completion request and their associated maximum file sizes. FoundationaLLM:DataPipeline:State:CosmosDB:Containers DataPipelines The names of the Azure Cosmos DB containers used by the Data Pipeline State service. FoundationaLLM:DataPipeline:State:CosmosDB:Database database The Azure Cosmos DB database name used by the Data Pipeline State service. FoundationaLLM:DataPipeline:State:CosmosDB:Endpoint <cosmos_db_endpoint> The endpoint URL of the Azure Cosmos DB used by the Data Pipeline State service. FoundationaLLM:DataPipeline:State:Storage:AccountName <storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Data Pipeline State service. This must be the same storage account as the one used by the FoundationaLLM Context API. FoundationaLLM:DataPipeline:State:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Data Pipeline State service to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:DataPipelineAPI:Configuration:Storage:AccountName <storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Data Pipeline API. FoundationaLLM:APIEndpoints:DataPipelineAPI:Configuration:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Data Pipeline API to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:DataPipelineAPI:Configuration:FrontendWorkerQueue frontend-worker The queue used to submit data pipeline work items for the Data Pipeline Frontend Worker service. FoundationaLLM:APIEndpoints:DataPipelineAPI:Configuration:BackendWorkerQueue backend-worker The queue used to submit data pipeline work items for the Data Pipeline Backend Worker service. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Configuration:Storage:AccountName <storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Data Pipeline Frontend Worker service. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Configuration:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Data Pipeline Frontend Worker service to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Configuration:Queue frontend-worker The queue used to process data pipeline work items by the Data Pipeline Frontend Worker service. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Configuration:ParallelProcessorsCount 10 The number of parallel processors of data pipeline run work items used by the Data Pipeline Frontend Worker service. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Configuration:Storage:AccountName <storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Data Pipeline Backend Worker service. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Configuration:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Data Pipeline Backend Worker service to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Configuration:Queue frontend-worker The queue used to process data pipeline work items by the Data Pipeline Backend Worker service. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Configuration:ParallelProcessorsCount 10 The number of parallel processors of data pipeline run work items used by the Data Pipeline Backend Worker service. FoundationaLLM:Events:Profiles:ContextAPI {\"EventProcessingCycleSeconds\":60,\"Topics\":[{\"Name\": \"resource-providers\", \"SubscriptionPrefix\": \"rp-context\"}]} The event processing settings for the Context API. FoundationaLLM:Events:Profiles:DataPipelineAPI {\"EventProcessingCycleSeconds\":60,\"Topics\":[]} The event processing settings for the Data Pipeline API. FoundationaLLM:Events:Profiles:DataPipelineFrontendWorker {\"EventProcessingCycleSeconds\":60,\"Topics\":[]} The event processing settings for the Data Pipeline Frontend Worker service. FoundationaLLM:Events:Profiles:DataPipelineBackendWorker {\"EventProcessingCycleSeconds\":60,\"Topics\":[]} The event processing settings for the Data Pipeline Backend Worker service. FoundationaLLM:ResourceProviders:Vector:Storage:AccountName <storage_account_name> The name of the storage account used by the FoundationaLLM Vector resource provider. FoundationaLLM:ResourceProviders:Vector:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Vector resource provider to connect to the storage account. FoundationaLLM:ResourceProviders:Plugin:Storage:AccountName <storage_account_name> The name of the storage account used by the FoundationaLLM Plugin resource provider. FoundationaLLM:ResourceProviders:Plugin:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Plugin resource provider to connect to the storage account. Important A new Azure Cosmos DB container named DataPipelines must be created with a parition key of /run_id and an autoscale transactional throughput of maximum 4000 RU/s. Important A new APIEndpointConfiguration artifact named DataPipelineAPI must be created and saved to a file named DataPipelineAPI.json. The file must be created in the FoundationaLLM.Configuration resource provider folder of the FoundationaLLM storage account. The file must contain the following content: { \"type\": \"api-endpoint\", \"name\": \"DataPipelineAPI\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/DataPipelineAPI\", \"display_name\": null, \"description\": null, \"cost_center\": null, \"category\": \"General\", \"subcategory\": null, \"authentication_type\": \"APIKey\", \"url\": \"<deployed_api_endpoint>\", \"status_endpoint\": \"/instances/<instance_id>/status\", \"url_exceptions\": [], \"authentication_parameters\": { \"api_key_configuration_name\": \"FoundationaLLM:APIEndpoints:DataPipelineAPI:Essentials:APIKey\", \"api_key_header_name\": \"X-API-KEY\" }, \"timeout_seconds\": 2400, \"retry_strategy_name\": \"ExponentialBackoff\", \"provider\": null, \"api_version\": null, \"operation_type\": null, \"properties\": null, \"created_on\": \"0001-01-01T00:00:00-05:00\", \"updated_on\": \"2025-02-25T11:10:55.3458874-05:00\", \"created_by\": null, \"updated_by\": \"ciprian@foundationaLLM.ai\", \"deleted\": false, \"expiration_date\": null } where <deployed_api_endpoint> is the endpoint of the Data Pipeline API. Also, a new entry must be added to the _resource-references.json file from the FoundationaLLM.Configuration resource provider folder with the following content: { \"Name\": \"DataPipelineAPI\", \"Filename\": \"/FoundationaLLM.Configuration/DataPipelineAPI.json\", \"Type\": \"api-endpoint\", \"Deleted\": false } Important Two new storage queues must be created in the FoundationaLLM storage account: frontend-worker - used by the Data Pipeline Frontend Worker to process requests. backend-worker - used by the Data Pipeline Backend Worker to process requests. The previous queues extract, partition, embed, and index are now deprecated. For each existing FoundationaLLM deployment, they should be removed as soon as it is confirmed that the previous version of vectorization process is no longer used. Important A new container with the name of the FoundationaLLM instance identifier must be created in the FoundationaLLM main storage account. The container will be used by the Data Pipeline API and the Data Pipeline Frontend/Backend Worker to store data pipeline work items. Role assignment changes The following role assignments must be added to the Management API's Managed Identity: Name Type Target Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. The following role assignments must be added to the Context API's Managed Identity: Name Type Target Reader FoundationaLLM FoundationaLLM instance. App Configuration Data Reader Azure Azure App Configuration service. Key Vault Secrets User Azure Azure Key Vault service. Storage Blob Data Contributor Azure Storage account used by the FoundationaLLM Context API. Storage Blob Data Contributor Azure Main FoundationaLLM storage account. Azure ContainerApps Session Executor Azure Azure Container Apps Dynamic Sessions used by the FoundationaLLM Context API. Azure ContainerApps Session Executor Azure Azure Container Apps Custom Container used by the FoundationaLLM Context API. EventGrid Contributor Azure Azure EventGrid Namespace used by the FoundationaLLM Data Pipeline API. Search Index Data Reader Azure Azure AI Search service used by the FoundationaLLM instance. The following role assignments must be added to the Orchestrations API's Managed Identity: Name Type Target Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. The following role assignments must be added to the Data Pipeline API's Managed Identity: Name Type Target App Configuration Data Reader Azure Azure App Configuration service. Key Vault Secrets User Azure Azure Key Vault service. Key Vault Certificate User Azure Azure Key Vault service. Storage Blob Data Contributor Azure Main FoundationaLLM storage account. Storage Blob Data Contributor Azure Storage account used by the FoundationaLLM Context API. Cosmos DB Built-in Data Contributor Azure Azure Cosmos DB account used by the FoundationaLLM Data Pipeline API. Storage Queue Data Message Sender Azure Storage account used by the FoundationaLLM Data Pipeline API. EventGrid Contributor Azure Azure EventGrid Namespace used by the FoundationaLLM Data Pipeline API. Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. Reader FoundationaLLM FoundationaLLM instance. The following role assignments must be added to the Data Pipeline Frontend Worker's Managed Identity: Name Type Target App Configuration Data Reader Azure Azure App Configuration service. Key Vault Secrets User Azure Azure Key Vault service. Key Vault Certificate User Azure Azure Key Vault service. Storage Blob Data Contributor Azure Main FoundationaLLM storage account. Storage Blob Data Contributor Azure Storage account used by the FoundationaLLM Context API. Cosmos DB Built-in Data Contributor Azure Azure Cosmos DB account used by the FoundationaLLM Data Pipeline Frontend Worker. Storage Queue Data Contributor Azure Storage account used by the FoundationaLLM Data Pipeline Frontend Worker. EventGrid Contributor Azure Azure EventGrid Namespace used by the FoundationaLLM Data Pipeline Frontend Worker. Search Service Contributor Azure Azure AI Search service used by the FoundationaLLM instance. Search Index Data Contributor Azure Azure AI Search service used by the FoundationaLLM instance. Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. Reader FoundationaLLM FoundationaLLM instance. The following role assignments must be added to the Data Pipeline Backend Worker's Managed Identity: Name Type Target App Configuration Data Reader Azure Azure App Configuration service. Key Vault Secrets User Azure Azure Key Vault service. Key Vault Certificate User Azure Azure Key Vault service. Storage Blob Data Contributor Azure Main FoundationaLLM storage account. Storage Blob Data Contributor Azure Storage account used by the FoundationaLLM Context API. Cosmos DB Built-in Data Contributor Azure Azure Cosmos DB account used by the FoundationaLLM Data Pipeline Backend Worker. Storage Queue Data Contributor Azure Storage account used by the FoundationaLLM Data Pipeline Backend Worker. EventGrid Contributor Azure Azure EventGrid Namespace used by the FoundationaLLM Data Pipeline Backend Worker. Search Service Contributor Azure Azure AI Search service used by the FoundationaLLM instance. Search Index Data Contributor Azure Azure AI Search service used by the FoundationaLLM instance. Data Pipelines Execution Manager FoundationaLLM FoundationaLLM instance. Reader FoundationaLLM FoundationaLLM instance. New resource providers FoundationaLLM.Vector The FoundationaLLM.Vector resource provider has been added. A new folder named FoundationaLLM.Vector must be created in the FoundationaLLM storage account under the resource-provider container. The folder must contain the following files: _resource-references.json - the resource references file. ConversationFiles.json - the resource file for the ConversationFiles vector database resource. The content of the resource-references.json file is as follows: { \"ResourceReferences\": [ { \"Name\": \"ConversationFiles\", \"Filename\": \"/FoundationaLLM.Vector/ConversationFiles.json\", \"Type\": \"vector-database\", \"Deleted\": false } ], \"DefaultResourceName\": null } The content of the ConversationFiles.json file is as follows: { \"type\": \"vector-database\", \"name\": \"ConversationFiles\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Vector/vectorDatabases/ConversationFiles\", \"display_name\": \"Conversation files vectorization\", \"description\": \"The vector database that used by default when processing files that are uploaded in conversations.\", \"cost_center\": null, \"database_type\": \"AzureAISearch\", \"database_name\": \"conversation-files\", \"embedding_property_name\": \"Embedding\", \"content_property_name\": \"Text\", \"vector_store_id_property_name\": \"VectorStoreId\", \"metadata_property_name\": \"Metadata\", \"api_endpoint_configuration_object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\", \"properties\": null, \"created_on\": \"2025-05-20T13:57:58.9770592+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"SYSTEM\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Important The configuration entries for the FoundationaLLM.Vector resource provider are specified above, in the Configuration changes section. FoundationaLLM.Plugin The FoundationaLLM.Plugin resource provider has been added. A new folder named FoundationaLLM.Plugin must be created in the FoundationaLLM storage account under the resource-provider container. Important The configuration entries for the FoundationaLLM.Plugin resource provider are specified above, in the Configuration changes section. Starting from 0.9.7-beta147 The following App Configuration value has been added: Name Default value Description FoundationaLLM:ResourceProviders:AzureAI:Storage:AccountName The name of the deployment storage account. The name of the Azure Blob Storage account used by the FoundationaLLM.AzureAI resource provider. FoundationaLLM:ResourceProviders:AzureAI:Storage:AuthenticationType AzureIdentity The authentication type used by the FoundationaLLM.AzureAI resource provider to connect to the storage account. Policy assignment changes The FLLM-Users will need an entry in the policy-assignments file for agentConversationMappings and agentFileMappings resources in the FoundationaLLM.AzureAI resource provider. { \"name\": \"87e6fd6e-ddd3-4054-b382-d7f0fa190aa9\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/87e6fd6e-ddd3-4054-b382-d7f0fa190aa9\", \"description\": \"Ownership on agent conversation mapping resources managed by the FoundationaLLM.AzureAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"c54871ba-1fa1-439a-9e86-30d74dfe4a4a\", \"principal_type\": \"Group\", \"scope\": \"/instances/{instanceId}/providers/FoundationaLLM.AzureAI/agentConversationMappings\", \"created_on\": \"2025-01-14T17:58:19.5954014Z\", \"updated_on\": \"2025-01-14T17:58:19.5954014Z\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" }, { \"name\": \"0e7684d2-e8b9-40ba-88b4-3f358f93afa3\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/0e7684d2-e8b9-40ba-88b4-3f358f93afa3\", \"description\": \"Ownership on agent file mapping resources managed by the FoundationaLLM.AzureAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"c54871ba-1fa1-439a-9e86-30d74dfe4a4a\", \"principal_type\": \"Group\", \"scope\": \"/instances/{instanceId}/providers/FoundationaLLM.AzureAI/agentFileMappings\", \"created_on\": \"2025-01-14T17:58:19.5954014Z\", \"updated_on\": \"2025-01-14T17:58:19.5954014Z\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" } Starting from 0.9.7-beta139 Configuration changes The following App Configuration value have been added: Name Default value Description FoundationaLLM:APIEndpoints:GatewayAPI:Configuration:AzureAIAgentServiceMaxVectorizationTimeSeconds 120 The maximum time in seconds allowed for an Azure AI Agent Service vectorization process to complete. FoundationaLLM:APIEndpoints:CoreAPI:Configuration:AzureAIAgentsFileSearchExensions c, cpp, cs, css, doc, docx, html, java, js, json, md, pdf, php, pptx, py, rb, sh, tex, ts, txt The comma-separated list file extensions that are supported by the Azure AI Agent Service file search tool. The following App Configuration value have been removed as they are no longer needed: ResourceProviders:AzureOpenAI:Storage:AuthenticationType ResourceProviders:AzureOpenAI:Storage:AccountName A new workflow resource must be added to support the AzureAIAgentServiceWorkflow. Ensure the reference is added to _resource-references.json as well. { \"type\": \"azure-ai-agent-service-workflow\", \"name\": \"AzureAIAgentServiceWorkflow\", \"object_id\": \"/instances/{instanceId}/providers/FoundationaLLM.Agent/workflows/AzureAIAgentServiceWorkflow\", \"display_name\": \"AzureAIAgentServiceWorkflow\", \"description\": \"Azure AI Agent Service Workflow\", \"cost_center\": null, \"properties\": null, \"created_on\": \"2024-11-08T10:08:27.1953263+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"SYSTEM\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting from 0.9.7-beta138 Configuration changes The FoundationaLLM:Code:CodeExecution:AzureContainerAppsDynamicSessions configuration setting has been replaced by the following configuration settings: Name Content Type Description Default value FoundationaLLM:Code:CodeExecution:AzureContainerAppsDynamicSessions:CodeInterpreter application/json The settings for the Azure Container Apps Dynamic Sessions code interpreter. { \"Endpoints\": { \"Python\": [], \"CSharp\": []} } FoundationaLLM:Code:CodeExecution:AzureContainerAppsDynamicSessions:CustomContainer application/json The settings for the Azure Container Apps Dynamic Sessions custom container. { \"Endpoints\": { \"Python\": [], \"CSharp\": []} } FoundationaLLM now supports Azure AI Inference APIs accessible to the agent using the LangChain workflow host. This functionality expects an Azure AI Service resource. Authentication is accomplished support is via Azure Identity (Entra) or API Key. If using Azure Identity, ensure the LangChain managed identity has Cognitive Services User role on the Azure AI Service resource (may be inherited). Within the project defined in AI Foundry, ensure the LangChain managed identity has the Azure AI Developer role. If using API Key, ensure the API Key is securely stored in the key vault and exposed via an Application Configuration value. When defining the API Endpoint Configuration, add the authentication parameter api_key_configuration_name with the name of the Application Configuration key. Code sessions The tool property foundationallm_aca_code_execution_enabled has been renamed to code_session_required. The property indicates whether the tool requires a code session during its execution. If the property is set to true, the following additional properties must be set: code_session_endpoint_provider: Supported values are AzureContainerAppsCodeInterpreter (indicates that the code session is provided by the Azure Container Apps Dynamic Sessions code interpreter) and AzureContainerAppsCustomContainer (indicates that the code session is provided by the Azure Container Apps Dynamic Sessions custom container). code_session_language: Supported values are Python and CSharp. Tools that require a code session should expect the following properties to be injected: code_session_endpoint (instead of the previous foundationallm_aca_code_execution_session_id) code_session_id (instead of the previous foundationallm_aca_code_execution_session_id) Starting from 0.9.7-beta128 Configuration changes Agent Workflow configuration now has a class_name field. This is the underlying implementation class of the workflow. This is not a breaking change, as in previous versions the name field contained the class name and if the class_name field is empty, it will default to the name field. However, it is recommended to set the class_name field to the implementation class name. Starting from 0.9.7-beta112 Configuration changes Added the following App Configuration value: Name Default value Description FoundationaLLM:APIEndpoints:DataPipelineAPI:Essentials:APIKey Points to foundationallm-apiendpoints-datapipelineapi-apikey KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:DataPipelineAPI:Essentials:AppInsightsConnectionString Points to foundationallm-appinsights-connectionstring KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Essentials:APIKey Points to foundationallm-apiendpoints-datapipelinefrontendworker-apikey KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:DataPipelineFrontendWorker:Essentials:AppInsightsConnectionString Points to foundationallm-appinsights-connectionstring KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Essentials:APIKey Points to foundationallm-apiendpoints-datapipelinebackendworker-apikey KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:DataPipelineBackendWorker:Essentials:AppInsightsConnectionString Points to foundationallm-appinsights-connectionstring KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:ContextAPI:Essentials:APIKey Points to foundationallm-apiendpoints-contextapi-apikey KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:ContextAPI:Essentials:AppInsightsConnectionString Points to foundationallm-appinsights-connectionstring KeyVault reference value pointing to the specified secret. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:Storage:AccountName <context_api_file_storage_account_name> The name of the dedicated storage account used by the FoundationaLLM Context API file service. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:Storage:AuthenticationType AzureIdentity The type of authentication used by the FoundationaLLM Context API file service to connect to the dedicated storage account. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:CosmosDB:Endpoint <cosmos_db_endpoint> The endpoint URL of the Azure Cosmos DB. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:CosmosDB:Database database The Azure Cosmos DB database name. FoundationaLLM:APIEndpoints:ContextAPI:Configuration:FileService:CosmosDB:Containers Context The list of Azure CosmosDB containers used by the Context API file service. Important A dedicated storage account for the FoundationaLLM Context API must be created. The account should be isolated in a dedicated resource group and permissions should only be granted to the FoundationaLLM Context API managed identity. The account should be configured the same way the main FoundationaLLM storage account is. A new storage container with a name equal to the FoundationaLLM instance identifier must be created in the storage account. Important A new Azure Cosmos DB container named Context must be created with a parition key of /upn and an autoscale transactional throughput of maximum 4000 RU/s. Important A new Azure Container Apps Session Pool must be created with in the same resource groups as the Context API storage account. Network egress should be disabled. The FoundationaLLM:Code:CodeExecution:AzureContainerAppsDynamicSessions must be set to the following value: {\"DynamicSessionsEndpoints\": [ \"<session_pool_endpoint>\" ]} where <session_pool_endpoint> is the endpoint of the Azure Container Apps Session Pool. Role assignment changes The following role assignments must be added to the Context API's Managed Identity: Name Target App Configuration Data Reader Azure App Configuration service. Key Vault Secrets User Azure Key Vault service. Storage Blob Data Contributor Dedicated storage account used by the FoundationaLLM Context API. Cosmos DB Built-in Data Contributor Azure Cosmos DB account used by the FoundationaLLM Context API. Azure ContainerApps Session Executor Azure Container Apps Session Pool. The following role assignments must be added to the LangChain API's Managed Identity: Name Target Azure ContainerApps Session Executor Azure Container Apps Session Pool. The following role assignments must be added to the Semantic Kernel API's Managed Identity: Name Target Azure ContainerApps Session Executor Azure Container Apps Session Pool. Starting from 0.9.7-beta107 For External Agent Workflows, the workflow invokation now includes conversation file history. The complete file history is passed regardless of conversation history settings. The signature is now as follows: async def invoke_async(self, operation_id: str, user_prompt: str, user_prompt_rewrite: Optional[str], message_history: List[MessageHistoryItem], file_history: List[FileHistoryItem])-> CompletionResponse: Starting from 0.9.7-beta106 For External Agent Workflows, the invocation of the workflow now sends the completion request MessageHistoryItem list vs the translated LangChain BaseMessage construct. This way workflows have more control on how to handle the message history of a conversation. Starting from 0.9.7-beta105 Configuration changes The FoundationaLLM.Common.Models.ResourceProviders.Agent.AgentTool definition now contains a ClassName property. This should be set to the implementation class name of the tool. The Name property of the tool is what will be used for uniqueness in the case multipe tools of the same type/class are used in the same agent. Note: This is not a breaking change as if the ClassName property is not set, the Name property will be used as the class name which is the existing pattern. However, it is recommended to set the ClassName property to adhere to the new pattern. Agent configuration tools property is impacted as follows: Old pattern sample: { \"name\": \"DALLEImageGeneration\", \"description\": \"Generates an image based on a prompt.\", \"package_name\": \"FoundationaLLM\", \"resource_object_ids\": { \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.AIModel/aiModels/DALLE3Model\": { \"object_id\": \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.AIModel/aiModels/DALLE3Model\", \"properties\": { \"object_role\": \"main_model\", \"model_parameters\": {} } } }, \"properties\": {} } New pattern: { \"name\": \"DALLETool1\", \"description\": \"Generates an image based on a prompt.\", \"package_name\": \"FoundationaLLM\", \"class_name\": \"DALLEImageGenerationTool\", \"resource_object_ids\": { \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.AIModel/aiModels/DALLE3Model\": { \"object_id\": \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.AIModel/aiModels/DALLE3Model\", \"properties\": { \"object_role\": \"main_model\", \"model_parameters\": {} } } }, \"properties\": {} } Starting from 0.9.7-beta103 Configuration changes Added the following App Configuration value: Name Default value Description FoundationaLLM:Quota:Storage:AccountName <storage_account_name> Provides the storage account used by the FoundationaLLM quota management service. FoundationaLLM:Quota:Storage:AuthenticationType AzureIdentity Indicates the authentication type used by the FoundationaLLM quota management service to connect to the storage account. A new container named quota must be created in the default storage account. The quota definitions are stored in the quota container in a file named quota-store.json. If the file does not exist, the file is automatically created. The file contains a list of quota definitions with the following structure: { \"name\": \"TestAPI01CompletionsUPNRawRequestRateLimit\", \"description\": \"Defines a per UPN raw request rate limit on the TestAPI01 Completions controller.\", \"context\": \"TestAPI01:Completions\", \"type\": \"RawRequestRateLimit\", \"metric_partition\": \"UserPrincipalName\", \"metric_limit\": 120, \"metric_window_seconds\": 60, \"lockout_duration_seconds\": 60, \"distributed_enforcement\": false } The following table provides details about the quota definition properties: Name Description Notes name The name of the quota definition. description A description of the quota definition. context The context of the quota definition. The format of the context is <service_name>:<controller_name> or <service_name>:<controller_name>:<agent_name>. Currently the following contexts can be used: CoreAPI:Completions, CoreAPI:Completions:<agent_name> where <agent_name> must be a valid agent name. type The type of the quota enforcement applied. The following types are supported: RawRequestRateLimit and AgentRequestRateLimit. RawRequestRateLimt defines the quota metric to be raw API requests and requires a context of <service_name>:<controller_name>. AgentRequestRateLimit defines the quota metric to be agent completion requests and requires a context of <service_name>:<controller_name>:<agent_name>. metric_partition The metric partition used to enforce the quota. The following partitions are supported: None (the metric is not partitioned) UserPrincipalName (the metric is partitioned by user principal name) and UserIdentifier (the metric is partitioned by user identifier). metric_limit The limit of the metric. The limit is enforced over the metric_window_seconds. In the example above, a maximum number of 120 raw API requests are allowed per user principal name in a 60-second window. metric_window_seconds The time window in seconds over which the limit is enforced. In the example above, a maximum number of 120 raw API requests are allowed per user principal name in a 60-second window. lockout_duration_seconds The duration in seconds for which the caller is locked out after exceeding the quota. The lockout duration is applied after the user exceeds the quota limit. The user is locked out for the specified duration before the quota is reset. distributed_enforcement Indicates whether the quota is enforced across multiple instances of the same API. If true, the quota is enforced across multiple instances. If false, the quota is enforced on a single instance. Currently, only false is supported. Starting from 0.9.7-beta101 Configuration changes Added the following App Configuration value: Name Default value Description FoundationaLLM:ResourceProviders:DataPipeline:Storage:AccountName <storage_account_name> Provides the storage account used by the FoundationaLLM.DataPipeline resource provider. FoundationaLLM:ResourceProviders:DataPipeline:Storage:AuthenticationType AzureIdentity Indicates the authentication type used by the FoundationaLLM.DataPipeline resource provider to connect to the storage account. FoundationaLLM:ResourceProviders:Plugin:Storage:AccountName <storage_account_name> Provides the storage account used by the FoundationaLLM.Plugin resource provider. FoundationaLLM:ResourceProviders:Plugin:Storage:AuthenticationType AzureIdentity Indicates the authentication type used by the FoundationaLLM.Plugin resource provider to connect to the storage account. Management API The POST /instances/{instanceId}/providers/{resourceProvider}/{resourcePath} endpoint now supports providing a resource form data key in the request body. This key is used to provide the serialized resource FoundationaLLM resource when a file upload is performed using a form-data request body. The call to POST /instances/{instanceId}/providers/FoundationaLLM.Plugin/pluginPackages/Dotnet-FoundationaLLMDataPipelinePlugins expects a form-data request body with the following keys: file (of type file) - the plugin package file to upload. resource (of type text) - the serialized plugin package FoundationaLLM resource. Starting with 0.9.7-beta100 Configuration changes App configuration settings The value of the FoundationaLLM:Events:Profiles:CoreAPI must be updated to include the api-statistics topic. The updated value is as follows: { \"EventProcessingCycleSeconds\": 5, \"Topics\": [ { \"Name\": \"resource-providers\", \"SubscriptionPrefix\": \"rp-core\" }, { \"Name\": \"api-statistics\", \"SubscriptionPrefix\": \"as-core\" } ] } Starting with 0.9.4-rc100 Configuration changes App configuration settings Important The App Config setting FoundationaLLM:Instance:EnableResourceProvidersCache is obsolete and should be removed from the App Config settings. The following App Config properties make cache settings for the resource providers configurable: Name Description Default Value FoundationaLLM:ResourceProvidersCache:EnableCache Indicates whether resource providers should cache resources or not. true FoundationaLLM:ResourceProvidersCache:AbsoluteCacheExpirationSeconds Absolute cache expiration in seconds. 300 FoundationaLLM:ResourceProvidersCache:SlidingCacheExpirationSeconds Sets how many seconds the cache entry can be inactive (e.g. not accessed) before it will be removed. This will not extend the entry lifetime beyond the absolute expiration (if set). 120 FoundationaLLM:ResourceProvidersCache:CacheSizeLimit The maximum number of items that can be stored in the cache. 10000 FoundationaLLM:ResourceProvidersCache:CacheExpirationScanFrequencySeconds Gets or sets the minimum length of time between successive scans for expired items in seconds. 30 Starting with 0.9.3 This version introduces the concept of a well-known virtual security group (AllAgentsVirtualSecurityGroup) that is used by agents using Agent Access Token authentication and have their own virtual security group defined. Assign the following PBAC and RBAC roles to the AllAgentsVirtualSecurityGroup (replace the tokens denoted by {{...}} with the actual values): PBAC changes { \"name\": \"{{pbacConversationsOwnerGuid}}\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/{{pbacConversationsOwnerGuid}}\", \"description\": \"Ownership on conversation resources for AllAgentsVirtualSecurityGroup by the FoundationaLLM.Conversation resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Conversation/conversations\", \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" }, { \"name\": \"{{pbacConversationMappingsGuid}}\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/{{pbacConversationMappingsGuid}}\", \"description\": \"Ownership on conversation mapping resources for AllAgentsVirtualSecurityGroup managed by the FoundationaLLM.AzureOpenAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.AzureOpenAI/conversationMappings\", \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" }, { \"name\": \"{{pbacAttachmentsOwnerGuid}}\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/{{pbacAttachmentsOwnerGuid}}\", \"description\": \"Ownership on attachment resources for AllAgentsVirtualSecurityGroup managed by the FoundationaLLM.Attachment resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Attachment/attachments\", \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" }, { \"name\": \"{{pbacFileMappingsGuid}}\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/{{pbacFileMappingsGuid}}\", \"description\": \"Ownership on file mapping resources for AllAgentsVirtualSecurityGroup managed by the FoundationaLLM.AzureOpenAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.AzureOpenAI/fileMappings\", \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" } RBAC changes { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{openAiAssistantsReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{openAiAssistantsReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for OpenAIAssistants for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/workflows/OpenAIAssistants\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{langGraphReactAgentReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{langGraphReactAgentReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for LangGraphReactAgent for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/workflows/LangGraphReactAgent\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{attachmentContributorGuid2}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{attachmentContributorGuid2}}\", \"display_name\": null, \"description\": \"Attachment contributor role for AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/8e77fb6a-7a78-43e1-b628-d9e2285fe25a\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{conversationContributorGuid2}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{conversationContributorGuid2}}\", \"display_name\": null, \"description\": \"Conversation contributor role for AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/d0d21b90-5317-499a-9208-3a6cb71b84f9\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{configReadAccessGuid3}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{configReadAccessGuid3}}\", \"display_name\": null, \"description\": \"Read Access for configuration for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Configuration/appConfigurations/FoundationaLLM:APIEndpoints:CoreAPI:Configuration:MaxUploadsPerMessage\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{configReadAccessGuid4}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{configReadAccessGuid4}}\", \"display_name\": null, \"description\": \"Read Access for configuration for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Configuration/appConfigurations/FoundationaLLM:APIEndpoints:CoreAPI:Configuration:CompletionResponsePollingIntervalSeconds\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{externalAgentWorkflowReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{externalAgentWorkflowReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for ExternalAgentWorkflow for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/workflows/ExternalAgentWorkflow\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{langChainExpressionLanguageReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{langChainExpressionLanguageReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for LangChainExpressionLanguage for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/workflows/LangChainExpressionLanguage\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{openAIAssistantsFileSearchReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{openAIAssistantsFileSearchReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for OpenAIAssistantsFileSearch for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/tools/OpenAIAssistantsFileSearch\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{openAIAssistantsCodeInterpreterReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{openAIAssistantsCodeInterpreterReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for OpenAIAssistantsCodeInterpreter for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/tools/OpenAIAssistantsCodeInterpreter\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{dalleImageGenerationReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{dalleImageGenerationReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for DALLEImageGeneration for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/tools/DALLEImageGeneration\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null }, { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"{{foundationaLLMContentSearchToolReaderGuid}}\", \"object_id\": \"/providers/FoundationaLLM.Authorization/roleAssignments/{{foundationaLLMContentSearchToolReaderGuid}}\", \"display_name\": null, \"description\": \"Read Access for FoundationaLLMContentSearchTool for the AllAgentsVirtualSecurityGroup group.\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/00a53e72-f66e-4c03-8f81-7e885fd2eb35\", \"principal_id\": \"5bb493a2-5909-4771-93ba-d83b7b5a1de9\", \"principal_type\": \"Group\", \"scope\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/tools/FoundationaLLMContentSearchTool\", \"properties\": null, \"created_on\": \"{{deployTime}}\", \"updated_on\": \"{{deployTime}}\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting with 0.9.3-rc016 Configuration Resource Provider The APIEnpointConfiguration class has been updated to change the previous StatusUrl property to StatusEndpoint, which is a relative path to the status endpoint. By extension, the related JSON files now have a status_endpoint property that contains the relative path. Here is the OrchestrationAPI JSON template as an example of this change: { \"type\": \"api-endpoint\", \"name\": \"OrchestrationAPI\", \"object_id\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/OrchestrationAPI\", \"display_name\": null, \"description\": null, \"cost_center\": null, \"category\": \"General\", \"authentication_type\": \"APIKey\", \"authentication_parameters\": { \"api_key_configuration_name\": \"FoundationaLLM:APIEndpoints:OrchestrationAPI:Essentials:APIKey\", \"api_key_header_name\": \"X-API-KEY\" }, \"url\": \"http://orchestration-api.{{serviceNamespaceName}}.svc.cluster.local\", \"status_endpoint\": \"/instances/{{instanceId}}/status\", \"url_exceptions\": [], \"timeout_seconds\": 2400, \"retry_strategy_name\": \"ExponentialBackoff\", \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": \"SYSTEM\", \"deleted\": false } The status path is used by the Management Portal's Deployment Information page to show the status of each of the APIs. This path is also used by the /Orchestration/Services/LangChainService and /Orchestration/Services/SemanticKernelService classes to check the status of the respective APIs. Important All files within the /resource-provider/FoundationaLLM.Configuration directory must be updated to change the name of the status_url field to status_endpoint and change the value to a relative path as needed. Vectorization resource provider changes Vectorization indexing and partitioning profile settings dictionary keys are now persisted as snake case (ex. IndexName becomes index_name). Agent resource provider changes Deployment notes Ensure the feature flag is enabled for FoundationaLLM.Agent.PrivateStore if using the private store feature on OpenAI Assistants workflow agents is desired. Open existing OpenAIAssistantsWorkflow agents in the Management portal and select Save to populate the global vector store in the OpenAI service for the assistant. Agent file resources The agent file references are now stored in a new Cosmos DB container, while the file contents are stored in the storage account. Here are the configuration parameters for the required Cosmos DB container: Name Value Name Agents Maximum RU/s 4000 Hierarchical Partition key /instanceId + /agentName As a result of the migration, the newly created Agents container will initially contain only ony type of times: AgentFileReference. This is an example of such item: { \"instanceId\": \"8ac6074c-bdde-43cb-a140-ec0002d96d2b\", \"agentName\": \"TestAgentFiles1\", \"originalFilename\": \"curious_cat_story.pdf\", \"contentType\": \"application/pdf\", \"size\": 2433, \"upn\": \"andrei@foundationaLLM.ai\", \"id\": \"af-0285ddb8-a5b8-48b0-8248-bd0ad2f123bf\", \"objectId\": \"/instances/8ac6074c-bdde-43cb-a140-ec0002d96d2b/providers/FoundationaLLM.Agent/agents/TestAgentFiles1/agentFiles/af-0285ddb8-a5b8-48b0-8248-bd0ad2f123bf\", \"name\": \"af-0285ddb8-a5b8-48b0-8248-bd0ad2f123bf\", \"filename\": \"/FoundationaLLM.Agent/8ac6074c-bdde-43cb-a140-ec0002d96d2b/TestAgentFiles1/private-file-store/af-0285ddb8-a5b8-48b0-8248-bd0ad2f123bf.pdf\", \"type\": \"agent-file\", \"deleted\": false, \"_rid\": \"ie9IAMu0+b0EAAAAAAAAAA==\", \"_self\": \"dbs/ie9IAA==/colls/ie9IAMu0+b0=/docs/ie9IAMu0+b0EAAAAAAAAAA==/\", \"_etag\": \"\\\"37012abc-0000-0200-0000-67afaa800000\\\"\", \"_attachments\": \"attachments/\", \"_ts\": 1739565696 } The agent-file type has been removed and the references are no longer saved in the agent reference store _resource-references.json. Tools In the agent resource provider storage folder (resource-provider/FoundationaLLM.Agent), add a tool resource reference entry (_resource_references.json) as well as configuration file for the following tools: OpenAIAssistantsFileSearch, OpenAIAssistantsCodeInterpreter, DALLEImageGeneration, and FoundationaLLMContentSearchTool. Reference example: { \"Name\": \"OpenAIAssistantsFileSearch\", \"Filename\": \"/FoundationaLLM.Agent/OpenAIAssistantsFileSearch.json\", \"Type\": \"tool\", \"Deleted\": false } File example: { \"type\": \"tool\", \"name\": \"OpenAIAssistantsFileSearch\", \"object_id\": \"/instances/{{instanceId}}/providers/FoundationaLLM.Agent/tools/OpenAIAssistantsFileSearch\", \"display_name\": \"OpenAIAssistantsFileSearch\", \"description\": \"OpenAIAssistantsFileSearch\", \"cost_center\": null, \"properties\": null, \"created_on\": \"2025-01-10T08:22:34.2682433+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"dev@foundationaLLM.ai\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting with 0.9.3-rc010 Resource provider cache warm-up Resource providers now support a cache warm-up mechanism. This mechanism allows the cache to be pre-populated with the resource provider data before the service starts processing requests. This feature is useful when the service is deployed in a cold environment and needs to be warmed up before it can handle requests. The cache warm-up mechanism is enabled when a file named _cache_warmup.json exists in the blob storage location associated with the resource provider. Here is an example of such a file: [ { \"ServiceName\": \"OrchestrationAPI\", \"Description\": \"Resources required by: service principal x, service principal y.\", \"ResourceObjectIds\": [ \"/instances/73fad442-.../providers/FoundationaLLM.Configuration/apiEndpointConfigurations/GatewayAPI\", \"/instances/73fad442-.../providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureAISearch\", \"/instances/73fad442-.../providers/FoundationaLLM.Configuration/apiEndpointConfigurations/LangChainAPI\", \"/instances/73fad442-.../providers/FoundationaLLM.Configuration/apiEndpointConfigurations/StateAPI\" ], \"SecurityPrincipalIds\": [ \"4150c6b3-...\", \"949195b1-...\" ] }, { \"ServiceName\": \"OrchestrationAPI\", \"Description\": \"Resources required by: service principal x, service principal y, service principal z.\", \"ResourceObjectIds\": [ \"/instances/73fad442-.../providers/FoundationaLLM.Configuration/apiEndpointConfigurations/AzureOpenAI\" ], \"SecurityPrincipalIds\": [ \"4150c6b3-...\", \"949195b1-...\", \"d6a6317a-...\" ] } ] The configuration contains an array of objects, each representing a cache warm-up configuration. Each object contains the following properties: ServiceName - The name of the service that the cache warm-up configuration is for. Description - A description of the cache warm-up configuration. ResourceObjectIds - The list of resource object identifiers that will be pre-loaded into the resource provider cache. Note The resource object identifiers must be specific to the resource provider. SecurityPrincipalIds - The list of security principal identifiers that will be used to authenticate the cache warm-up requests. Important As a result of the cache warm-up process, the client authorization cache will be populated with all combinations of security principal and resource object identifiers that exist in the cache warm-up configuration. Make sure the two lists only contain the necessary values to avoid a long startup time for the resource provider. Starting with 0.9.3-rc002 App configuration settings The following App Config properties make cache settings for the AuthorizationServiceClientCacheService configurable: Name Description Default Value FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:EnableCache Indicates whether calls to the Authorization API should be cached or not. false FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:AbsoluteCacheExpirationSeconds Absolute cache expiration in seconds. 300 FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:SlidingCacheExpirationSeconds Sets how many seconds the cache entry can be inactive (e.g. not accessed) before it will be removed. This will not extend the entry lifetime beyond the absolute expiration (if set). 120 FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:CacheSizeLimit The maximum number of items that can be stored in the cache. 10000 FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:CacheExpirationScanFrequencySeconds Gets or sets the minimum length of time between successive scans for expired items in seconds. 30 Starting with 0.9.2-rc005 Agent configuration changes Starting with this version, all agents MUST transition to the agent workflow configuraiton approach. The following agent properties are no longer supported and should be deleted as part of upgrading to this version: OrchestrationSettings - fully replaced by the agent worflow settings. PromptObjectId - replaced by the agent workflow resource object identifier with an object_role of main_prompt. AIModelObjectId - replaced by the agent workflow resource object identifier with an object_role of main_model. Capabilities - removed. The equivalent of having Azure OpenAI Assistants capabilities is having an agent workflow with the type azure-openai-assistants-workflow. Azure.OpenAI.Assistant.Id property in properties - replaced by the assistant_id property of an agent workflow witht the type azure-openai-assistants-workflow. Important If the Azure.OpenAI.Assistant.Id property is set in the agent properties, it's value must be copied to the assistant_id property of the agent workflow. Here is an example of a fully configured worfklow section for an agent: { \"type\": \"azure-openai-assistants-workflow\", \"name\": \"OpenAIAssistants\", \"package_name\": \"FoundationaLLM\", \"assistant_id\": \"asst_...\", \"resource_object_ids\": { \"/instances/.../providers/FoundationaLLM.Agent/workflows/OpenAIAssistants\": { \"object_id\": \"/instances/.../providers/FoundationaLLM.Agent/workflows/OpenAIAssistants\", \"properties\": {} }, \"/instances/.../providers/FoundationaLLM.AIModel/aiModels/GPT4oMiniCompletionAIModel\" : { \"object_id\": \"/instances/.../providers/FoundationaLLM.AIModel/aiModels/GPT4oMiniCompletionAIModel\", \"properties\": { \"object_role\": \"main_model\", \"model_parameters\": {} } }, \"/instances/.../providers/FoundationaLLM.Prompt/prompts/FoundationaLLM-mini\": { \"object_id\": \"/instances/.../providers/FoundationaLLM.Prompt/prompts/FoundationaLLM-mini\", \"properties\": { \"object_role\": \"main_prompt\" } } } } Starting from 0.9.1 App configuration settings To support the event grid infrastructure, the following new App Configuration settings are required. [ { \"key\": \"FoundationaLLM:Events:Profiles:CoreAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\": 5,\\\"Topics\\\": [{\\\"Name\\\": \\\"resource-providers\\\",\\\"SubscriptionPrefix\\\": \\\"rp-core\\\"}]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:GatekeeperAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\":60,\\\"Topics\\\":[]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:GatewayAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\": 5,\\\"Topics\\\": [{\\\"Name\\\": \\\"resource-providers\\\",\\\"SubscriptionPrefix\\\": \\\"rp-gateway\\\"}]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:ManagementAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\": 5,\\\"Topics\\\": [{\\\"Name\\\": \\\"resource-providers\\\",\\\"SubscriptionPrefix\\\": \\\"rp-management\\\"}]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:OrchestrationAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\": 5,\\\"Topics\\\": [{\\\"Name\\\": \\\"resource-providers\\\",\\\"SubscriptionPrefix\\\": \\\"rp-orch\\\"}]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:VectorizationAPI\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\":60,\\\"Topics\\\":[]}\", \"content_type\": \"application/json\", \"tags\": {} }, { \"key\": \"FoundationaLLM:Events:Profiles:VectorizationWorker\", \"label\": null, \"value\": \"{\\\"EventProcessingCycleSeconds\\\":60,\\\"Topics\\\":[]}\", \"content_type\": \"application/json\", \"tags\": {} } ] Note: The event grid system topics need to be removed. The following topic needs to be created in the event grid namespace, must have a resource-providers topic with a publisher type of Custom and an input schema of Cloud Events v1.0. Configuration changes Added the following App Configuration value: Name Default value Description FoundationaLLM:UserPortal:Authentication:Entra:TimeoutInMinutes 60 The timeout in minutes for a user's auth token in the User Portal. FoundationaLLM:UserPortal:Configuration:ShowFileUpload true Global setting to determine if file upload is allowed on chat messages. Starting with 0.9.1-rc117 Agent configuration changes \"text_rewrite_settings\": { \"user_prompt_rewrite_enabled\" : true, \"user_prompt_rewrite_settings\": { \"user_prompt_rewrite_ai_model_object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.AIModel/aiModels/GPT4oCompletionAIModel\", \"user_prompt_rewrite_prompt_object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.Prompt/prompts/FoundationaLLM-v2-Rewrite\", \"user_prompts_window_size\": 1 } }, \"cache_settings\": { \"semantic_cache_enabled\": true, \"semantic_cache_settings\": { \"embedding_ai_model_object_id\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.AIModel/aiModels/DefaultEmbeddingAIModel\", \"embedding_dimensions\": 2048, \"minimum_similarity_threshold\": 0.975 } }, Semantic cache Enable vector search in the Cosmos DB database using the following CLI command: az cosmosdb update --resource-group <resource-group-name> --name <account-name> --capabilities EnableNoSQLVectorSearch Create the CompletionsCache container in the Cosmos DB database with the following properties: Container id: CompletionsCache Partition key: /operationId Container Vector Policy: a policy with the following properties: Path: /userPromptEmbedding Data type: float32 Distance function: Cosine Dimensions: 2048 Index type: diskANN (leave the default values) After the container is created, set the Time to Live property on the container to 300 seconds. Starting with 0.9.1-rc105 Configuration changes The following new App Configuration settings are required: Name Default value Description FoundationaLLM:PythonSDK:Logging:LogLevel:Azure Warning Provides the default level of logging for Azure modules in the Python SDK. Agent workflow configuration changes Agent resource configuration files that have a workflow property now requires a name and package_name property. This is to support loading external workflows via plugins. For internal workflows, the package_name should be set to FoundationaLLM. Example below truncated for brevity. { \"workflow\": { \"type\": \"langgraph-react-agent-workflow\", \"name\": \"LangGraphReactAgent\", \"package_name\": \"FoundationaLLM\", \"workflow_host\": \"LangChain\", \"graph_recursion_limit\": 10, \"resource_object_ids\": {} } } A new Workflow resource must be added to the FoundationaLLM.Agent resource provider: { \"type\": \"external-agent-workflow\", \"name\": \"ExternalAgentWorkflow\", \"object_id\": \"/instances/<instance_id>/providers/FoundationaLLM.Agent/workflows/ExternalAgentWorkflow\", \"display_name\": \"ExternalAgentWorkflow\", \"description\": \"External Agent workflow\", \"cost_center\": null, \"properties\": null, \"created_on\": \"2024-11-13T18:12:07.0223039+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": \"dev@foundationaLLM.ai\", \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting with 0.9.1-rc102 Configuration changes The following new App Configuration settings are required: Name Default value Description FoundationaLLM:APIEndpoints:OrchestrationAPI:Configuration:CompletionRequestsStorage:AccountName <main_storage_account_name> Provides the storage account used by the Orchestration API to persist completion requests. FoundationaLLM:APIEndpoints:OrchestrationAPI:Configuration:CompletionRequestsStorage:AuthenticationType AzureIdentity Indicates that managed identity authentication should be used to access the storage account. FoundationaLLM:APIEndpoints:OrchestrationAPI:Configuration:CompletionRequestsStorage:ContainerName orchestration-completion-requests Provides the storage container name used by the Orchestration API to persist completion requests. Should always be orchestration-completion-requests User profile changes A new flag named persistOrchestrationCompletionRequests is added to the user profile. This flag is used to determine whether the user's completion requests should be persisted in the storage account. The default value is false. Sample configuration: \"flags\": { \"oneDriveWorkSchoolEnabled\": true, \"persistOrchestrationCompletionRequests\": true }, Starting with 0.9.1-rc101 Configuration changes The following new App Configuration settings are required: Name Default value Description FoundationaLLM:Code:CodeExecution:AzureContainerAppsDynamicSessions {\"DynamicSessionsEndpoints\": []} Provides the configuration for the Azure Container Apps Dynamic Sessions code execution service. DynamicSessionsEnpoints is a list of Dynamic Sessions endpoints that are used to run code execution sessions. Must contain at least one value. Agent tool configuration changes Each agent tool should have an entry in the properties dictionary named foundationallm_aca_code_execution_enabled (true or false) to indicate whether the tool requires code execution sessions based on the the Azure Container Apps Dynamic Sessions service. Prompt definition changes Prompt prefixes and suffixes support FoundationaLLM variables for dynamic replacement at runtime. The variable format is {{foundationallm:variable_name[:format]}} where variable_name is the name of the well-known variable. format is the optional formatting applied to the value of the variable. The following variables are supported: Name Value Example current_datetime_utc The current UTC date and time. The current date is {{foundationallm:current_datetime_utc:dddd, MMMM dd, yyyy}}. This looks great. -> The current date is Sunday, December 15, 2024. This looks great. Starting with 0.9.0 Configuration changes The following new App Configuration settings are required: Name Default value Description FoundationaLLM:PythonSDK:Logging:LogLevel:Default Information - FoundationaLLM:PythonSDK:Logging:EnableConsoleLogging false - FoundationaLLM:APIEndpoints:CoreAPI:Configuration:Entra:RequireScopes true Indicates whether a scope claim (scp) is required for authorization. Set to false to allow authentication from an external proxy API. FoundationaLLM:APIEndpoints:CoreAPI:Configuration:Entra:AllowACLAuthorization false Indicates whether tokens that do not have either of the \"scp\" or \"roles\" claims are accepted (True means they are accepted). Set to true to allow authentication from an external proxy API. FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:Storage:AccountName - - FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:Storage:AuthenticationType - - FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:RootStorageContainer - - FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:ExternalModules:Modules - - FoundationaLLM:APIEndpoints:LangChainAPI:Configuration:PollingIntervalSeconds 1 The interval in seconds at which the LangChain API will be polled for status. FoundationaLLM:UserPortal:Configuration:ShowMessageRating true If true, rating options on agent messages will appear. FoundationaLLM:UserPortal:Configuration:ShowLastConversationOnStartup false If true, the last conversation will be displayed when the user logs in. Otherwise, a new conversation placeholder appears on page load. FoundationaLLM:UserPortal:Configuration:ShowMessageTokens true If true, the number of consumed tokens on agent and user messages will appear. FoundationaLLM:UserPortal:Configuration:ShowViewPrompt true If true, the \"View Prompt\" button on agent messages will appear. FoundationaLLM:Instance:EnableResourceProvidersCache false If true, the caching of resource providers will be enabled. FoundationaLLM:APIEndpoints:AuthorizationAPI:Essentials:EnableCache false If true, the caching of authorization call results will be enabled. Agent Tool configuration changes Agent tools are now an array of AgentTool objects rather than a dictionary. When defining tools for an agent, each tool now requires a package_name property. This property is used to identify the package that contains the tool's implementation. If the tool is internal, the package_name should be set to FoundationaLLM, if the tool is external, the package_name should be set to the name of the external package. Security-related changes The Authorization API now requires the ability to write to the Key Vault account contained within the auth resource group. Currently, the Authorization APIs managed identity is assigned to the Key Vault Secrets User role on the Key Vault account. This role assignment must be updated to include the Key Vault Secrets Officer role in addition to the user role. Renamed classes The following classes have been renamed: Original Class New Class FoundationaLLM.Common.Models.Orchestration.Response.Citation FoundationaLLM.Common.Models.Orchestration.Response.ContentArtifact API endpoint changes Core API The /instances/{instanceId}/sessions/{sessionId}/message/{id}/rate endpoint has been updated to accept the rating in the message body, rather than as a query parameter. Send the following payload in the request body: { \"rating\": true, \"comments\": \"string\" } Note Please note that both properties are nullable. Set them to null to clear out the rating and comments. Starting with 0.8.4 Configuration changes The following new App Configuration settings are required: Name Default value FoundationaLLM:APIEndpoints:ManagementAPI:Configuration:AllowedUploadFileExtensions c, cpp, cs, css, csv, doc, docx, gif, html, java, jpeg, jpg, js, json, md, pdf, php, png, pptx, py, rb, sh, tar, tex, ts, txt, xlsx, xml, zip FoundationaLLM:Branding:NoAgentsMessage No agents available. Please check with your system administrator for assistance. FoundationaLLM:Branding:DefaultAgentWelcomeMessage Start the conversation using the text box below. The following new App Configuration feature flags are required: Name Default value FoundationaLLM.Agent.PrivateStore Not enabled Assistants API enabled Agent(s) Important Any existing agent that has the Assistants API enabled needs to be saved from the Management UI to update itself. Resource provider changes FoundationaLLM.Authorization The following entries need to be added to the policy store file: { \"name\": \"GUID03\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/GUID03\", \"description\": \"Ownership on conversation mapping resources managed by the FoundationaLLM.AzureOpenAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"SECURITY_GROUP_ID\", \"principal_type\": \"Group\", \"scope\": \"/instances/FOUNDATIONALLM_INSTANCEID/providers/FoundationaLLM.AzureOpenAI/conversationMappings\", \"created_on\": \"DEPLOY_TIME\", \"updated_on\": \"DEPLOY_TIME\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" }, { \"name\": \"GUID04\", \"type\": \"FoundationaLLM.Authorization/policyAssignments\", \"object_id\": \"/providers/FoundationaLLM.Authorization/policyAssignments/GUID04\", \"description\": \"Ownership on file mapping resources managed by the FoundationaLLM.AzureOpenAI resource provider.\", \"policy_definition_id\": \"/providers/FoundationaLLM.Authorization/policyDefinitions/00000000-0000-0000-0001-000000000001\", \"principal_id\": \"SECURITY_GROUP_ID\", \"principal_type\": \"Group\", \"scope\": \"/instances/FOUNDATIONALLM_INSTANCEID/providers/FoundationaLLM.AzureOpenAI/fileMappings\", \"created_on\": \"DEPLOY_TIME\", \"updated_on\": \"DEPLOY_TIME\", \"created_by\": \"SYSTEM\", \"updated_by\": \"SYSTEM\" } The following placehoders need to be replaced with the actual values: SECURITY_GROUP_ID - the ID of the security group that needs to be assigned to the policy FOUNDATIONALLM_INSTANCEID - the ID of the FoundationaLLM instance DEPLOY_TIME - the time when the policy was deployed GUID03 and GUID04 - unique identifiers for the policy assignments FoundationaLLM.AzureOpenAI The assistant and file user context artifacts are now simplified and stored in a new Cosmos DB container. Here are the configuration parameters for the new Cosmos DB container: Name Value Name ExternalResources Maximum RU/s 1000 Time to live Off Partition key /partitionKey Part of the upgrade to this version is to migrate the existing assistant and file user context artifacts to the new Cosmos DB container. Refer to the dedicated upgrade tool for instructions on how to perform this update. As a result of the migration, the newly created ExternalResources container will contain two types of items: AzureOpenAIConversationMapping and AzureOpenAIFileMapping. This is an example of an AzureOpenAIConversationMapping item: { \"conversationId\": \"0e56a170-5355-...\", \"openAIAssistantsAssistantId\": \"asst_kc...\", \"openAIAssistantsThreadId\": \"thread_73...\", \"openAIAssistantsThreadCreatedOn\": \"2024-10-14T17:57:10.510345+00:00\", \"openAIVectorStoreId\": \"vs_X6...\", \"openAIVectorStoreCreatedOn\": null, \"type\": \"AzureOpenAIConversationMapping\", \"id\": \"0e56a170-5355-...\", \"partitionKey\": \"...-73fad442-f614-4510-811f-414cb3a3d34b\", \"upn\": \"jackthecat@foundationaLLM.ai\", \"instanceId\": \"73fad442-f614-4510-811f-414cb3a3d34b\", \"openAIEndpoint\": \"https://openai-....openai.azure.com/\", \"objectId\": null, \"displayName\": null, \"description\": null, \"costCenter\": null, \"properties\": null, \"createdOn\": \"0001-01-01T00:00:00+00:00\", \"updatedOn\": \"0001-01-01T00:00:00+00:00\", \"createdBy\": null, \"updatedBy\": null, \"deleted\": false, \"expirationDate\": null, \"name\": \"0e56a170-5355-...\", \"_rid\": \"J2BUAKktW41bAAAAAAAAAA==\", \"_self\": \"dbs/J2BUAA==/colls/J2BUAKktW40=/docs/J2BUAKktW41bAAAAAAAAAA==/\", \"_etag\": \"\\\"8702b793-0000-0200-0000-672a60b90000\\\"\", \"_attachments\": \"attachments/\", \"_ts\": 1730830521 } This is an example of an AzureOpenAIFileMapping item: { \"fileObjectId\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.Attachment/attachments/a-f8...\", \"originalFileName\": \"some_file.csv\", \"fileContentType\": \"text/csv\", \"fileRequiresVectorization\": false, \"openAIFileId\": \"assistant-8G...\", \"openAIFileUploadedOn\": \"2024-10-14T23:01:02.3075592+00:00\", \"openAIAssistantsFileGeneratedOn\": null, \"openAIVectorStoreId\": null, \"type\": \"AzureOpenAIFileMapping\", \"id\": \"assistant-8G...\", \"partitionKey\": \"...-73fad442-f614-4510-811f-414cb3a3d34b\", \"upn\": \"jackthecat@foundationaLLM.ai\", \"instanceId\": \"73fad442-f614-4510-811f-414cb3a3d34b\", \"openAIEndpoint\": \"https://openai-....openai.azure.com/\", \"objectId\": null, \"displayName\": null, \"description\": null, \"costCenter\": null, \"properties\": null, \"createdOn\": \"0001-01-01T00:00:00+00:00\", \"updatedOn\": \"0001-01-01T00:00:00+00:00\", \"createdBy\": null, \"updatedBy\": null, \"deleted\": false, \"expirationDate\": null, \"name\": \"assistant-8G...\", \"_rid\": \"J2BUAKktW40yAAAAAAAAAA==\", \"_self\": \"dbs/J2BUAA==/colls/J2BUAKktW40=/docs/J2BUAKktW40yAAAAAAAAAA==/\", \"_etag\": \"\\\"87025e93-0000-0200-0000-672a60b70000\\\"\", \"_attachments\": \"attachments/\", \"_ts\": 1730830519 } Cleanup role assignments As a result of migrated resources from storage account to Cosmos DB, as well as the new policy-assignments mentioned above, the role-assignments store will have obsolete Owner role assignments on those objects. Please refer to the dedicated tool for instructions on how to perform this cleanup. The dedicated tool will cleanup role assignments for the following resources: FoundationaLLM.Attachment/attachments FoundationaLLM.AzureOpenAI/fileUserContexts FoundationaLLM.AzureOpenAI/assistantUserContexts FoundationaLLM.Conversation/conversations Configuration changes Resource provider templates The AzureOpenAI.template.json files within deploy/quick-start/data/resource-provider/FoundationaLLM.Configuration and deploy/standard/data/resource-provider/FoundationaLLM.Configuration have been updated to set the category field to the value LLM. This discriminator allows the Management Portal to filter the list of API endpoints by category and provide options to add AI Models to endpoints with the LLM category. The existing category property needs to be set to LLM on existing API endpoint configurations in the FoundationaLLM.Configuration resource provider that fit this description, including the AzureOpenAI endpoint configuration. Starting with 0.8.3 Resource provider changes If a user/group is not assigned to the instance-level Contributor role, then they will not be able to create new Conversations or upload Attachments. To adjust their permissions, the following changes are required: FoundationaLLM.Conversation In addition to assigning users/groups to the policy-assignments/<instance_id>-policy.json file within the FoundationaLLM.Authorization resource provider to assign them to the Conversation policy, we must now add them to the new Conversation contributor role (role_definition_id: d0d21b90-5317-499a-9208-3a6cb71b84f9) within the role-assignments/<instance_id>-role.json file within the FoundationaLLM.Authorization resource provider if the user/group is not assigned to the Contributor role on the FoundationaLLM instance (role_definition_id: a9f0020f-6e3a-49bf-8d1d-35fd53058edf). Here is an example entry: { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"a40b15f1-75ce-4a40-a857-1093ac9adf4d\", \"object_id\": \"/instances/0a1840df-71b6-496d-905a-145d93d827f3/providers/FoundationaLLM.Authorization/roleAssignments/a40b15f1-75ce-4a40-a857-1093ac9adf4d\", \"display_name\": null, \"description\": \"Conversation contributor role for FLLM Users\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/d0d21b90-5317-499a-9208-3a6cb71b84f9\", \"principal_id\": \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\", \"principal_type\": \"Group\", \"scope\": \"/instances/0a1840df-71b6-496d-905a-145d93d827f3\", \"properties\": null, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } FoundationaLLM.Attachment In addition to assigning users/groups to the policy-assignments/<instance_id>-policy.json file within the FoundationaLLM.Authorization resource provider to assign them to the Attachment policy, we must now add them to the new Attachment contributor role (role_definition_id: 8e77fb6a-7a78-43e1-b628-d9e2285fe25a) within the role-assignments/<instance_id>-role.json file within the FoundationaLLM.Authorization resource provider if the user/group is not assigned to the Contributor role on the FoundationaLLM instance (role_definition_id: a9f0020f-6e3a-49bf-8d1d-35fd53058edf). Here is an example entry: { \"type\": \"FoundationaLLM.Authorization/roleAssignments\", \"name\": \"891ca947-e648-46cf-a12a-774b52ded886\", \"object_id\": \"/instances/0a1840df-71b6-496d-905a-145d93d827f3/providers/FoundationaLLM.Authorization/roleAssignments/891ca947-e648-46cf-a12a-774b52ded886\", \"display_name\": null, \"description\": \"Attachment contributor role for FLLM Users\", \"cost_center\": null, \"role_definition_id\": \"/providers/FoundationaLLM.Authorization/roleDefinitions/8e77fb6a-7a78-43e1-b628-d9e2285fe25a\", \"principal_id\": \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\", \"principal_type\": \"Group\", \"scope\": \"/instances/0a1840df-71b6-496d-905a-145d93d827f3\", \"properties\": null, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false, \"expiration_date\": null } Starting with 0.8.2 Configuration changes The following settings are required: Name Default value FoundationaLLM:APIEndpoints:CoreAPI:Configuration:AllowedUploadFileExtensions c, cpp, cs, css, csv, doc, docx, gif, html, java, jpeg, jpg, js, json, md, pdf, php, png, pptx, py, rb, sh, tar, tex, ts, txt, xlsx, xml, zip FoundationaLLM:APIEndpoints:CoreAPI:Configuration:AzureOpenAIAssistantsFileSearchFileExtensions c, cpp, cs, css, doc, docx, html, java, js, json, md, pdf, php, pptx, py, rb, sh, tex, ts, txt FoundationaLLM:APIEndpoints:CoreAPI:Configuration:MaxUploadsPerMessage { \"value\": 10, \"value_exceptions\": [] } FoundationaLLM:APIEndpoints:CoreAPI:Configuration:CompletionResponsePollingIntervalSeconds { \"value\": 5, \"value_exceptions\": [] } FoundationaLLM:APIEndpoints:GatewayAPI:Configuration:AzureOpenAIAssistantsMaxVectorizationTimeSeconds 120 Note Here is an example of an override for the MaxUploadsPerMessage setting: { \"value\": 10, \"value_exceptions\": [ { \"user_principal_name\": \"ciprian@solliance.net\", \"value\": 5, \"enabled\": true } ] } Note Here is an example of an override for the CompletionResponsePollingIntervalSeconds setting: { \"value\": 5, \"value_exceptions\": [ { \"user_principal_name\": \"ciprian@solliance.net\", \"value\": 3, \"enabled\": true } ] } The following settings are optional (they should not be set by default): Name Default value FoundationaLLM:Instance:IdentitySubstitutionSecurityPrincipalId <security_principal_id> FoundationaLLM:Instance:IdentitySubstitutionUserPrincipalNamePattern ^fllm_load_test_user_\\d{5}_\\d{3}@solliance\\.net$ Note The FoundationaLLM:Instance:IdentitySubstitutionSecurityPrincipalId and FoundationaLLM:Instance:IdentitySubstitutionUserPrincipalNamePattern settings are used for load testing purposes only. If set, their values must be replaced with the appropriate values for the specific Entra ID tenant. Resource provider changes The following resource provider files must be renamed (if they already exist): Location Old name New name resource-provider/FoundationaLLM.Agent _agent-references.json _resource-references.json resource-provider/FoundationaLLM.AIModel _ai-model-references.json _resource-references.json resource-provider/FoundationaLLM.Configuration _api-endpoint-references.json _resource-references.json resource-provider/FoundationaLLM.DataSource _data-source-references.json _resource-references.json resource-provider/FoundationaLLM.Prompt _prompt-references.json _resource-references.json Note Within each of the renamed files, the <entity>References property must be renamed to ResourceReferences. FoundationaLLM.Agent A new property can be added to agent definitions: \"tools\": { \"dalle-image-generation\": { \"name\": \"dalle-image-generation\", \"description\": \"Generates an image based on a prompt.\", \"ai_model_object_ids\": { \"main_model\": \"/instances/73fad442-f614-4510-811f-414cb3a3d34b/providers/FoundationaLLM.AIModel/aiModels/DALLE3\" } } } FoundationaLLM.Authorization A new storage container named policy-assignments is required. The FoundationaLLM.Authorization resource provider will use this container to store policy assignments. Within the container, the <instance_id>-policy.json must be deployed with the default policy assignments. The template for the default policy assignments is available in Common/Constants/Data/DefaultPolicyAssignments.json. FoundationaLLM.Conversation When upgrading an existing FoundationaLLM instance, the items in the Sessions collection in Cosmos DB must be updated according to the following rules: if Object is of type Session of KioskSession: If the property DisplayName exists and is set to a non-empty string: Don't touch the item else: Set DisplayName to the value of Name Set Name to the value of SessionId else: No action needed Refer to the dedicated upgrade tool for instruction on how to perform this update. FoundationaLLM.Configuration The OneDrive (Work or School) integration requires the following API Endpoint Configuration entry in the storage account: FoundationaLLM.Configuration/OneDriveFileStoreConnector.json { \"type\": \"api-endpoint\", \"name\": \"OneDriveFileStoreConnector\", \"object_id\": \"/instances/{{instance_id}}/providers/FoundationaLLM.Configuration/apiEndpointConfigurations/OneDriveFileStoreConnector\", \"display_name\": null, \"description\": null, \"cost_center\": null, \"category\": \"FileStoreConnector\", \"subcategory\": \"OneDriveWorkSchool\", \"authentication_type\": \"AzureIdentity\", \"authentication_parameters\": { \"scope\": \"Files.Read.All\" }, \"url\": \"{{onedrive_base_url}}\", \"status_url\": \"\", \"url_exceptions\": [], \"timeout_seconds\": 2400, \"retry_strategy_name\": \"ExponentialBackoff\", \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": \"SYSTEM\", \"deleted\": false } Update FoundationaLLM.Configuration/_resource-references_.json with the reference to the file above. { \"Name\": \"OneDriveFileStoreConnector\", \"Filename\": \"/FoundationaLLM.Configuration/OneDriveFileStoreConnector.json\", \"Type\": \"api-endpoint\", \"Deleted\": false } FoundationaLLM.Attachment The Attachment resource provider now saves the attachment references to Cosmos DB, instead of Data Lake storage. A new Cosmos DB container must be created, named Attachments, with the following partition key: /upn. The following MSIs require a Cosmos DB role assigned: Gateway API Orchestration API Management API Long-Running Operations The context for a long-running operation is now stored in Cosmos DB. A new Cosmos DB container must be created, named Operations, with a partition key /id. Starting with 0.8.0 Core API changes: All Core API endpoints have been moved to the /instances/{instanceId} path. For example, the /status endpoint is now /instances/{instanceId}/status. The /orchestration/* endpoints have been moved to /instances/{instanceId}/completions/*. The previous /orchestration/completions endpoint is now /instances/{instanceId}/completions. The /sessions/{sessionId}/completion endpoint has been moved to /instances/{instanceId}/completions. Instead of having the sessionId as a path parameter, it is now in the request body as part of the CompletionRequest payload. /sessions/{sessionId}/summarize-name has been removed. In the future, the /completions endpoint will be used to generate summaries. OrchestrationRequest and CompletionRequest have combined into a single CompletionRequest object. DirectionCompletionRequest has been removed. Use CompletionRequest instead. Status controllers \\status action in the .NET API projects return value has renamed the Instance property to InstanceName. The CompletionController.cs file under dotnet/CoreApi/controllers has introduced the Async-Completions endpoint to handle asynchronous completions. With the introduction of Async-Completions, long running operations can now report on completion status based on Pending, InProgress, Completed and Failed states. Vectorization Embedding Profile introduces a required key in the Settings property named model_name. Every embedding request now flows through the Gateway API. Vectorization Indexing Profile introduces a required key api_endpoint_configuration_object_id in the Settings property. Retirement of SemanticKernel embedding type. All embedding requests now flow through the Gateway API. Gatekeeper API changes: All Gatekeeper API endpoints have been moved to the /instances/{instanceId} path. For example, the /status endpoint is now /instances/{instanceId}/status. The /orchestration/* endpoints have been moved to /instances/{instanceId}/completions/*. Orchestration API changes: All Gatekeeper API endpoints have been moved to the /instances/{instanceId} path. For example, the /status endpoint is now /instances/{instanceId}/status. The /orchestration/* endpoints have been moved to /instances/{instanceId}/completions/*. ======= New APIs Gateway Adapter API - requires the following configuration settings: FoundationaLLM:APIs:GatewayAdapterAPI:APIUrl FoundationaLLM:APIs:GatewayAdapterAPI:APIKey (mapped to the foundationallm-apis-gatewayadapterapi-apikey secret) FoundationaLLM:APIs:GatewayAdapterAPI:APIAppInsightsConnectionString (mapped to the foundationallm-app-insights-connection-string secret) State API - requires the following configuration settings: FoundationaLLM:APIs:StateAPI:APIUrl FoundationaLLM:APIs:StateAPI:APIKey (mapped to the foundationallm-apis-stateapi-apikey secret) FoundationaLLM:APIs:StateAPI:APIAppInsightsConnectionString (mapped to the foundationallm-app-insights-connection-string secret) Note These new APIs will be converted to use the new APIEndpoint artifacts. Changes in app registration names API Name Entra ID app registration name Application ID URI Scope name Core API FoundationaLLM-Core-API api://FoundationaLLM-Core Data.Read Management API FoundationaLLM-Management-API api://FoundationaLLM-Management Data.Manage Authorization API FoundationaLLM-Authorization-API api://FoundationaLLM-Authorization Authorization.Manage User Portal FoundationaLLM-Core-Portal api://FoundationaLLM-Core-Portal N/A Management Portal FoundationaLLM-Management-Portal api://FoundationaLLM-Management-Portal N/A Changes in app configuration settings The FoundationaLLM:APIs and FoundationaLLM:ExternalAPIs configuration namespaces have been replaced with the FoundationaLLM:APIEndpoints configuration namespace. Important All existing API registrations need to be updated to reflect these changes. The only two settings that will exist under FoundationaLLM:APIEndpoints are APIKey (for those API enpoints which use API key authentication) and AppInsightsConnectionString, all the other settings are now part of the APIEndpoint artifact managed by the FoundationaLLM.Configuration resource provider. This is an example for CoreAPI: FoundationaLLM:APIEndpoints:CoreAPI:APIKey FoundationaLLM:APIEndpoints:CoreAPI:AppInsightsConnectionString The FoundationaLLM:AzureAIStudio configuration namespace expects an APIEndpointConfigurationName property instead of BaseUrl. A new configuration setting named FoundationaLLM:Instance:SecurityGroupRetrievalStrategy with a value of IdentityManagementService must exist in the app configuration. It will be added by default in new deployments. Two new configuration settings required by the new FoundationaLLM.AzureOpenAI resource provider: FoundationaLLM:ResourceProviders:AzureOpenAI:Storage:AuthenticationType FoundationaLLM:ResourceProviders:AzureOpenAI:Storage:AccountName Pre-0.8.0 Vectorization resource stores use a unique collection name, Resources. They also add a new top-level property named DefaultResourceName. The items in the index_references collection have a property incorrectly named type which was renamed to index_entry_id. New gateway API, requires the following app configurations: FoundationaLLM:APIs:GatewayAPI:APIUrl FoundationaLLM:APIs:GatewayAPI:APIKey (with secret foundationallm-apis-gatewayapi-apikey) FoundationaLLM:APIs:GatewayAPI:AppInsightsConnectionString (with secret foundationallm-app-insights-connection-string) FoundationaLLM:Gateway:AzureOpenAIAccounts The AgentFactory and AgentFactoryAPI classes have been renamed to Orchestration and OrchestrationAPI, respectively. The following App Config settings need to be replaced in existing environments: FoundationaLLM:APIs:AgentFactoryAPI:APIKey -> FoundationaLLM:APIs:OrchestrationAPI:APIKey FoundationaLLM:APIs:AgentFactoryAPI:APIUrl -> FoundationaLLM:APIs:OrchestrationAPI:APIUrl FoundationaLLM:APIs:AgentFactoryAPI:AppInsightsConnectionString -> FoundationaLLM:APIs:OrchestrationAPI:AppInsightsConnectionString FoundationaLLM:Events:AzureEventGridEventService:Profiles:AgentFactoryAPI -> FoundationaLLM:Events:AzureEventGridEventService:Profiles:OrchestrationAPI FoundationaLLM:APIs:AgentFactoryAPI:ForceHttpsRedirection -? FoundationaLLM:APIs:OrchestrationAPI:ForceHttpsRedirection The following Key Vault secrets need to be replaced in existing environments: foundationallm-apis-agentfactoryapi-apikey -> foundationallm-apis-orchestrationapi-apikey There is an upgrade script available that migrates these settings and secrets to their new names. The following App Config settings are no longer needed: FoundationaLLM:Vectorization:Queues:Embed:ConnectionString FoundationaLLM:Vectorization:Queues:Extract:ConnectionString FoundationaLLM:Vectorization:Queues:Index:ConnectionString FoundationaLLM:Vectorization:Queues:Partition:ConnectionString The following Key Vault secret is no longer needed: foundationallm-vectorization-queues-connectionstring The following App Config settings need to be added as key-values: FoundationaLLM:Vectorization:Queues:Embed:AccountName (set to the name of the storage account that contains the vectorization queues - e.g., stejahszxcubrpi) FoundationaLLM:Vectorization:Queues:Extract:AccountName (set to the name of the storage account that contains the vectorization queues - e.g., stejahszxcubrpi) FoundationaLLM:Vectorization:Queues:Index:AccountName (set to the name of the storage account that contains the vectorization queues - e.g., stejahszxcubrpi) FoundationaLLM:Vectorization:Queues:Partition:AccountName (set to the name of the storage account that contains the vectorization queues - e.g., stejahszxcubrpi) The value for the App Config setting FoundationaLLM:Events:AzureEventGridEventService:Profiles:OrchestrationAPI should be set in the following format: { \"EventProcessingCycleSeconds\": 20, \"Topics\": [ { \"Name\": \"storage\", \"SubscriptionPrefix\": \"orch\", \"EventTypeProfiles\": [ { \"EventType\": \"Microsoft.Storage.BlobCreated\", \"EventSets\": [ { \"Namespace\": \"ResourceProvider.FoundationaLLM.Agent\", \"Source\": \"/subscriptions/0a03d4f9-c6e4-4ee1-87fb-e2005d2c213d/resourceGroups/rg-fllm-aca-050/providers/Microsoft.Storage/storageAccounts/stejahszxcubrpi\", \"SubjectPrefix\": \"/blobServices/default/containers/resource-provider/blobs/FoundationaLLM.Agent\" }, { \"Namespace\": \"ResourceProvider.FoundationaLLM.Vectorization\", \"Source\": \"/subscriptions/0a03d4f9-c6e4-4ee1-87fb-e2005d2c213d/resourceGroups/rg-fllm-aca-050/providers/Microsoft.Storage/storageAccounts/stejahszxcubrpi\", \"SubjectPrefix\": \"/blobServices/default/containers/resource-provider/blobs/FoundationaLLM.Vectorization\" }, { \"Namespace\": \"ResourceProvider.FoundationaLLM.Prompt\", \"Source\": \"/subscriptions/0a03d4f9-c6e4-4ee1-87fb-e2005d2c213d/resourceGroups/rg-fllm-aca-050/providers/Microsoft.Storage/storageAccounts/stejahszxcubrpi\", \"SubjectPrefix\": \"/blobServices/default/containers/resource-provider/blobs/FoundationaLLM.Prompt\" } ] } ] } ] } Vectorization text embedding profiles require only two items in the configuration_references section: DeploymentName and Endpoint. Optionally, a deployment_name entry can be specified in the settings section to override the default value in configuration_references.Endpoint. Here is an example of the updated format for a text embedding profile: { \"type\": \"text-embedding-profile\", \"name\": \"AzureOpenAI_Embedding_BaselineGlobalMacro\", \"object_id\": \"/instances/a6221c30-0bf2-4003-adb8-d3086bb2ad49/providers/FoundationaLLM.Vectorization/textEmbeddingProfiles/AzureOpenAI_Embedding_BaselineGlobalMacro\", \"display_name\": null, \"description\": null, \"text_embedding\": \"SemanticKernelTextEmbedding\", \"settings\": { \"deployment_name\": \"embeddings-3-large\" }, \"configuration_references\": { \"DeploymentName\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:DeploymentName\", \"Endpoint\": \"FoundationaLLM:Vectorization:SemanticKernelTextEmbeddingService:Endpoint\" }, \"created_on\": \"0001-01-01T00:00:00+00:00\", \"updated_on\": \"0001-01-01T00:00:00+00:00\", \"created_by\": null, \"updated_by\": null, \"deleted\": false } External orchestration APIs must be configured using the FoundationaLLM:ExternalAPIs configuration namespace. For example, the BaselineTradingGlobalMacro external API has the following configurations: FoundationaLLM:ExternalAPIs:BaselineTradingGlobalMacro:APIUrl FoundationaLLM:ExternalAPIs:BaselineTradingGlobalMacro:APIKey Note These entries do not need to be created as part of the deployment process. App Config key namespace that was previously FoundationaLLM:Vectorization:ContentSources:* has been moved to FoundationaLLM:DataSources:*. All existing keys need to be moved to the new namespace. New app config entries required: FoundationaLLM:Attachment:ResourceProviderService:Storage:AuthenticationType FoundationaLLM:Attachment:ResourceProviderService:Storage:AccountName App Config key namespace that was previously FoundationaLLM:Vectorization:ContentSources:* has been moved to FoundationaLLM:DataSources:*. All existing keys need to be moved to the new namespace. The following App Config setting needs to be added/updated as key-values: Add FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableAzureContentSafetyPromptShield Add FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableLakeraGuard Add FoundationaLLM:APIs:GatekeeperAPI:Configuration:EnableEnkryptGuardrails Rename FoundationaLLM:AzureContentSafety:APIKey in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:APIKey Rename FoundationaLLM:AzureContentSafety:APIUrl in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:APIUrl Rename FoundationaLLM:AzureContentSafety:HateSeverity in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:HateSeverity Rename FoundationaLLM:AzureContentSafety:SelfHarmSeverity in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SelfHarmSeverity Rename FoundationaLLM:AzureContentSafety:SexualSeverity in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:SexualSeverity Rename FoundationaLLM:AzureContentSafety:ViolenceSeverity in FoundationaLLM:APIs:Gatekeeper:AzureContentSafety:ViolenceSeverity Add FoundationaLLM:APIs:Gatekeeper:LakeraGuard:APIKey Add FoundationaLLM:APIs:Gatekeeper:LakeraGuard:APIUrl Add FoundationaLLM:APIs:Gatekeeper:EnkryptGuardrails:APIKey Add FoundationaLLM:APIs:Gatekeeper:EnkryptGuardrails:APIUrl The following Key Vault secret is needed: lakera-guard-api-key enkrypt-guardrails-apikey"
  },
  "docs/release-notes/release_notes_0.9.7.html": {
    "href": "docs/release-notes/release_notes_0.9.7.html",
    "title": "FoundationaLLM 0.9.7 Release Notes | FoundationaLLM",
    "summary": "FoundationaLLM 0.9.7 Release Notes What's New This release introduces several major features that enhance the FoundationaLLM experience for end users: End User Agent Selection: An enhanced Settings dialog allows you to pick and choose which agents are displayed in your agent dropdown, giving you control over your agent list and enabling you to customize your workspace to show only the agents you use most frequently. Featured Agents in Chat Portal: A new \"Featured agents\" section in the agent dropdown enables highlighting new agents that users should try, making it easier to discover and explore new capabilities. Model-Agnostic Agents with Agent Parity: The new Model Agnostic Agents enables agents and tools to use different models (GPT, Claude, Gemini) as best suited to each task. Three new agents have been built with agent parity in mind, ensuring consistent behavior across models. Capabilities not natively supported by one model are now enabled consistently for all. Data Pipelines: Data pipelines provide powerful capabilities for processing and indexing files across FoundationaLLM. In the front end, data pipelines automatically process end user file uploads, extracting content, generating embeddings, and making files searchable for your agents. On the backend, data pipelines handle files private to specific agents, ensuring secure and isolated processing. Additionally, data pipelines enable the ingestion and indexing of large document collections, making it possible to build comprehensive knowledge bases from thousands or millions of documents efficiently and at scale. Prompt Injection Detection for Knowledge Sources: If this shield is enabled for the data pipeline used by your agent, whenever you or a user upload a file, it will be automatically scanned for malicious prompt injections. If a prompt injection is detected, you will receive an error message and the file will not be uploaded, indexed, or available for subsequent conversations. This acts like an \"antivirus\" for knowledge sources, protecting your agents from malicious content embedded in files. Enhanced Code Interpreter Tool: The code interpreter has been significantly improved with structured data handling (automatic previews of pandas DataFrames), a protocol that guides LLM code generation to produce results of interest, and improved error handling that provides clear feedback to users (e.g., when images exceed available memory in the sandbox). Multiple File Uploads: You can now upload up to 10 documents at a time with a single request, making it faster and more convenient to share multiple files with your agent. Later, you can ask the agent to tell you what files are available in your conversation. Artifacts improvements: Improved artifact display and organization for better visibility into tool execution details make it easier to understand how the agent processed your request. For example, the code interpreter tool's Code artifact now makes it easier to read both the code that was generated and input to the tool, as well as the code that was actually executed by the tool Enhancements and Features What's New in the User Portal End user agent selection: A new Settings dialog allows you to pick and choose which agents are displayed in your agent dropdown. This gives you control over your agent list, enabling you to customize your workspace to show only the agents you use most frequently and hide those you don't need. Featured agents section: New \"Featured agents\" section in the agent dropdown that enables highlighting new agents that users should try. This helps users discover and try new agents more easily, improving agent adoption and exploration. Better agent discovery & navigation: Filters by name for quick agent search Enabled-only view to focus on active agents Featured agents prominently displayed Persist selection across sessions Correct returns to agent lists after navigation Redesigned prompt area: Consistent branding and improved editors in the User Portal for a more polished experience. Multimodal content support: Files embedded into completion requests: Files from conversation history can be embedded directly into completion requests with proper content type detection and handling Direct image sending: Image files are automatically detected and sent with proper base64 encoding and MIME type handling Audio content blocks support: Support for WAV and MP3 audio files in completion requests with dedicated audio analysis service Multiple file uploads: Support for uploading up to 10 documents at a time with a single request, making it faster and more convenient to share multiple files with your agent. You can later ask the agent to tell you what files are available in your conversation. File handling improvements: File names are enclosed in / delimiters to prevent misinterpretation (e.g., files starting with numbers like 1., 2. won't be treated as numbered lists) Context file embedding: Files from conversation history can be embedded directly into completion requests with proper content type detection Multimodal file support: Automatic detection and handling of image files (base64 encoding), audio files (WAV, MP3 with proper MIME types), and text files in context messages File path normalization: Improved handling of file paths in annotations and message content Artifacts improvements: Improved artifact display and organization for better visibility into tool execution details make it easier to understand how the agent processed your request. For example, the code interpreter tool's Code artifact now makes it easier to read both the code that was generated and input to the tool, as well as the code that was actually executed by the tool Accessibility & reliability: Numerous accessibility improvements including keyboard-only actions, Safari button states, consistent pop-up timeouts, visible scrollbars. What's New in Terms of Agents Model-Agnostic Agents: FoundationaLLMFunctionCallingWorkflow: New workflow enabling agents and tools to use different models as best suited to the problem. This model-agnostic approach allows optimal model selection per task. Agent Parity: Three new agents have been built with agent parity in mind, ensuring that the behavior of an agent using GPT is similar to an agent built with Claude, which is similar to Gemini. This consistency provides a uniform experience regardless of the underlying model. Tool-Based Capability Enablement: Capabilities that might not be supported natively by one model are now enabled consistently for all through the agentic use of tools. Examples include file processing and generation, which work uniformly across all model types. Code Interpreter Tool Enhancements: Structured data handling with previews: Automatic detection and handling of pandas DataFrames and Series. DataFrames are auto-saved to CSV with preview generation (first 10 rows) returned as structured metadata. Series are converted to dictionaries for serialization, making it easy to work with structured data in code execution results. Protocol for guiding LLM code generation: AST-based variable extraction from the last line of code (_extract_last_variable_names function) guides the LLM to structure code that returns results as variables in the last line. This protocol supports single variables, tuples, and assignments, enabling the LLM to generate code that produces results of interest more reliably. Enhanced error handling and resilience: Comprehensive try-except blocks catch container crashes, timeouts, and memory limit errors. Error details are captured and provided to the agent for user communication (e.g., \"image exceeds available memory in sandbox\"). Errors within the code interpreter are conveyed to the agent for summary to the user, providing clear feedback on execution issues. File management improvements: Upload/download capabilities, automatic detection of newly generated files by comparing before/after file lists, comprehensive file metadata tracking including file object IDs, original file names, file paths, sizes, content types, and conversation IDs. Code preparation: Automatic markdown code block stripping via __prepare_code method ensures clean code execution. Session management: Persistent code sessions with dynamic session IDs from runnable_config, enabling stateful code execution across multiple tool invocations. Comprehensive artifact generation: Detailed metadata including original prompts, generated code, tool output, errors, and results, enabling full traceability of code execution. Token usage tracking: Input/output token tracking for monitoring and cost analysis across code generation and execution. What's New in Backend Improvements Data Pipelines: Filtering, re-run/testing, state persistence, duration metrics, and performance optimization Better artifact creation, content item listing, retrieval, and metadata handling (extraction, serialization, configurable property names, array filters) Indexing reliability (fix repeated runs, payload limits, refresh & failed stage handling) and Azure AI Search handling/logging Prompt Injection Detection (Antivirus for Knowledge Sources): Azure AI Content Safety Shielding Data Pipeline Stage Plugin automatically scans files during pipeline processing to detect file-based indirect prompt injection attacks Acts as an antivirus for knowledge sources, protecting against malicious content embedded in uploaded files Automatic scanning of content parts during pipeline processing Unsafe content detection with detailed logging and reporting Notification to data source plugins when prompt injection is detected, allowing them to handle unsafe content appropriately Content part shielding marks safe content parts for downstream processing Prevents malicious prompts from being injected into the knowledge base through file uploads Code Sessions & Execution: Standardized JSON response schema: Consistent response format across all code execution endpoints Enhanced error reporting: Better error messages for timeout scenarios, gateway timeouts, and container crashes Container crash detection: Improved handling and reporting when custom containers crash or become unresponsive Extended HTTP timeouts: Longer timeout values for complex code execution scenarios Pandas DataFrame/Series auto-serialization: Automatic CSV export and metadata generation for pandas objects in code results Gateway API: Optimized embeddings, limited retries, better timeouts, richer logging, race condition fixes. Execute completions via Gateway API with model parameters added; support for new tokenizers and sparse text chunk positions. Context API & Orchestration: Deserialization fixes, dependency injection corrections, improved logging JSON handling improvements Encoding detection/standardization (force UTF-8 when needed) Conversation/file history correctness Message history conversion: Automatic conversion between FoundationaLLM message history format and LangChain message types Context file message construction: Intelligent construction of multimodal messages with proper content type handling for images, audio, and text Azure AI Search: Safer payload sizes, filter expression logging, vector store filter fixes, query corrections, index update handling. Developer Tooling & Extensibility: Python/.NET plugins support (wheel loading, plugin base classes published), PowerShell module broadened (agents, prompts, conversations, vector DBs), and Azure OpenAI & Cosmos DB CLIs Databricks added as a model provider and a dedicated Databricks agent tool Agent Evaluations: Comprehensive test framework: Automated test execution, validation, and result management for FoundationaLLM agents Multiple validation modes: Rule-based validation (pattern matching, numeric validation, artifact checks), LLM-based validation (semantic similarity), and hybrid approaches Test generation capabilities: Automated test expansion with multiple strategies (variations, edge cases, negative tests, combinations) Interactive test suite creation: Interactive mode for creating and managing test suites with multiline input support Rich reporting: HTML dashboards with detailed analysis, CSV/JSON outputs, and performance metrics Cross-agent comparison: Support for running the same test suite across multiple agents for comparison Reliability testing: Built-in support for repeating tests multiple times to assess consistency Encoding/content type fixes, dynamic loading fixes, standardized schemas for code session results Management Portal improvements: Redesigned prompt area with consistent branding and improved editors. Management endpoints for restricted operations, role definitions, and resource role assignments; instance identifier required in resource paths. Extensive UI/UX work in Settings and creation/edit flows (progress patterns, disabled states, read-only handling, enabled filters, featured agents, spinners, tooltip/accessibility fixes). Improvements Security & Authorization Stronger authorization checks across resource providers, clearer exception messages with resource paths, ignore optional role when parent is used, improved identity management (on‑premises names, UPN casing), and role assignments loaded & filtered for current user. Code Sessions & Execution Standardized JSON response schema: Consistent response format across all code execution endpoints Enhanced error reporting: Better error messages for timeout scenarios, gateway timeouts, and container crashes Container crash detection: Improved handling and reporting when custom containers crash or become unresponsive Extended HTTP timeouts: Longer timeout values for complex code execution scenarios Pandas DataFrame/Series auto-serialization: Automatic CSV export and metadata generation for pandas objects in code results Longer HTTP timeouts, improved error reporting for custom containers, and JSON error handling improvements Developer Experience & CI/CD Release workflow updates (RC tags, versioning fixes), removal of deprecated references, improved OpenTelemetry source alignment, and simplified internal deployment flows. SDK/clients updated to allow empty response collections, correct headers, enforce filename resolution, and expose missing properties. Workflow Execution & Content Artifacts Execution metrics: Workflow execution time tracking and token usage aggregation across router and final LLM calls Content artifact generation: Automatic creation of workflow execution artifacts with timing and token information Intermediate response tracking: Captures intermediate tool responses for final response generation Tool not found handling: Graceful error handling when a tool specified in tool_calls is not found in the tools list Empty response handling: SDK/clients updated to allow empty response collections without errors User Interface & Usability Fixes Numerous fixes across portals: avatar rendering/clipping, welcome message HTML, inconsistent branding at login, disabled states, tooltips association, \"no agents\" messaging, delete confirmations, edit icon behavior, and keyboard‑only deletion. Consistent progress indicators for lists/create/edit, stronger pencil icon visibility, filtered dropdowns, empty states, and correct handling when no agent is selected or after refresh. Contact Information For support and further inquiries regarding this release, please reach out to us: Support Contact: https://foundationallm.ai/contact Website: FoundationalLLM Conclusion We hope you enjoy the new features and improvements in FoundationalLLM version 0.9.7. Your feedback continues to be instrumental in driving our product forward. Thank you for your continued support."
  },
  "docs/schema.html": {
    "href": "docs/schema.html",
    "title": "Agent Schemas | FoundationaLLM",
    "summary": "Agent Schemas This document provides a general overview of agent, data source, and vectorization configuration."
  },
  "index.html": {
    "href": "index.html",
    "title": "FoundationaLLM: Deploy secure, extensible AI agents inside your own cloud. Designed for scale, built for control, and ready to work. | FoundationaLLM",
    "summary": "FoundationaLLM: Deploy secure, extensible AI agents inside your own cloud. Designed for scale, built for control, and ready to work. FoundationaLLM simplifies and streamlines building knowledge management (e.g., question/answer agents) and analytic (e.g., self-service business intelligence) copilots over the data sources present across your enterprise. FoundationaLLM deploys a secure, comprehensive and highly configurable copilot platform to your Azure cloud environment: Simplifies integration with enterprise data sources used by agent for in-context learning (e.g., enabling RAG, CoT, ReAct and inner monologue patterns). Provides defense in depth with fine-grain security controls over data used by agent and pre/post completion filters that guard against attack. Hardened solution attacked by an LLM red team from inception. Scalable solution load balances across multiple LLM endpoints. Extensible to new data sources, new LLM orchestrators and LLMs. Reality What it really takes to create a secure, well governed, scalable and extensible enterprise copilot solution: Where does FoundationaLLM stack against the other copilot solutions? What do WE mean by \"copilot\" It's a rapidly evolving AI world out there, so let's level set on what we mean when we say copilot as this is concept core to FoundationaLLM. At its most basic, a copilot uses enterprise supplied knowledge and generative AI models to author text, write code or render images, often by reasoning over human supplied prompts. Across these modalities, the AI is used to assist a human directly with a specific task. That's what makes it a copilot. This basic capability emerges in copilots which power these scenarios: Knowledge Management: Help users quickly find the information they seek and deliver at the right level and in the right format. Examples include summarization, rephrasing or retargeting to address a persona (e.g., explain it like I'm five), sentiment analysis and recommendations. Analytics: Help users quickly get to the data driven insights they seek. Examples include recommendations, predictions, anomaly detection, statistical analysis and data querying and reporting. Why is FoundationaLLM Needed? Simply put we saw lot of folks reinventing the wheel just to get a customized copilot that was grounded and bases its responses in their own data as opposed to the trained parametric knowledge of the model. Many of the solutions we saw made for great demos, but were effectively toys wrapping calls to OpenAI endpoints- they were not something intended or ready to take into production. We built FoundationaLLM to provide a continous journey, one that was quick to get started with so folks could experiment quickly with LLM's but not fall off a cliff after that with a solution that would be insecure, unlicensed, inflexible and not fully featured enough to grow from the prototype into a production solution without having to start all over. The core problems to deliver enterprise copilots are: Enterprise grade copilots are complex and have lots of moving parts (not to mention infrastructure). The industry has a skills gap when it comes to filling the roles needed to deliver these complex copilot solutions. The top AI risks (inaccuracy, cybersecurity, compliance, explainability, privacy) are not being mitigated. Delivery of a copilot solution is time consuming, expensive and frustrating. Where can FoundationaLLM fill the need? Documentation Get up to speed with FoundationaLLM by reading the documentation. This includes deployment instructions, quickstarts, architecture, and API references. Getting Started FoundationalLLM provides a simple command line driven approach to getting your first deployment up and running. Basically, it's two commands. After that, you can customize the solution, run it locally on your machine and update the deployment with your customizations. Follow the Quick Start Deployment instructions to get FoundationaLLM deployed in your Azure subscription. If you want to run the solution locally, follow the Local Development instructions. You can browse the documentation using the sidebar or visit the API section for the reference documentation."
  },
  "operations/network-architecture.html": {
    "href": "operations/network-architecture.html",
    "title": "FoundationaLLM Network Architecture | FoundationaLLM",
    "summary": "FoundationaLLM Network Architecture This document provides a visual overview of the FoundationaLLM network architecture when deployed to Azure using Azure Container Apps. Network Diagram flowchart TB subgraph Internet[\"\uD83C\uDF10 Internet\"] Dev1[\"\uD83D\uDCBB Developer Laptop 1<br/>Home Network\"] Dev2[\"\uD83D\uDCBB Developer Laptop 2<br/>Office Network\"] Users[\"\uD83D\uDC65 End Users\"] Admins[\"\uD83D\uDC54 Administrators\"] end subgraph GitHub[\"☁️ GitHub Cloud\"] Repo[\"\uD83D\uDCE6 foundationallm/foundationallm\"] GHCR[\"\uD83D\uDCE6 GitHub Container Registry<br/>ghcr.io/foundationallm\"] end subgraph Azure[\"☁️ Azure Region (e.g., East US 2)\"] subgraph PublicEndpoints[\"\uD83D\uDD13 Public Endpoints (HTTPS 443)\"] UserPortalIP[\"\uD83C\uDF10 user-portal<br/>*.azurecontainerapps.io\"] MgmtPortalIP[\"\uD83C\uDF10 management-portal<br/>*.azurecontainerapps.io\"] end subgraph VNet[\"\uD83D\uDD12 Virtual Network (10.0.0.0/16)\"] subgraph CAESubnet[\"\uD83D\uDCE1 Container Apps Subnet (10.0.0.0/21)\"] subgraph CAE[\"\uD83D\uDC33 Container Apps Environment\"] subgraph FrontendApps[\"Frontend (External Ingress)\"] UserPortal[\"\uD83D\uDC64 user-portal:80\"] MgmtPortal[\"\uD83D\uDCCB management-portal:80\"] end subgraph BackendAPIs[\"Backend APIs (Internal Ingress)\"] CoreAPI[\"\uD83D\uDE80 core-api:80\"] MgmtAPI[\"\uD83D\uDCCB management-api:80\"] AuthAPI[\"\uD83D\uDD10 authorization-api:80\"] OrchAPI[\"\uD83C\uDFAD orchestration-api:80\"] GatewayAPI[\"\uD83D\uDEAA gateway-api:80\"] LangChainAPI[\"\uD83D\uDD17 langchain-api:80\"] StateAPI[\"\uD83D\uDCBE state-api:80\"] ContextAPI[\"\uD83D\uDCDD context-api:80\"] CoreWorker[\"⚡ core-worker\"] end end end subgraph PESubnet[\"\uD83D\uDD10 Private Endpoints Subnet (10.0.8.0/24)\"] PE_KV[\"PE: Key Vault\"] PE_Storage[\"PE: Storage\"] PE_Cosmos[\"PE: Cosmos DB\"] PE_AppConfig[\"PE: App Config\"] PE_AI[\"PE: AI Services\"] end end subgraph PaaS[\"☁️ Azure PaaS Services\"] subgraph CoreRG[\"Core Resource Group\"] KV[\"\uD83D\uDD11 Key Vault\"] AppConfig[\"⚙️ App Configuration\"] AppInsights[\"\uD83D\uDCCA App Insights\"] EventGrid[\"\uD83D\uDCE8 Event Grid\"] end subgraph DataRG[\"Data Resource Group\"] Storage[\"\uD83D\uDCE6 Blob Storage<br/>(ADLS Gen2)\"] CosmosDB[\"\uD83C\uDF0D Cosmos DB\"] end subgraph ContextRG[\"Context Resource Group\"] ContextStorage[\"\uD83D\uDCE6 Context Storage\"] end subgraph AIRG[\"AI Resource Group\"] AIServices[\"\uD83E\uDD16 AI Services<br/>(OpenAI Endpoint)\"] end end subgraph EntraID[\"\uD83D\uDD37 Microsoft Entra ID\"] OAuth[\"OAuth 2.0 / OIDC<br/>Token Validation\"] end end %% Developer connections Dev1 -->|\"HTTPS 443<br/>git push\"| Repo Dev2 -->|\"HTTPS 443<br/>git push\"| Repo Repo -->|\"Build & Push<br/>Images\"| GHCR GHCR -->|\"Pull Images<br/>HTTPS 443\"| CAE %% User traffic flow Users -->|\"HTTPS 443\"| UserPortalIP Admins -->|\"HTTPS 443\"| MgmtPortalIP UserPortalIP --> UserPortal MgmtPortalIP --> MgmtPortal %% Internal API calls (within CAE - internal DNS) UserPortal -->|\"HTTP 80\"| CoreAPI MgmtPortal -->|\"HTTP 80\"| MgmtAPI CoreAPI -->|\"HTTP 80\"| AuthAPI CoreAPI -->|\"HTTP 80\"| OrchAPI MgmtAPI -->|\"HTTP 80\"| AuthAPI OrchAPI -->|\"HTTP 80\"| LangChainAPI LangChainAPI -->|\"HTTP 80\"| GatewayAPI CoreAPI -->|\"HTTP 80\"| StateAPI CoreAPI -->|\"HTTP 80\"| ContextAPI %% Private endpoint connections PE_KV -.->|\"Private Link\"| KV PE_Storage -.->|\"Private Link\"| Storage PE_Cosmos -.->|\"Private Link\"| CosmosDB PE_AppConfig -.->|\"Private Link\"| AppConfig PE_AI -.->|\"Private Link\"| AIServices %% Service to PaaS via Private Endpoints BackendAPIs -->|\"TCP 443\"| PE_KV BackendAPIs -->|\"TCP 443\"| PE_AppConfig BackendAPIs -->|\"TCP 443\"| PE_Storage BackendAPIs -->|\"TCP 443\"| PE_Cosmos GatewayAPI -->|\"TCP 443\"| PE_AI ContextAPI -->|\"TCP 443\"| ContextStorage %% Telemetry (outbound) CAE -.->|\"HTTPS 443<br/>Telemetry\"| AppInsights CAE -.->|\"HTTPS 443<br/>Events\"| EventGrid %% Auth flow FrontendApps -.->|\"HTTPS 443<br/>Token Validation\"| OAuth BackendAPIs -.->|\"HTTPS 443<br/>Token Validation\"| OAuth %% Styling classDef internet fill:#E8E8E8,stroke:#999,color:#333 classDef github fill:#24292E,stroke:#1B1F23,color:#fff classDef public fill:#28A745,stroke:#1E7E34,color:#fff classDef vnet fill:#0078D4,stroke:#005A9E,color:#fff classDef container fill:#326CE5,stroke:#1E4F9E,color:#fff classDef privateendpoint fill:#FF6B6B,stroke:#CC5555,color:#fff classDef paas fill:#FF8C00,stroke:#CC7000,color:#fff classDef identity fill:#68217A,stroke:#4B1557,color:#fff class Dev1,Dev2,Users,Admins internet class Repo,GHCR github class UserPortalIP,MgmtPortalIP public class UserPortal,MgmtPortal,CoreAPI,MgmtAPI,AuthAPI,OrchAPI,GatewayAPI,LangChainAPI,StateAPI,ContextAPI,CoreWorker container class PE_KV,PE_Storage,PE_Cosmos,PE_AppConfig,PE_AI privateendpoint class KV,AppConfig,AppInsights,EventGrid,Storage,CosmosDB,ContextStorage,AIServices paas class OAuth identity Network Zones Zone CIDR / Endpoint Purpose Internet Public Developers, End Users, GitHub Public Endpoints *.azurecontainerapps.io User Portal, Management Portal (HTTPS 443) Container Apps Subnet 10.0.0.0/21 All containerized microservices Private Endpoints Subnet 10.0.8.0/24 Secure PaaS connectivity Traffic Flows Flow Protocol Source → Destination User Access HTTPS 443 Internet → Public Endpoints → Portals API Calls HTTP 80 Container ↔ Container (internal) Data Access TCP 443 Containers → Private Endpoints → PaaS AI Inference HTTPS 443 gateway-api → AI Services Telemetry HTTPS 443 All containers → App Insights Authentication HTTPS 443 All services → Entra ID CI/CD HTTPS 443 GitHub → GHCR → Container Apps Security Boundaries External ingress: Only user-portal and management-portal are publicly accessible Internal ingress: All backend APIs use internal ingress (not publicly accessible) Private Link: All Azure PaaS services are accessed via private endpoints within the VNet OAuth 2.0: All requests are authenticated via Microsoft Entra ID Container Apps Services Frontend Services (External Ingress) Service Description user-portal End-user web application for interacting with AI agents management-portal Administrative web application for managing agents, prompts, and configuration Backend APIs (Internal Ingress) Service Description core-api Main API gateway for user-facing operations management-api API for administrative operations authorization-api Handles authentication and authorization orchestration-api Routes requests to appropriate AI orchestrators gateway-api Gateway for AI model interactions langchain-api LangChain-based orchestration service state-api Manages conversation and application state context-api Handles context and file operations core-worker Background job processing Azure PaaS Services Service Resource Group Purpose Key Vault Core Secrets and certificate management App Configuration Core Centralized application configuration Application Insights Core Monitoring and telemetry Event Grid Core Event-driven messaging Blob Storage (ADLS Gen2) Data Resource provider storage, file storage Cosmos DB Data Conversations, state, and document storage Context Storage Context Context files and queue storage AI Services AI Azure OpenAI endpoints for LLM inference"
  }
}